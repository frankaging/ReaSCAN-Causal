{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sparse_situation(situation_representation: dict, grid_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Each grid cell in a situation is fully specified by a vector:\n",
    "    [_ _ _ _ _ _ _   _       _      _       _   _ _ _ _]\n",
    "     1 2 3 4 r g b circle square cylinder agent E S W N\n",
    "     _______ _____ ______________________ _____ _______\n",
    "       size  color        shape           agent agent dir.\n",
    "    :param situation_representation: data from dataset.txt at key \"situation\".\n",
    "    :param grid_size: int determining row/column number.\n",
    "    :return: grid to be parsed by computational models.\n",
    "    \"\"\"\n",
    "    # num_object_attributes = len([int(bit) for bit in situation_representation[\"target_object\"][\"vector\"]])\n",
    "    num_object_attributes = 12\n",
    "    # Object representation + agent bit + agent direction bits (see docstring).\n",
    "    num_grid_channels = num_object_attributes + 1 + 4\n",
    "\n",
    "    # Initialize the grid.\n",
    "    grid = np.zeros([grid_size, grid_size, num_grid_channels], dtype=int)\n",
    "\n",
    "    # Place the agent.\n",
    "    agent_row = int(situation_representation[\"agent_position\"][\"row\"])\n",
    "    agent_column = int(situation_representation[\"agent_position\"][\"column\"])\n",
    "    agent_direction = int(situation_representation[\"agent_direction\"])\n",
    "    agent_representation = np.zeros([num_grid_channels], dtype=np.int)\n",
    "    agent_representation[-5] = 1\n",
    "    agent_representation[-4 + agent_direction] = 1\n",
    "    grid[agent_row, agent_column, :] = agent_representation\n",
    "\n",
    "    # Loop over the objects in the world and place them.\n",
    "    placed_position = set([])\n",
    "    for placed_object in situation_representation[\"placed_objects\"].values():\n",
    "        object_vector = [int(bit) for bit in placed_object[\"vector\"]] \n",
    "        if len(object_vector) < num_object_attributes:\n",
    "            object_vector += [0]\n",
    "        object_vector = np.array(object_vector, dtype=np.int)\n",
    "        object_row = int(placed_object[\"position\"][\"row\"])\n",
    "        object_column = int(placed_object[\"position\"][\"column\"])\n",
    "        placed_position.add((object_row, object_column))\n",
    "        if (object_row, object_column) not in placed_position:\n",
    "            grid[object_row, object_column, :] = np.concatenate([object_vector, np.zeros([5], dtype=np.int)])\n",
    "        else:\n",
    "            overlay = np.concatenate([object_vector, np.zeros([5], dtype=np.int)])\n",
    "            grid[object_row, object_column, :] += overlay # simply add it.\n",
    "    return grid\n",
    "\n",
    "\n",
    "def data_loader(file_path: str) -> Dict[str, Union[List[str], np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads grounded SCAN dataset from text file and ..\n",
    "    :param file_path: Full path to file containing dataset (dataset.txt)\n",
    "    :returns: dict with as keys all splits and values list of example dicts with input, target and situation.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as infile:\n",
    "        all_data = json.load(infile)\n",
    "        grid_size = int(all_data[\"grid_size\"])\n",
    "        splits = list(all_data[\"examples\"].keys())\n",
    "        logger.info(\"Found data splits: {}\".format(splits))\n",
    "        loaded_data = {}\n",
    "        for split in splits:\n",
    "            loaded_data[split] = []\n",
    "            logger.info(\"Now loading data for split: {}\".format(split))\n",
    "            for data_example in all_data[\"examples\"][split]:\n",
    "                input_command = data_example[\"command\"].split(',')\n",
    "                target_command = data_example[\"target_commands\"].split(',')\n",
    "                situation = parse_sparse_situation(situation_representation=data_example[\"situation\"],\n",
    "                                                   grid_size=grid_size)\n",
    "                loaded_data[split].append({\"input\": input_command,\n",
    "                                           \"target\": target_command,\n",
    "                                           \"situation\": situation.tolist()})  # .tolist() necessary to be serializable\n",
    "            logger.info(\"Loaded {} examples in split {}.\\n\".format(len(loaded_data[split]), split))\n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ReaSCAN-novel-action-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader(\"../../../data-files/ReaSCAN-novel-action-length/data-compositional-splits.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open('./data-files/ReaSCAN-novel-action-length/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ReaSCAN-novel-attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader(\"../../../data-files/ReaSCAN-novel-attribute/data-compositional-splits.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open('./data-files/ReaSCAN-novel-attribute/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ReaSCAN-novel-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader(\"../../../data-files/ReaSCAN-novel-direction/data-compositional-splits.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open('./data-files/ReaSCAN-novel-direction/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading gSCAN-novel-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 03:17 Found data splits: ['train', 'test', 'dev', 'situational_1']\n",
      "2021-09-15 03:17 Now loading data for split: train\n",
      "2021-09-15 03:17 Loaded 34343 examples in split train.\n",
      "\n",
      "2021-09-15 03:17 Now loading data for split: test\n",
      "2021-09-15 03:17 Loaded 1201 examples in split test.\n",
      "\n",
      "2021-09-15 03:17 Now loading data for split: dev\n",
      "2021-09-15 03:17 Loaded 357 examples in split dev.\n",
      "\n",
      "2021-09-15 03:17 Now loading data for split: situational_1\n",
      "2021-09-15 03:17 Loaded 8282 examples in split situational_1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_loader(\"../../../data-files/gSCAN-novel-direction/data-compositional-splits.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open('./data-files/gSCAN-novel-direction/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
