{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "condition = \"-e\"\n",
    "if condition != \"\":\n",
    "    postfix = \"-splits\"\n",
    "condition_clean = condition.strip(\"-\")\n",
    "    \n",
    "flags = {\n",
    "    f\"dataset_path\": f\"../../../data-files-updated/ReaSCAN-compositional{condition}/data-compositional{postfix}.txt\",\n",
    "    f\"output_file\": f\"../../../data-files-updated/ReaSCAN-compositional{condition}/parsed_dataset.txt\",\n",
    "    \"save_data\": False\n",
    "}\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sparse_situation(situation_representation: dict, grid_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Each grid cell in a situation is fully specified by a vector:\n",
    "    [_ _ _ _ _ _ _   _       _      _       _   _ _ _ _]\n",
    "     1 2 3 4 r g b circle square cylinder agent E S W N\n",
    "     _______ _____ ______________________ _____ _______\n",
    "       size  color        shape           agent agent dir.\n",
    "    :param situation_representation: data from dataset.txt at key \"situation\".\n",
    "    :param grid_size: int determining row/column number.\n",
    "    :return: grid to be parsed by computational models.\n",
    "    \"\"\"\n",
    "    num_object_attributes = len([int(bit) for bit in situation_representation[\"target_object\"][\"vector\"]])\n",
    "    # Object representation + agent bit + agent direction bits (see docstring).\n",
    "    num_grid_channels = num_object_attributes + 1 + 4\n",
    "\n",
    "    # Initialize the grid.\n",
    "    grid = np.zeros([grid_size, grid_size, num_grid_channels], dtype=int)\n",
    "\n",
    "    # Place the agent.\n",
    "    agent_row = int(situation_representation[\"agent_position\"][\"row\"])\n",
    "    agent_column = int(situation_representation[\"agent_position\"][\"column\"])\n",
    "    agent_direction = int(situation_representation[\"agent_direction\"])\n",
    "    agent_representation = np.zeros([num_grid_channels], dtype=np.int)\n",
    "    agent_representation[-5] = 1\n",
    "    agent_representation[-4 + agent_direction] = 1\n",
    "    grid[agent_row, agent_column, :] = agent_representation\n",
    "\n",
    "    # Loop over the objects in the world and place them.\n",
    "    placed_position = set([])\n",
    "    for placed_object in situation_representation[\"placed_objects\"].values():\n",
    "        object_vector = np.array([int(bit) for bit in placed_object[\"vector\"]], dtype=np.int)\n",
    "        object_row = int(placed_object[\"position\"][\"row\"])\n",
    "        object_column = int(placed_object[\"position\"][\"column\"])\n",
    "        placed_position.add((object_row, object_column))\n",
    "        if (object_row, object_column) not in placed_position:\n",
    "            grid[object_row, object_column, :] = np.concatenate([object_vector, np.zeros([5], dtype=np.int)])\n",
    "        else:\n",
    "            overlay = np.concatenate([object_vector, np.zeros([5], dtype=np.int)])\n",
    "            grid[object_row, object_column, :] += overlay # simply add it.\n",
    "    return grid\n",
    "\n",
    "\n",
    "def data_loader(file_path: str) -> Dict[str, Union[List[str], np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads grounded SCAN dataset from text file and ..\n",
    "    :param file_path: Full path to file containing dataset (dataset.txt)\n",
    "    :returns: dict with as keys all splits and values list of example dicts with input, target and situation.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as infile:\n",
    "        all_data = json.load(infile)\n",
    "        grid_size = int(all_data[\"grid_size\"])\n",
    "        splits = list(all_data[\"examples\"].keys())\n",
    "        logger.info(\"Found data splits: {}\".format(splits))\n",
    "        loaded_data = {}\n",
    "        for split in splits:\n",
    "            loaded_data[split] = []\n",
    "            logger.info(\"Now loading data for split: {}\".format(split))\n",
    "            for data_example in all_data[\"examples\"][split]:\n",
    "                input_command = data_example[\"command\"].split(',')\n",
    "                target_command = data_example[\"target_commands\"].split(',')\n",
    "                situation = parse_sparse_situation(situation_representation=data_example[\"situation\"],\n",
    "                                                   grid_size=grid_size)\n",
    "                loaded_data[split].append({\"input\": input_command,\n",
    "                                           \"target\": target_command,\n",
    "                                           \"situation\": situation.tolist()})  # .tolist() necessary to be serializable\n",
    "            logger.info(\"Loaded {} examples in split {}.\\n\".format(len(loaded_data[split]), split))\n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 19:17 Found data splits: ['test']\n",
      "2021-06-14 19:17 Now loading data for split: test\n",
      "2021-06-14 19:17 Loaded 8003 examples in split test.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_loader(flags[\"dataset_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open(f'parsed_dataset{condition}/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save helpers for compositional splits only.\n",
    "for split, dt in data.items():\n",
    "    with open(f'parsed_dataset/{condition_clean}.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flags[\"save_data\"]:\n",
    "    with open(flags[\"output_file\"], 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dataloader experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "train_iter, input_vocab, target_vocab = dataloader('parsed_dataset-p1/train.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
