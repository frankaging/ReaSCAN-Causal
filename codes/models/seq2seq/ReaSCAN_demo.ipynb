{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read-in ReaSCAN and Manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "from ReaSCAN_dataset import *\n",
    "\n",
    "# Init of the vocab.\n",
    "intransitive_verbs = [\"walk\"]\n",
    "transitive_verbs = [\"push\", \"pull\"]\n",
    "adverbs = [\"while zigzagging\", \"while spinning\", \"cautiously\", \"hesitantly\"]\n",
    "nouns = [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "size_adjectives = [\"big\", \"small\"]\n",
    "relative_pronouns = [\"that is\"]\n",
    "relation_clauses = [\"in the same row as\", \n",
    "                    \"in the same column as\", \n",
    "                    \"in the same color as\", \n",
    "                    \"in the same shape as\", \n",
    "                    \"in the same size as\",\n",
    "                    \"inside of\"]\n",
    "vocabulary = ReaSCANVocabulary.initialize(intransitive_verbs=intransitive_verbs,\n",
    "                                   transitive_verbs=transitive_verbs, adverbs=adverbs, nouns=nouns,\n",
    "                                   color_adjectives=color_adjectives,\n",
    "                                   size_adjectives=size_adjectives, \n",
    "                                   relative_pronouns=relative_pronouns, \n",
    "                                   relation_clauses=relation_clauses)\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "object_vocabulary = ObjectVocabulary(shapes=vocabulary.get_semantic_shapes(),\n",
    "                                     colors=vocabulary.get_semantic_colors(),\n",
    "                                     min_size=min_object_size, max_size=max_object_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in info.\n",
    "path_to_data = \"../../../data-files/ReaSCAN-Simple/data-compositional-splits.txt\"\n",
    "ReaSCAN_data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command: walk,to,the,big,yellow,cylinder,cautiously\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAHr0lEQVR4nO3dvW4bVxqA4cPFFm7SmY2QwGUaIy5MpI37NCoNbCtegAH3hnsDvgCqDeBSl+C0CzZOl85GFmrG3W4/KRZMZGtEcf7PN+d5ADcWf0ajefUdkiNyVdd1AvL3j7k3ADiNWCEIsUIQYoUgxApBiBWC+GebCz948KB+/PjxWNvS2qdPn9KjR4/m3oys5baPfvvtj1nv/4cfvrv1f6dsU9P1xvDx48f0+fPnVdPXVm1eZ12v13VVVYNtWF/b7Tbtdru5NyNrue2js7MXs97/9fXbW/93yjY1XW8Mm80m7ff7xlgtgyEIsUIQYoUgxApBiBWCECsEIVYIotVJEbBEU72G2pfJCkGIdU6/7ufeAgKxDJ7bzWB/2sy3HWTPZM3Jr3vTljuJNUeipYFlcM4skbnBZI3CtC2eWKMRbbEsgyOxFC6aWCMQKUms+RIoXxFrTgTKEZ5gyoVQuYfJOieB0oLJCkGIFYIQKwQhVghCrBCEWCEIsUIQXmdlUj///L+sPtUuEpMVghArBCFWCKL1J5+fn5+PuDkM7d/f/3fS+/vx928mvb+lubq6SlVVNX7yeetYq6oabMP62m63nqy4x5M3zye9vw8v3x39up/ZcZvNJu33+8ZYLYMhCLFCEGKFIMQKQYgVghArBCFWCEKsEIRYIQixQhBihSDECkGIFYIQKwQhVghCrBCEWCEIsUIQYoUgxApBiBWCECsEIVYIQqwQhFghCLFCED6fdeF+/P0bH1exECYrBCFWCEKsEIRYIQixQhBihSDECkGIFYJwUgQxvV91u96zetjtmJDJSjwFhpqSWCEMsUIQYoUgxEoshT5eTUmsRFJwqCmJFcIQKwQhVmIofAmcklghDKcbcrdj02zKiWWqppTESpNT4jhcZmFB5MwymLyZqn8RK19qG0fXmGhNrBCEWPlb1yk51nS1BP6CWCEIsZInU/UWsUIQYuVvXafS0NPMVG0kVghCrBCEWPlS26WkJfBknBvMbYcDP5cT+Ukpmawcc1eQQp2Fycpx/hQuGyYreRDqvcQKQYgVghAr87MEPolYiamwUFPybDA5KDC8LkxWCEKsEIRYIYhVXZ/+eGG9Xtfn5+cjbg6U7erqKlVV1fz0eF3XJ/97+PBhnZOLi4u5NyF7ue2j3LYnN0+fPq3rO/qzDIYgxApBiBWCcFJEJp68eX7vZT68fDfBlpArk3VmT948PynUw2Upl8k6k67hHa431ZR9vbr9KsKrFi/3MRyxzmCICTlmtE2B3vV14U7HMnhiuS9l7wu17+XpTqwTGiPUIW+za3iCnUbxy+CvD7RSl3V9g3u9WhW776ZSZKzHDszD14Y+8MZc/vZ9/DrUZBTsuIpbBp96YL5erYpY3pXwPS5FUbHOdWDm/qTSkMQ/nqJi7cLBRy6KibWE6NpO8LH2SQn7eg7FxArRiRWCECsEIVYIophYS3ixvu1JEWPtkxL29RyKibUrBx65KCrWucIr6R0e/HIbT1GxpnT6wfSqros48Er4HpeiyBP5bx6gU/3VzYeX70Y77bDv5H5V14OcyCD8cRUZ600OsP/rG6z9OL7ilsFzGuOx65C32TU4oU5DrBMbMq4x4m/zWL2Ux/W5EOsMhohs7GeYj4Uo0nkU/5h1LofY2j7pNPXLQKLMh1hndjO+u8It6XVa7ibWjIiSYzxmhSDECkGIFYJY1S2e7Vuv1/X5+fmIm8PB7l+Xna63/eVikNsZytfbw3FXV1epqqrGU8lax1pV1WAb1td2u0273W7uzRje+46n/T1r+Fl2va2hfLVNi/2ZDWSz2aT9ft/4Q7MMhiDEmpshpyqLItaczL1kJWtOilgCU3U2Z2cvGv//+vrt4Pcl1lyYqmHcFehdlxkqXMvg6EzVSZ0S6hDXaTLNZD1lajjoyFjf4A7X7zNlx52s71enL+/aXHZpPAOctaEmY9/bsgyeW6m/oArWNdhxlsF9DsDDdU2N4+yfSQw5VfsafrIONSlKmDglfI+BjRnq2dmL1rc/bKwOvmmYqkXK+zGr+G8T6iRyWv4e5B3rkvlFREvDxTrWwbfEg3qJ3xOdtJngJmsklsBFE+vUTFU6EmsUpmrxxBqBUElinZYlMD0MF+tYv/2XMlWESoM2f4WT92RdSqh92AezGOOdHvrKO1bgL8PG+qwebhIsaaL4e1UatJ3e40zWvgeZg9Q+yMCYS+Eutz3eMrjrwba0g9QTS6Hl9Nh13Pdgela3O1iXFmpKy/yeCnN9/XbQv8Lp+gtg/DdMu3mwNoXrYCaAIYLtO6Wnfd9gYRJYn2CHWE57k29o4WZ0p4Q75GNesS7c9pcLH7E4kqmffHJSBAQhVghCrBCEWCGI0E8wfXt5mV5fXs52/69qL0UxHZMVghArBCFWCEKsEIRYIQixQhBihSDECkGIFYIQKwQhVghCrBCEWCEIsUIQYoUgxApBiBWCECsEIVYIQqwQhFghCLFCEGKFIMQKQYgVghArBCFWCEKsEIRYIQixQhBihSBCfz7rfy4u0m63m3szYBImKwQhVghCrBDEqq7r0y+8WlUppU/jbQ4U71Fd1+umL7SKFZiPZTAEIVYIQqwQhFghCLFCEGKFIMQKQYgVghArBPEn9Pd9P+aaGc8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: ['walk', 'walk', 'turn right', 'walk', 'walk']\n"
     ]
    }
   ],
   "source": [
    "example = ReaSCAN_data_json[\"examples\"][\"train\"][random.randint(0, len(ReaSCAN_data_json[\"examples\"][\"train\"])-1)]\n",
    "command = example[\"command\"]\n",
    "print(f\"command: {command}\")\n",
    "situation = example[\"situation\"]\n",
    "world = World(grid_size=6, colors=vocabulary.get_semantic_colors(),\n",
    "              object_vocabulary=object_vocabulary,\n",
    "              shapes=vocabulary.get_semantic_shapes(),\n",
    "              save_directory=\"./tmp/\")\n",
    "world.clear_situation()\n",
    "for obj_idx, obj in situation[\"placed_objects\"].items():\n",
    "    world.place_object(\n",
    "        Object(size=int(obj[\"object\"][\"size\"]), color=obj[\"object\"][\"color\"], shape=obj[\"object\"][\"shape\"]), \n",
    "        position=Position(row=int(obj[\"position\"][\"row\"]), column=int(obj[\"position\"][\"column\"]))\n",
    "    )\n",
    "world.place_agent_at(\n",
    "    Position(\n",
    "        row=int(situation[\"agent_position\"][\"row\"]), \n",
    "        column=int(situation[\"agent_position\"][\"column\"])\n",
    "))\n",
    "_ = world.render_simple()\n",
    "\n",
    "# HERE: you can change to other target object.\n",
    "target_position = Position(\n",
    "    row=int(situation['target_object']['position'][\"row\"]), \n",
    "    column=int(situation['target_object']['position'][\"column\"])\n",
    ")\n",
    "world.go_to_position(position=target_position, manner='', primitive_command=\"walk\")\n",
    "target_commands, target_demonstration = world.get_current_observations()\n",
    "print(f\"action: {target_commands}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare very simple ReaSCAN/gSCAN split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/gSCAN-Simple/data-compositional-splits.txt\"\n",
    "ReaSCAN_data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = ReaSCAN_data_json[\"examples\"][\"train\"]\n",
    "all_dev = ReaSCAN_data_json[\"examples\"][\"dev\"]\n",
    "all_test = ReaSCAN_data_json[\"examples\"][\"test\"]\n",
    "situational_1_test = ReaSCAN_data_json[\"examples\"][\"situational_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command': 'walk,to,a,yellow,small,cylinder',\n",
       " 'meaning': 'walk,to,a,yellow,small,cylinder',\n",
       " 'derivation': \"NP -> NN,NP -> JJ NP,NP -> JJ NP,DP -> 'a' NP,VP -> VV_intrans 'to' DP,ROOT -> VP;T:walk,NT:VV_intransitive -> walk,T:to,T:a,T:yellow,NT:JJ -> small:JJ -> yellow,T:small,T:cylinder,NT:NN -> cylinder\",\n",
       " 'situation': {'grid_size': 6,\n",
       "  'agent_position': {'row': '1', 'column': '2'},\n",
       "  'agent_direction': 0,\n",
       "  'target_object': {'vector': '10000101000',\n",
       "   'position': {'row': '2', 'column': '2'},\n",
       "   'object': {'shape': 'cylinder', 'color': 'yellow', 'size': '1'}},\n",
       "  'distance_to_target': '1',\n",
       "  'direction_to_target': 's',\n",
       "  'placed_objects': {'0': {'vector': '10000101000',\n",
       "    'position': {'row': '2', 'column': '2'},\n",
       "    'object': {'shape': 'cylinder', 'color': 'yellow', 'size': '1'}},\n",
       "   '1': {'vector': '01000101000',\n",
       "    'position': {'row': '3', 'column': '2'},\n",
       "    'object': {'shape': 'cylinder', 'color': 'yellow', 'size': '2'}},\n",
       "   '2': {'vector': '10001000010',\n",
       "    'position': {'row': '5', 'column': '3'},\n",
       "    'object': {'shape': 'square', 'color': 'red', 'size': '1'}},\n",
       "   '3': {'vector': '01001000010',\n",
       "    'position': {'row': '3', 'column': '4'},\n",
       "    'object': {'shape': 'square', 'color': 'red', 'size': '2'}},\n",
       "   '4': {'vector': '10000100001',\n",
       "    'position': {'row': '5', 'column': '2'},\n",
       "    'object': {'shape': 'cylinder', 'color': 'blue', 'size': '1'}},\n",
       "   '5': {'vector': '01000100001',\n",
       "    'position': {'row': '3', 'column': '5'},\n",
       "    'object': {'shape': 'cylinder', 'color': 'blue', 'size': '2'}},\n",
       "   '6': {'vector': '01000010001',\n",
       "    'position': {'row': '5', 'column': '5'},\n",
       "    'object': {'shape': 'circle', 'color': 'blue', 'size': '2'}},\n",
       "   '7': {'vector': '01000010001',\n",
       "    'position': {'row': '3', 'column': '3'},\n",
       "    'object': {'shape': 'circle', 'color': 'blue', 'size': '2'}},\n",
       "   '8': {'vector': '00010011000',\n",
       "    'position': {'row': '2', 'column': '5'},\n",
       "    'object': {'shape': 'circle', 'color': 'yellow', 'size': '4'}},\n",
       "   '9': {'vector': '00010011000',\n",
       "    'position': {'row': '0', 'column': '1'},\n",
       "    'object': {'shape': 'circle', 'color': 'yellow', 'size': '4'}},\n",
       "   '10': {'vector': '00010010100',\n",
       "    'position': {'row': '5', 'column': '1'},\n",
       "    'object': {'shape': 'circle', 'color': 'green', 'size': '4'}},\n",
       "   '11': {'vector': '00010010100',\n",
       "    'position': {'row': '4', 'column': '3'},\n",
       "    'object': {'shape': 'circle', 'color': 'green', 'size': '4'}}},\n",
       "  'carrying_object': None},\n",
       " 'target_commands': 'turn right,walk',\n",
       " 'verb_in_command': 'walk',\n",
       " 'manner': '',\n",
       " 'referred_target': 'small yellow cylinder'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to avoid examples with adverb, because we don't allow them.\n",
    "select_train = []\n",
    "select_dev = []\n",
    "select_test = []\n",
    "select_situational_1_test = []\n",
    "for example in all_train:\n",
    "    if example[\"manner\"] == \"\" and example[\"verb_in_command\"] == \"walk\":\n",
    "        select_train += [example]\n",
    "for example in all_dev:\n",
    "    if example[\"manner\"] == \"\" and example[\"verb_in_command\"] == \"walk\":\n",
    "        select_dev += [example]\n",
    "for example in all_test:\n",
    "    if example[\"manner\"] == \"\" and example[\"verb_in_command\"] == \"walk\":\n",
    "        select_test += [example]\n",
    "for example in situational_1_test:\n",
    "    if example[\"manner\"] == \"\" and example[\"verb_in_command\"] == \"walk\":\n",
    "        select_situational_1_test += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReaSCAN_data_json[\"examples\"] = {\n",
    "    \"train\":select_train,\n",
    "    \"test\":select_test,\n",
    "    \"dev\":select_dev,\n",
    "    \"situational_1\":select_situational_1_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to the disk.\n",
    "with open(\"../../../data-files/gSCAN-Simple/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(ReaSCAN_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2145, 0.3951, 0.4269],\n",
       "        [0.5654, 0.8510, 0.0725],\n",
       "        [0.2132, 0.4830, 0.2361]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2950, 0.9072, 0.8376],\n",
       "        [0.4900, 0.7411, 0.8824],\n",
       "        [0.5466, 0.2683, 0.3414]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1,1] = b[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2145, 0.3951, 0.4269],\n",
       "        [0.5654, 0.7411, 0.0725],\n",
       "        [0.2132, 0.4830, 0.2361]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
