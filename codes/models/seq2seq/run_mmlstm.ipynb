{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from seq2seq.model import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(data_iterator):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths, \\\n",
    "            dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "            dual_agent_positions, dual_target_positions, \\\n",
    "            dual_input_lengths, dual_target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "        yield (input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_iterator,\n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx,\n",
    "    sos_idx,\n",
    "    eos_idx,\n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    "):\n",
    "    accuracies = []\n",
    "    target_accuracies = []\n",
    "    exact_match = 0\n",
    "    for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "            data_iterator=data_iterator, model=model, max_decoding_steps=max_decoding_steps, pad_idx=pad_idx,\n",
    "            sos_idx=sos_idx, eos_idx=eos_idx, max_examples_to_evaluate=max_examples_to_evaluate, device=device):\n",
    "        accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "        if accuracy == 100:\n",
    "            exact_match += 1\n",
    "        accuracies.append(accuracy)\n",
    "        target_accuracies.append(aux_acc_target)\n",
    "    return (float(np.mean(np.array(accuracies))), (exact_match / len(accuracies)) * 100,\n",
    "            float(np.mean(np.array(target_accuracies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_path: str, \n",
    "    args,\n",
    "    data_directory: str, \n",
    "    generate_vocabularies: bool, \n",
    "    input_vocab_path: str,   \n",
    "    target_vocab_path: str, \n",
    "    embedding_dimension: int, \n",
    "    num_encoder_layers: int, \n",
    "    encoder_dropout_p: float,\n",
    "    encoder_bidirectional: bool, \n",
    "    training_batch_size: int, \n",
    "    test_batch_size: int, \n",
    "    max_decoding_steps: int,\n",
    "    num_decoder_layers: int, \n",
    "    decoder_dropout_p: float, \n",
    "    cnn_kernel_size: int, \n",
    "    cnn_dropout_p: float,\n",
    "    cnn_hidden_num_channels: int, \n",
    "    simple_situation_representation: bool, \n",
    "    decoder_hidden_size: int,\n",
    "    encoder_hidden_size: int, \n",
    "    learning_rate: float, \n",
    "    adam_beta_1: float, \n",
    "    adam_beta_2: float, \n",
    "    lr_decay: float,\n",
    "    lr_decay_steps: int, \n",
    "    resume_from_file: str, \n",
    "    max_training_iterations: int, \n",
    "    output_directory: str,\n",
    "    print_every: int, \n",
    "    evaluate_every: int, \n",
    "    conditional_attention: bool, \n",
    "    auxiliary_task: bool,\n",
    "    weight_target_loss: float, \n",
    "    attention_type: str, \n",
    "    k: int, \n",
    "    # counterfactual training arguments\n",
    "    run_name: str,\n",
    "    cf_mode: str,\n",
    "    cf_sample_p: float,\n",
    "    checkpoint_save_every: int,\n",
    "    include_cf_loss: bool,\n",
    "    include_task_loss: bool,\n",
    "    cf_loss_weight: float,\n",
    "    is_wandb: bool,\n",
    "    intervene_attribute: int,\n",
    "    intervene_time: int,\n",
    "    intervene_dimension_size: int,\n",
    "    include_cf_auxiliary_loss: bool,\n",
    "    intervene_method: str,\n",
    "    no_cuda: bool,\n",
    "    restrict_sampling: str,\n",
    "    max_training_examples=None, \n",
    "    seed=42,\n",
    "    **kwargs\n",
    "):\n",
    "    # we at least need to have one kind of loss.\n",
    "    logger.info(f\"LOSS CONFIG: include_task_loss={include_task_loss}, \"\n",
    "                f\"include_cf_loss={include_cf_loss} with cf_loss_weight = {cf_loss_weight}...\")\n",
    "    \n",
    "    cfg = locals().copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    from pathlib import Path\n",
    "    # the output directory name is generated on-the-fly.\n",
    "    dataset_name = data_directory.strip(\"/\").split(\"/\")[-1]\n",
    "    run_name = f\"counterfactual_{dataset_name}_seed_{seed}_lr_{learning_rate}_attr_{intervene_attribute}_size_{intervene_dimension_size}_cf_loss_{include_cf_loss}_aux_loss_{include_cf_auxiliary_loss}_restrict_{restrict_sampling}\"\n",
    "    output_directory = os.path.join(output_directory, run_name)\n",
    "    cfg[\"output_directory\"] = output_directory\n",
    "    logger.info(f\"Create the output directory if not exist: {output_directory}\")\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # initialize w&b in the beginning.\n",
    "    if is_wandb:\n",
    "        logger.warning(\"Enabling wandb for tensorboard logging...\")\n",
    "        import wandb\n",
    "        run = wandb.init(\n",
    "            project=\"ReaSCAN-Causal-ICLR-Official\", \n",
    "            entity=\"wuzhengx\",\n",
    "            name=run_name,\n",
    "        )\n",
    "        wandb.config.update(args)\n",
    "    \n",
    "    logger.info(\"Loading all data into memory...\")\n",
    "    logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "    data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "    logger.info(\"Loading Training set...\")\n",
    "    training_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"train\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=k\n",
    "    )\n",
    "    training_set.read_dataset(\n",
    "        max_examples=max_training_examples,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "    logger.info(\"Done Loading Training set.\")\n",
    "    logger.info(\"  Loaded {} training examples.\".format(training_set.num_examples))\n",
    "    logger.info(\"  Input vocabulary size training set: {}\".format(training_set.input_vocabulary_size))\n",
    "    logger.info(\"  Most common input words: {}\".format(training_set.input_vocabulary.most_common(5)))\n",
    "    logger.info(\"  Output vocabulary size training set: {}\".format(training_set.target_vocabulary_size))\n",
    "    logger.info(\"  Most common target words: {}\".format(training_set.target_vocabulary.most_common(5)))\n",
    "\n",
    "    if generate_vocabularies:\n",
    "        training_set.save_vocabularies(input_vocab_path, target_vocab_path)\n",
    "        logger.info(\"Saved vocabularies to {} for input and {} for target.\".format(input_vocab_path, target_vocab_path))\n",
    "\n",
    "    logger.info(\"Loading Dev. set...\")\n",
    "    test_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"dev\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=0\n",
    "    )\n",
    "    test_set.read_dataset(\n",
    "        max_examples=None,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "\n",
    "    # Shuffle the test set to make sure that if we only evaluate max_testing_examples we get a random part of the set.\n",
    "    test_set.shuffle_data()\n",
    "    logger.info(\"Done Loading Dev. set.\")\n",
    "    \n",
    "    # some important variables.\n",
    "    grid_size = training_set.grid_size\n",
    "    target_position_size = 2*grid_size - 1\n",
    "    \n",
    "    # create modell based on our dataset.\n",
    "    model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "                  target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "                  num_cnn_channels=training_set.image_channels,\n",
    "                  input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "                  target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "                  target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "                  target_position_size=target_position_size,\n",
    "                  **cfg)\n",
    "    \n",
    "    # gpu setups\n",
    "    use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "\n",
    "    # optimizer\n",
    "    log_parameters(model)\n",
    "    trainable_parameters = [parameter for parameter in model.parameters() if parameter.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_parameters, lr=learning_rate, betas=(adam_beta_1, adam_beta_2))\n",
    "    scheduler = LambdaLR(optimizer,\n",
    "                         lr_lambda=lambda t: lr_decay ** (t / lr_decay_steps))\n",
    "    \n",
    "    \n",
    "    # Load model and vocabularies if resuming.\n",
    "    start_iteration = 1\n",
    "    best_iteration = 1\n",
    "    best_accuracy = 0\n",
    "    best_exact_match = -99\n",
    "    best_loss = float('inf')\n",
    "    if resume_from_file:\n",
    "        assert os.path.isfile(resume_from_file), \"No checkpoint found at {}\".format(resume_from_file)\n",
    "        logger.info(\"Loading checkpoint from file at '{}'\".format(resume_from_file))\n",
    "        optimizer_state_dict = model.load_model(resume_from_file)\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "        start_iteration = model.trained_iterations\n",
    "        logger.info(\"Loaded checkpoint '{}' (iter {})\".format(resume_from_file, start_iteration))\n",
    "    \n",
    "    # Loading dataset and preprocessing a bit.\n",
    "    train_data, _ = training_set.get_dual_dataset()\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.training_batch_size)\n",
    "    test_data, _ = test_set.get_dual_dataset()\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False)\n",
    "    \n",
    "    if use_cuda and n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # graphical model\n",
    "    train_max_decoding_steps = int(training_set.get_max_seq_length_target())\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    logger.info(f\"MAX_DECODING_STEPS for Training: {train_max_decoding_steps}\")\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "\n",
    "    hi_model = HighLevelModel(\n",
    "        # None\n",
    "    )\n",
    "    hi_model.to(device)\n",
    "    logger.info(\"Finish loading both low and high models..\")\n",
    "    \n",
    "    logger.info(\"Training starts..\")\n",
    "    cf_sample_per_batch_in_percentage = cf_sample_p\n",
    "    logger.info(f\"Setting cf_sample_per_batch_in_percentage = {cf_sample_per_batch_in_percentage}\")\n",
    "    training_iteration = start_iteration\n",
    "    while training_iteration < max_training_iterations:\n",
    "\n",
    "        # Shuffle the dataset and loop over it.\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # main batch\n",
    "            input_batch, target_batch, situation_batch, \\\n",
    "                agent_positions_batch, target_positions_batch, \\\n",
    "                input_lengths_batch, target_lengths_batch, \\\n",
    "                dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "                dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "                dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "            is_best = False\n",
    "            model.train()\n",
    "            \n",
    "            is_best = False\n",
    "            model.train()\n",
    "            \n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            situation_batch = situation_batch.to(device)\n",
    "            agent_positions_batch = agent_positions_batch.to(device)\n",
    "            target_positions_batch = target_positions_batch.to(device)\n",
    "            input_lengths_batch = input_lengths_batch.to(device)\n",
    "            target_lengths_batch = target_lengths_batch.to(device)\n",
    "            \n",
    "            dual_input_max_seq_lens = max(dual_input_lengths_batch)[0]\n",
    "            dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "            \n",
    "            # let us allow shuffling here, so that we have more diversity.\n",
    "#             perm_idx = torch.randperm(dual_input_batch.size()[0])\n",
    "#             dual_input_batch = dual_input_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_target_batch = dual_target_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_situation_batch = dual_situation_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_agent_positions_batch = dual_agent_positions_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_target_positions_batch = dual_target_positions_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_input_lengths_batch = dual_input_lengths_batch.index_select(dim=0, index=perm_idx)\n",
    "#             dual_target_lengths_batch = dual_target_lengths_batch.index_select(dim=0, index=perm_idx)\n",
    "            dual_input_batch = dual_input_batch.to(device)\n",
    "            dual_target_batch = dual_target_batch.to(device)\n",
    "            dual_situation_batch = dual_situation_batch.to(device)\n",
    "            dual_agent_positions_batch = dual_agent_positions_batch.to(device)\n",
    "            dual_target_positions_batch = dual_target_positions_batch.to(device)\n",
    "            dual_input_lengths_batch = dual_input_lengths_batch.to(device)\n",
    "            dual_target_lengths_batch = dual_target_lengths_batch.to(device)\n",
    "            \n",
    "            # init\n",
    "            loss = 0.0\n",
    "            task_loss = 0.0\n",
    "            cf_loss = 0.0\n",
    "            cf_position_loss = 0.0\n",
    "            batch_size = agent_positions_batch.size(0)\n",
    "            cf_sample_per_batch = int(batch_size*cf_sample_per_batch_in_percentage)   \n",
    "            auxiliary_accuracy_target = 0.\n",
    "            metrics_position_x, metrics_position_y = 0.0, 0.0\n",
    "            cf_accuracy, cf_exact_match = 0.0, 0.0\n",
    "            \n",
    "            # we use the main hidden to track.\n",
    "            task_encoded_image = model(\n",
    "                situations_input=situation_batch,\n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "            task_hidden, task_encoder_outputs = model(\n",
    "                commands_input=input_batch, \n",
    "                commands_lengths=input_lengths_batch,\n",
    "                tag=\"command_input_encode_no_dict\"\n",
    "            )\n",
    "            task_hidden = model(\n",
    "                command_hidden=task_hidden,\n",
    "                tag=\"initialize_hidden\"\n",
    "            )\n",
    "            task_projected_keys_visual = model(\n",
    "                encoded_situations=task_encoded_image,\n",
    "                tag=\"projected_keys_visual\"\n",
    "            )\n",
    "            task_projected_keys_textual = model(\n",
    "                command_encoder_outputs=task_encoder_outputs[\"encoder_outputs\"],\n",
    "                tag=\"projected_keys_textual\"\n",
    "            )\n",
    "            task_outputs = []\n",
    "            for j in range(train_max_decoding_steps):\n",
    "                task_token = target_batch[:,j]\n",
    "                (task_output, task_hidden) = model(\n",
    "                    lstm_input_tokens_sorted=task_token,\n",
    "                    lstm_hidden=task_hidden,\n",
    "                    lstm_projected_keys_textual=task_projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=task_projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "                task_output = F.log_softmax(task_output, dim=-1)\n",
    "                task_outputs += [task_output]\n",
    "            target_scores = torch.stack(task_outputs, dim=1)\n",
    "            task_loss = model(\n",
    "                loss_target_scores=target_scores, \n",
    "                loss_target_batch=target_batch,\n",
    "                tag=\"loss\"\n",
    "            )\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                task_loss = task_loss.mean() # mean() to average on multi-gpu.\n",
    "            \n",
    "            input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "            target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "            dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "            intervene_attribute = random.choice([0,1])\n",
    "            intervene_time = 1\n",
    "            intervene_with_time = intervene_time\n",
    "            \n",
    "            batch_size = agent_positions_batch.size(0)\n",
    "            intervened_target_batch = []\n",
    "            intervened_target_lengths_batch = []\n",
    "\n",
    "            high_hidden_states = hi_model(\n",
    "                agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "                target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "            high_actions = torch.zeros(\n",
    "                high_hidden_states.size(0), 1\n",
    "            ).long().to(device)\n",
    "            dual_high_hidden_states = hi_model(\n",
    "                agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "                target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "            dual_high_actions = torch.zeros(\n",
    "                dual_high_hidden_states.size(0), 1\n",
    "            ).long().to(device)\n",
    "            # get the intercepted dual hidden states.\n",
    "            for j in range(intervene_with_time):\n",
    "                dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "                    hmm_states=dual_high_hidden_states, \n",
    "                    hmm_actions=dual_high_actions, \n",
    "                    tag=\"_hmm_step_fxn\"\n",
    "                )\n",
    "            # main intervene for loop.\n",
    "            cf_high_hidden_states = high_hidden_states\n",
    "            cf_high_actions = high_actions\n",
    "            intervened_target_batch = [torch.ones(high_hidden_states.size(0), 1).long().to(device)] # SOS tokens\n",
    "            intervened_target_lengths_batch = torch.zeros(high_hidden_states.size(0), 1).long().to(device)\n",
    "            saved_intervened_high_hidden_states = None\n",
    "            # we need to take of the SOS and EOS tokens.\n",
    "            for j in range(train_max_decoding_steps-1):\n",
    "                # intercept like antra!\n",
    "                if j == intervene_time-1:\n",
    "                    # we need idle once by getting the states but not continue the HMM!\n",
    "                    cf_high_hidden_states, _ = hi_model(\n",
    "                        hmm_states=cf_high_hidden_states, \n",
    "                        hmm_actions=cf_high_actions, \n",
    "                        tag=\"_hmm_step_fxn\"\n",
    "                    )\n",
    "                    # we also include a probe loss.\n",
    "                    if include_cf_auxiliary_loss:\n",
    "                        true_target_positions = dual_high_hidden_states+5\n",
    "\n",
    "                    # only swap out this part.\n",
    "                    cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "                    saved_intervened_high_hidden_states = cf_high_hidden_states.clone()\n",
    "                    cf_high_actions = torch.zeros(\n",
    "                        dual_high_hidden_states.size(0), 1\n",
    "                    ).long().to(device)\n",
    "                    # comment out two lines below if it is not for testing.\n",
    "                    # cf_high_hidden_states = dual_high_hidden_states\n",
    "                    # cf_high_actions = dual_high_actions\n",
    "                cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "                    hmm_states=cf_high_hidden_states, \n",
    "                    hmm_actions=cf_high_actions, \n",
    "                    tag=\"_hmm_step_fxn\"\n",
    "                )\n",
    "                # record the output for loss calculation.\n",
    "                intervened_target_batch += [cf_high_actions]\n",
    "                intervened_target_lengths_batch += (cf_high_actions!=0).long()\n",
    "            intervened_target_lengths_batch += 2\n",
    "            # we do not to increase the EOS for non-ending sequences\n",
    "            for i in range(high_hidden_states.size(0)):\n",
    "                if intervened_target_lengths_batch[i,0] == train_max_decoding_steps+1:\n",
    "                    intervened_target_lengths_batch[i,0] = train_max_decoding_steps\n",
    "            intervened_target_batch = torch.cat(intervened_target_batch, dim=-1)\n",
    "            for i in range(high_hidden_states.size(0)):\n",
    "                if intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] == 0:\n",
    "                    intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] = 2\n",
    "            # intervened data.\n",
    "            intervened_target_batch = intervened_target_batch.clone()\n",
    "            intervened_target_lengths_batch = intervened_target_lengths_batch.clone()\n",
    "            # correct the length.\n",
    "            intervened_target_lengths_batch[intervened_target_lengths_batch>train_max_decoding_steps] = train_max_decoding_steps\n",
    "            \n",
    "            # Major update:\n",
    "            # We need to batch these operations.\n",
    "            intervened_scores_batch = []\n",
    "            # here, we only care where those intervened target is different\n",
    "            # from the original main target.\n",
    "            idx_generator = []\n",
    "            for i in range(batch_size):\n",
    "                match_target_intervened = intervened_target_batch[i,:intervened_target_lengths_batch[i,0]]\n",
    "                match_target_main = target_batch[i,:target_lengths_batch[i,0]]\n",
    "                # is_bad_intervened = torch.equal(match_target_intervened, match_target_main)\n",
    "                is_bad_intervened = (intervene_time>(target_lengths_batch[i,0]-2)) or \\\n",
    "                    (intervene_with_time>(dual_target_lengths_batch[i,0]-2))\n",
    "                if is_bad_intervened:\n",
    "                    continue # we need to skip these.\n",
    "                else:\n",
    "                    # we also need to make sure no testing time samples have been encountered before.\n",
    "                    if restrict_sampling == \"by_direction\":\n",
    "                        row_diff = saved_intervened_high_hidden_states[i][0]\n",
    "                        col_diff = saved_intervened_high_hidden_states[i][1]\n",
    "                        if row_diff > 0 and col_diff < 0:\n",
    "                            # skip these examples in new direction split.\n",
    "                            continue\n",
    "                        else:\n",
    "                            idx_generator += [i]\n",
    "                    elif restrict_sampling == \"by_length\":\n",
    "                        if intervened_target_lengths_batch[i,0] >= 12:\n",
    "                            # skip these examples in new action length split.\n",
    "                            continue\n",
    "                        else:\n",
    "                            idx_generator += [i]\n",
    "                    elif restrict_sampling == \"none\":\n",
    "                        idx_generator += [i]\n",
    "                    else:\n",
    "                        assert False\n",
    "\n",
    "            # Let us get rid of antra, using a very simple for loop\n",
    "            # to do this intervention.\n",
    "            idx_selected = []\n",
    "            if len(idx_generator) > 0:\n",
    "                # overwrite a bit.\n",
    "                cf_sample_per_batch = min(cf_sample_per_batch, len(idx_generator))\n",
    "                idx_selected = random.sample(idx_generator, k=cf_sample_per_batch)\n",
    "                intervened_target_batch_selected = []\n",
    "\n",
    "                # filter based on selection all together!\n",
    "                situation_batch = situation_batch[idx_selected]\n",
    "                input_batch = input_batch[idx_selected]\n",
    "                input_lengths_batch = input_lengths_batch[idx_selected]\n",
    "                dual_situation_batch = dual_situation_batch[idx_selected]\n",
    "                dual_input_batch = dual_input_batch[idx_selected]\n",
    "                dual_input_lengths_batch = dual_input_lengths_batch[idx_selected]\n",
    "                dual_target_batch = dual_target_batch[idx_selected]\n",
    "                intervened_target_lengths_batch = intervened_target_lengths_batch[idx_selected]\n",
    "                intervened_target_batch = intervened_target_batch[idx_selected]\n",
    "                if include_cf_auxiliary_loss:\n",
    "                    true_target_positions = true_target_positions[idx_selected]\n",
    "                \n",
    "                # we use the main hidden to track.\n",
    "                encoded_image = model(\n",
    "                    situations_input=situation_batch,\n",
    "                    tag=\"situation_encode\"\n",
    "                )\n",
    "                main_hidden, encoder_outputs = model(\n",
    "                    commands_input=input_batch, \n",
    "                    commands_lengths=input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict\"\n",
    "                )\n",
    "\n",
    "                main_hidden = model(\n",
    "                    command_hidden=main_hidden,\n",
    "                    tag=\"initialize_hidden\"\n",
    "                )\n",
    "                projected_keys_visual = model(\n",
    "                    encoded_situations=encoded_image,\n",
    "                    tag=\"projected_keys_visual\"\n",
    "                )\n",
    "                projected_keys_textual = model(\n",
    "                    command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "                    tag=\"projected_keys_textual\"\n",
    "                )\n",
    "\n",
    "                # dual setup.\n",
    "                dual_input_batch = dual_input_batch[:,:dual_input_max_seq_lens]\n",
    "                dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "                dual_encoded_image = model(\n",
    "                    situations_input=dual_situation_batch,\n",
    "                    tag=\"situation_encode\"\n",
    "                )\n",
    "                dual_hidden, dual_encoder_outputs = model(\n",
    "                    commands_input=dual_input_batch, \n",
    "                    commands_lengths=dual_input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict\"\n",
    "                )\n",
    "\n",
    "                dual_hidden = model(\n",
    "                    command_hidden=dual_hidden,\n",
    "                    tag=\"initialize_hidden\"\n",
    "                )\n",
    "                dual_projected_keys_visual = model(\n",
    "                    encoded_situations=dual_encoded_image,\n",
    "                    tag=\"projected_keys_visual\"\n",
    "                )\n",
    "                dual_projected_keys_textual = model(\n",
    "                    command_encoder_outputs=dual_encoder_outputs[\"encoder_outputs\"],\n",
    "                    tag=\"projected_keys_textual\"\n",
    "                )\n",
    "\n",
    "                # get the intercepted dual hidden states.\n",
    "                for j in range(intervene_with_time):\n",
    "                    (dual_output, dual_hidden) = model(\n",
    "                        lstm_input_tokens_sorted=dual_target_batch[:,j],\n",
    "                        lstm_hidden=dual_hidden,\n",
    "                        lstm_projected_keys_textual=dual_projected_keys_textual,\n",
    "                        lstm_commands_lengths=dual_input_lengths_batch,\n",
    "                        lstm_projected_keys_visual=dual_projected_keys_visual,\n",
    "                        tag=\"_lstm_step_fxn\"\n",
    "                    )\n",
    "                \n",
    "                # main intervene for loop.\n",
    "                cf_hidden = main_hidden\n",
    "                cf_outputs = []\n",
    "                for j in range(intervened_target_batch.shape[1]):\n",
    "                    cf_token=intervened_target_batch[:,j]\n",
    "                    # intercept like antra!\n",
    "                    if j == intervene_time-1:\n",
    "                        # we need idle once by getting the states but not continue the HMM!\n",
    "                        (_, cf_hidden) = model(\n",
    "                            lstm_input_tokens_sorted=cf_token,\n",
    "                            lstm_hidden=cf_hidden,\n",
    "                            lstm_projected_keys_textual=projected_keys_textual,\n",
    "                            lstm_commands_lengths=input_lengths_batch,\n",
    "                            lstm_projected_keys_visual=projected_keys_visual,\n",
    "                            tag=\"_lstm_step_fxn\"\n",
    "                        )\n",
    "                        # we also include a probe loss.\n",
    "                        if include_cf_auxiliary_loss:\n",
    "                            x_s_idx = 0\n",
    "                            y_s_idx = intervene_dimension_size\n",
    "                            y_e_idx = 2*intervene_dimension_size\n",
    "                            cf_target_positions_x = model(\n",
    "                                auxiliary_hidden=dual_hidden[0][:,:,x_s_idx:y_s_idx].squeeze(dim=1),\n",
    "                                cf_auxiliary_task_tag=\"x\",\n",
    "                                tag=\"cf_auxiliary_task\"\n",
    "                            )\n",
    "                            cf_target_positions_x = F.log_softmax(cf_target_positions_x, dim=-1)\n",
    "                            cf_target_positions_y = model(\n",
    "                                auxiliary_hidden=dual_hidden[0][:,:,y_s_idx:y_e_idx].squeeze(dim=1),\n",
    "                                cf_auxiliary_task_tag=\"x\",\n",
    "                                tag=\"cf_auxiliary_task\"\n",
    "                            )\n",
    "                            cf_target_positions_y = F.log_softmax(cf_target_positions_y, dim=-1)\n",
    "                            loss_position_x = model(\n",
    "                                loss_pred_target_auxiliary=cf_target_positions_x,\n",
    "                                loss_true_target_auxiliary=true_target_positions[:,0],\n",
    "                                tag=\"cf_auxiliary_task_loss\"\n",
    "                            )\n",
    "                            loss_position_y = model(\n",
    "                                loss_pred_target_auxiliary=cf_target_positions_y,\n",
    "                                loss_true_target_auxiliary=true_target_positions[:,1],\n",
    "                                tag=\"cf_auxiliary_task_loss\"\n",
    "                            )\n",
    "                            cf_position_loss = loss_position_x + loss_position_y\n",
    "                            if use_cuda and n_gpu > 1:\n",
    "                                cf_position_loss = cf_position_loss.mean() # mean() to average on multi-gpu.\n",
    "                            # some metrics\n",
    "                            metrics_position_x = model(\n",
    "                                loss_pred_target_auxiliary=cf_target_positions_x,\n",
    "                                loss_true_target_auxiliary=true_target_positions[:,0],\n",
    "                                tag=\"cf_auxiliary_task_metrics\"\n",
    "                            )\n",
    "                            metrics_position_y = model(\n",
    "                                loss_pred_target_auxiliary=cf_target_positions_y,\n",
    "                                loss_true_target_auxiliary=true_target_positions[:,1],\n",
    "                                tag=\"cf_auxiliary_task_metrics\"\n",
    "                            )\n",
    "                            \n",
    "                        # intervene!\n",
    "                        s_idx = intervene_attribute*intervene_dimension_size\n",
    "                        e_idx = (intervene_attribute+1)*intervene_dimension_size\n",
    "                        if intervene_method == \"cat\":\n",
    "                            updated_hidden = torch.cat(\n",
    "                                [\n",
    "                                   cf_hidden[0][:,:,:s_idx],\n",
    "                                   dual_hidden[0][:,:,s_idx:e_idx],\n",
    "                                   cf_hidden[0][:,:,e_idx:]\n",
    "                                ], dim=-1\n",
    "                            )\n",
    "                            cf_hidden = (updated_hidden, cf_hidden[1])\n",
    "                        elif intervene_method == \"inplace\":\n",
    "                            cf_hidden[0][:,:,s_idx:e_idx] = dual_hidden[0][:,:,s_idx:e_idx] # only swap out this part.\n",
    "                        else:\n",
    "                            assert False\n",
    "                        # WARNING: this is a special way to bypassing the state\n",
    "                        # updates during intervention!\n",
    "                        cf_token = None\n",
    "                    (cf_output, cf_hidden) = model(\n",
    "                        lstm_input_tokens_sorted=cf_token,\n",
    "                        lstm_hidden=cf_hidden,\n",
    "                        lstm_projected_keys_textual=projected_keys_textual,\n",
    "                        lstm_commands_lengths=input_lengths_batch,\n",
    "                        lstm_projected_keys_visual=projected_keys_visual,\n",
    "                        tag=\"_lstm_step_fxn\"\n",
    "                    )\n",
    "                    # record the output for loss calculation.\n",
    "                    cf_output = cf_output.unsqueeze(0)\n",
    "                    cf_outputs += [cf_output]\n",
    "                cf_outputs = torch.cat(cf_outputs, dim=0)\n",
    "                intervened_scores_batch = cf_outputs.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "                \n",
    "                # Counterfactual loss\n",
    "                intervened_scores_batch = F.log_softmax(intervened_scores_batch, dim=-1)\n",
    "                cf_loss = model(\n",
    "                    loss_target_scores=intervened_scores_batch, \n",
    "                    loss_target_batch=intervened_target_batch,\n",
    "                    tag=\"loss\"\n",
    "                )\n",
    "                if use_cuda and n_gpu > 1:\n",
    "                    cf_loss = cf_loss.mean() # mean() to average on multi-gpu.\n",
    "            \n",
    "            # LOSS COMBO\n",
    "            loss = task_loss\n",
    "            if include_cf_loss:\n",
    "                loss += cf_loss*cf_loss_weight \n",
    "            if include_cf_auxiliary_loss:\n",
    "                loss += cf_position_loss*cf_loss_weight\n",
    "\n",
    "            # Backward pass and update model parameters.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model(\n",
    "                is_best=is_best,\n",
    "                tag=\"update_state\"\n",
    "            )\n",
    "            \n",
    "            # Print current metrics.\n",
    "            if training_iteration % print_every == 0:\n",
    "                task_loss_to_write = task_loss.clone()\n",
    "                # main task evaluation\n",
    "                accuracy, exact_match = model(\n",
    "                    loss_target_scores=target_scores, \n",
    "                    loss_target_batch=target_batch,\n",
    "                    tag=\"get_metrics\"\n",
    "                )\n",
    "                # cf evaluation\n",
    "                cf_accuracy, cf_exact_match = model(\n",
    "                    loss_target_scores=intervened_scores_batch, \n",
    "                    loss_target_batch=intervened_target_batch,\n",
    "                    tag=\"get_metrics\"\n",
    "                )\n",
    "\n",
    "                auxiliary_accuracy_target = 0.\n",
    "                learning_rate = scheduler.get_lr()[0]\n",
    "                logger.info(\"Iteration %08d, task loss %8.4f, cf loss %8.4f, cf pos loss %8.4f, accuracy %5.2f, exact match %5.2f, \"\n",
    "                            \"cf count %03d, cf accuracy %5.2f, cf exact match %5.2f, cf pos_x %5.2f, cf pos_y %5.2f,\"\n",
    "                            \" learning_rate %.5f\" % (\n",
    "                                training_iteration, task_loss_to_write, cf_loss, cf_position_loss, accuracy, exact_match,\n",
    "                                len(idx_selected), cf_accuracy, cf_exact_match, metrics_position_x, metrics_position_y,\n",
    "                                learning_rate,\n",
    "                            ))\n",
    "                # logging to wandb.\n",
    "                if is_wandb:\n",
    "                    wandb.log({'train/training_iteration': training_iteration})\n",
    "                    wandb.log({'train/task_loss': task_loss_to_write})\n",
    "                    wandb.log({'train/task_accuracy': accuracy})\n",
    "                    wandb.log({'train/task_exact_match': exact_match})\n",
    "                    if cf_loss and len(idx_selected) != 0:\n",
    "                        wandb.log({'train/counterfactual_loss': cf_loss})\n",
    "                        wandb.log({'train/counterfactual count': len(idx_selected)})\n",
    "                        wandb.log({'train/counterfactual_accuracy': cf_accuracy})\n",
    "                        wandb.log({'train/counterfactual_exact_match': cf_exact_match})\n",
    "                    if include_cf_auxiliary_loss and len(idx_selected) != 0:\n",
    "                        wandb.log({'train/pred_target_position_x': metrics_position_y})\n",
    "                        wandb.log({'train/pred_target_position_y': metrics_position_y})\n",
    "                    wandb.log({'train/learning_rate': learning_rate})\n",
    "                    \n",
    "            # Evaluate on test set.\n",
    "            if training_iteration % evaluate_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    logger.info(\"Evaluating..\")\n",
    "                    accuracy, exact_match, target_accuracy = evaluate(\n",
    "                        test_dataloader, model=model,\n",
    "                        max_decoding_steps=max_decoding_steps, pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                        sos_idx=test_set.target_vocabulary.sos_idx,\n",
    "                        eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                        max_examples_to_evaluate=kwargs[\"max_testing_examples\"],\n",
    "                        device=device\n",
    "                    )\n",
    "                    logger.info(\"  Evaluation Accuracy: %5.2f Exact Match: %5.2f \"\n",
    "                                \" Target Accuracy: %5.2f\" % (accuracy, exact_match, target_accuracy))\n",
    "                    if is_wandb:\n",
    "                        wandb.log({'eval/accuracy': accuracy})\n",
    "                        wandb.log({'eval/exact_match': exact_match})\n",
    "                    if exact_match > best_exact_match:\n",
    "                        is_best = True\n",
    "                        best_accuracy = accuracy\n",
    "                        best_exact_match = exact_match\n",
    "                        model(\n",
    "                            accuracy=accuracy, exact_match=exact_match, \n",
    "                            is_best=is_best,\n",
    "                            tag=\"update_state\"\n",
    "                        )\n",
    "                    file_name = f\"checkpoint-{training_iteration}.pth.tar\"\n",
    "                    model.save_checkpoint(file_name=file_name, is_best=is_best,\n",
    "                                          optimizer_state_dict=optimizer.state_dict())\n",
    "                \n",
    "            training_iteration += 1\n",
    "            if training_iteration > max_training_iterations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags, args):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        train(data_path=data_path, args=args, **flags)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(input_flags, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
