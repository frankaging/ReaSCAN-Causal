{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "\n",
    "from decode_graphical_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "\n",
    "):\n",
    "    pass\n",
    "\n",
    "def evaluate(\n",
    "    data_iterator,\n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx,\n",
    "    sos_idx,\n",
    "    eos_idx,\n",
    "    max_examples_to_evaluate\n",
    "):\n",
    "    accuracies = []\n",
    "    target_accuracies = []\n",
    "    exact_match = 0\n",
    "    for input_sequence, _, _, output_sequence, target_sequence, _, _, aux_acc_target in predict(\n",
    "            data_iterator=data_iterator, model=model, max_decoding_steps=max_decoding_steps, pad_idx=pad_idx,\n",
    "            sos_idx=sos_idx, eos_idx=eos_idx, max_examples_to_evaluate=max_examples_to_evaluate):\n",
    "        accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "        if accuracy == 100:\n",
    "            exact_match += 1\n",
    "        accuracies.append(accuracy)\n",
    "        target_accuracies.append(aux_acc_target)\n",
    "    return (float(np.mean(np.array(accuracies))), (exact_match / len(accuracies)) * 100,\n",
    "            float(np.mean(np.array(target_accuracies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_path: str, \n",
    "    data_directory: str, \n",
    "    generate_vocabularies: bool, \n",
    "    input_vocab_path: str,   \n",
    "    target_vocab_path: str, \n",
    "    embedding_dimension: int, \n",
    "    num_encoder_layers: int, \n",
    "    encoder_dropout_p: float,\n",
    "    encoder_bidirectional: bool, \n",
    "    training_batch_size: int, \n",
    "    test_batch_size: int, \n",
    "    max_decoding_steps: int,\n",
    "    num_decoder_layers: int, \n",
    "    decoder_dropout_p: float, \n",
    "    cnn_kernel_size: int, \n",
    "    cnn_dropout_p: float,\n",
    "    cnn_hidden_num_channels: int, \n",
    "    simple_situation_representation: bool, \n",
    "    decoder_hidden_size: int,\n",
    "    encoder_hidden_size: int, \n",
    "    learning_rate: float, \n",
    "    adam_beta_1: float, \n",
    "    adam_beta_2: float, \n",
    "    lr_decay: float,\n",
    "    lr_decay_steps: int, \n",
    "    resume_from_file: str, \n",
    "    max_training_iterations: int, \n",
    "    output_directory: str,\n",
    "    print_every: int, \n",
    "    evaluate_every: int, \n",
    "    conditional_attention: bool, \n",
    "    auxiliary_task: bool,\n",
    "    weight_target_loss: float, \n",
    "    attention_type: str, \n",
    "    k: int, \n",
    "    max_training_examples=None, \n",
    "    seed=42, **kwargs\n",
    "):\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    logger.info(\"Loading all data into memory...\")\n",
    "    logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "    data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "    logger.info(\"Loading Training set...\")\n",
    "    training_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"train\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=k\n",
    "    )\n",
    "    training_set.read_dataset(\n",
    "        max_examples=max_training_examples,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "    logger.info(\"Done Loading Training set.\")\n",
    "    logger.info(\"  Loaded {} training examples.\".format(training_set.num_examples))\n",
    "    logger.info(\"  Input vocabulary size training set: {}\".format(training_set.input_vocabulary_size))\n",
    "    logger.info(\"  Most common input words: {}\".format(training_set.input_vocabulary.most_common(5)))\n",
    "    logger.info(\"  Output vocabulary size training set: {}\".format(training_set.target_vocabulary_size))\n",
    "    logger.info(\"  Most common target words: {}\".format(training_set.target_vocabulary.most_common(5)))\n",
    "\n",
    "    if generate_vocabularies:\n",
    "        training_set.save_vocabularies(input_vocab_path, target_vocab_path)\n",
    "        logger.info(\"Saved vocabularies to {} for input and {} for target.\".format(input_vocab_path, target_vocab_path))\n",
    "\n",
    "    logger.info(\"Loading Dev. set...\")\n",
    "    test_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"dev\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=0\n",
    "    )\n",
    "    test_set.read_dataset(\n",
    "        max_examples=None,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "\n",
    "    # Shuffle the test set to make sure that if we only evaluate max_testing_examples we get a random part of the set.\n",
    "    test_set.shuffle_data()\n",
    "    logger.info(\"Done Loading Dev. set.\")\n",
    "    \n",
    "    # create modell based on our dataset.\n",
    "    model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "                  target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "                  num_cnn_channels=training_set.image_channels,\n",
    "                  input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "                  target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "                  target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "                  **cfg)\n",
    "    \n",
    "    # gpu setups\n",
    "    use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "\n",
    "    # optimizer\n",
    "    log_parameters(model)\n",
    "    trainable_parameters = [parameter for parameter in model.parameters() if parameter.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_parameters, lr=learning_rate, betas=(adam_beta_1, adam_beta_2))\n",
    "    scheduler = LambdaLR(optimizer,\n",
    "                         lr_lambda=lambda t: lr_decay ** (t / lr_decay_steps))\n",
    "    \n",
    "    \n",
    "    # Load model and vocabularies if resuming.\n",
    "    start_iteration = 1\n",
    "    best_iteration = 1\n",
    "    best_accuracy = 0\n",
    "    best_exact_match = -99\n",
    "    best_loss = float('inf')\n",
    "    if resume_from_file:\n",
    "        assert os.path.isfile(resume_from_file), \"No checkpoint found at {}\".format(resume_from_file)\n",
    "        logger.info(\"Loading checkpoint from file at '{}'\".format(resume_from_file))\n",
    "        optimizer_state_dict = model.load_model(resume_from_file)\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "        start_iteration = model.trained_iterations\n",
    "        logger.info(\"Loaded checkpoint '{}' (iter {})\".format(resume_from_file, start_iteration))\n",
    "    \n",
    "    # Loading dataset and preprocessing a bit.\n",
    "    train_data, _ = training_set.get_dataset()\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.training_batch_size)\n",
    "    test_data, _ = test_set.get_dataset()\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False)\n",
    "    \n",
    "    if use_cuda and n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # graphical model\n",
    "    train_max_decoding_steps = int(training_set.get_max_seq_length_target())\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    logger.info(f\"MAX_DECODING_STEPS for Training: {train_max_decoding_steps}\")\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    g_model = ReaSCANMultiModalLSTMCompGraph(\n",
    "         model,\n",
    "         train_max_decoding_steps,\n",
    "         cache_results=False\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Training starts..\")\n",
    "    training_iteration = start_iteration\n",
    "    while training_iteration < max_training_iterations:\n",
    "\n",
    "        # Shuffle the dataset and loop over it.\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            input_batch, target_batch, situation_batch, \\\n",
    "                agent_positions_batch, target_positions_batch, \\\n",
    "                input_lengths_batch, target_lengths_batch = batch\n",
    "            is_best = False\n",
    "            model.train()\n",
    "            \n",
    "            input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "            target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            situation_batch = situation_batch.to(device)\n",
    "            agent_positions_batch = agent_positions_batch.to(device)\n",
    "            target_positions_batch = target_positions_batch.to(device)\n",
    "            input_lengths_batch = input_lengths_batch.to(device)\n",
    "            target_lengths_batch = target_lengths_batch.to(device)\n",
    "            \n",
    "            # Instead of calling forward(), we call the graph model wrapper.\n",
    "            input_dict = {\n",
    "                \"commands_input\": input_batch, \n",
    "                \"commands_lengths\": input_lengths_batch,\n",
    "                \"situations_input\": situation_batch,\n",
    "                \"target_batch\": target_batch,\n",
    "                \"target_lengths\": target_lengths_batch,\n",
    "            }\n",
    "            g_input_dict = GraphInput(input_dict, batched=True, batch_dim=0, cache_results=False)\n",
    "            target_scores = g_model.compute(g_input_dict)\n",
    "            loss = model.get_loss(target_scores, target_batch)\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            # we need to average over actual length to get rid of padding losses.\n",
    "            loss /= target_lengths_batch.sum()\n",
    "            \n",
    "            if auxiliary_task:\n",
    "                target_loss = 0\n",
    "                pass\n",
    "                # TODO: implement this.\n",
    "            else:\n",
    "                target_loss = 0\n",
    "            loss += weight_target_loss * target_loss\n",
    "            # Backward pass and update model parameters.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.update_state(is_best=is_best)\n",
    "            \n",
    "            # Print current metrics.\n",
    "            if training_iteration % print_every == 0:\n",
    "                accuracy, exact_match = model.get_metrics(target_scores, target_batch)\n",
    "                if auxiliary_task:\n",
    "                    pass\n",
    "                # TODO: implement this.\n",
    "                else:\n",
    "                    auxiliary_accuracy_target = 0.\n",
    "                learning_rate = scheduler.get_lr()[0]\n",
    "                logger.info(\"Iteration %08d, loss %8.4f, accuracy %5.2f, exact match %5.2f, learning_rate %.5f,\"\n",
    "                            \" aux. accuracy target pos %5.2f\" % (training_iteration, loss, accuracy, exact_match,\n",
    "                                                                 learning_rate, auxiliary_accuracy_target))\n",
    "\n",
    "            # Evaluate on test set.\n",
    "            if training_iteration % evaluate_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    logger.info(\"Evaluating..\")\n",
    "                    accuracy, exact_match, target_accuracy = evaluate(\n",
    "                        test_dataloader, model=model,\n",
    "                        max_decoding_steps=max_decoding_steps, pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                        sos_idx=test_set.target_vocabulary.sos_idx,\n",
    "                        eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                        max_examples_to_evaluate=kwargs[\"max_testing_examples\"])\n",
    "                    logger.info(\"  Evaluation Accuracy: %5.2f Exact Match: %5.2f \"\n",
    "                                \" Target Accuracy: %5.2f\" % (accuracy, exact_match, target_accuracy))\n",
    "                    if exact_match > best_exact_match:\n",
    "                        is_best = True\n",
    "                        best_accuracy = accuracy\n",
    "                        best_exact_match = exact_match\n",
    "                        model.update_state(accuracy=accuracy, exact_match=exact_match, is_best=is_best)\n",
    "                    file_name = \"checkpoint.pth.tar\".format(str(training_iteration))\n",
    "                    if is_best:\n",
    "                        model.save_checkpoint(file_name=file_name, is_best=is_best,\n",
    "                                              optimizer_state_dict=optimizer.state_dict())\n",
    "                \n",
    "            training_iteration += 1\n",
    "            if training_iteration > max_training_iterations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        train(data_path=data_path, **flags)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 01:07 mode: train\n",
      "2021-08-04 01:07 output_directory: output\n",
      "2021-08-04 01:07 resume_from_file: \n",
      "2021-08-04 01:07 split: test\n",
      "2021-08-04 01:07 data_directory: ../../../data-files/ReaSCAN-Simple/\n",
      "2021-08-04 01:07 input_vocab_path: input_vocabulary.txt\n",
      "2021-08-04 01:07 target_vocab_path: target_vocabulary.txt\n",
      "2021-08-04 01:07 generate_vocabularies: False\n",
      "2021-08-04 01:07 training_batch_size: 50\n",
      "2021-08-04 01:07 k: 0\n",
      "2021-08-04 01:07 test_batch_size: 1\n",
      "2021-08-04 01:07 max_training_examples: 100\n",
      "2021-08-04 01:07 learning_rate: 0.001\n",
      "2021-08-04 01:07 lr_decay: 0.9\n",
      "2021-08-04 01:07 lr_decay_steps: 20000\n",
      "2021-08-04 01:07 adam_beta_1: 0.9\n",
      "2021-08-04 01:07 adam_beta_2: 0.999\n",
      "2021-08-04 01:07 print_every: 1\n",
      "2021-08-04 01:07 evaluate_every: 1000\n",
      "2021-08-04 01:07 max_training_iterations: 5\n",
      "2021-08-04 01:07 weight_target_loss: 0.3\n",
      "2021-08-04 01:07 max_testing_examples: None\n",
      "2021-08-04 01:07 splits: test\n",
      "2021-08-04 01:07 max_decoding_steps: 30\n",
      "2021-08-04 01:07 output_file_name: predict.json\n",
      "2021-08-04 01:07 simple_situation_representation: True\n",
      "2021-08-04 01:07 cnn_hidden_num_channels: 50\n",
      "2021-08-04 01:07 cnn_kernel_size: 7\n",
      "2021-08-04 01:07 cnn_dropout_p: 0.1\n",
      "2021-08-04 01:07 auxiliary_task: False\n",
      "2021-08-04 01:07 embedding_dimension: 25\n",
      "2021-08-04 01:07 num_encoder_layers: 1\n",
      "2021-08-04 01:07 encoder_hidden_size: 100\n",
      "2021-08-04 01:07 encoder_dropout_p: 0.3\n",
      "2021-08-04 01:07 encoder_bidirectional: True\n",
      "2021-08-04 01:07 num_decoder_layers: 1\n",
      "2021-08-04 01:07 attention_type: bahdanau\n",
      "2021-08-04 01:07 decoder_dropout_p: 0.3\n",
      "2021-08-04 01:07 decoder_hidden_size: 100\n",
      "2021-08-04 01:07 conditional_attention: True\n",
      "2021-08-04 01:07 seed: 42\n",
      "2021-08-04 01:07 Loading all data into memory...\n",
      "2021-08-04 01:07 Reading dataset from file: ../../../data-files/ReaSCAN-Simple/data-compositional-splits.txt...\n",
      "2021-08-04 01:07 Loading Training set...\n",
      "2021-08-04 01:07 Formulating the dataset from the passed in json file...\n",
      "2021-08-04 01:07 Loading vocabularies...\n",
      "2021-08-04 01:07 Done loading vocabularies.\n",
      "2021-08-04 01:07 Converting dataset to tensors...\n",
      "2021-08-04 01:07 Done Loading Training set.\n",
      "2021-08-04 01:07   Loaded 100 training examples.\n",
      "2021-08-04 01:07   Input vocabulary size training set: 20\n",
      "2021-08-04 01:07   Most common input words: [('walk', 38037), ('to', 38037), ('the', 38037), ('while', 15241), ('small', 12693)]\n",
      "2021-08-04 01:07   Output vocabulary size training set: 7\n",
      "2021-08-04 01:07   Most common target words: [('turn left', 235252), ('walk', 152287), ('turn right', 81124), ('stay', 30525)]\n",
      "2021-08-04 01:07 Loading Dev. set...\n",
      "2021-08-04 01:07 Formulating the dataset from the passed in json file...\n",
      "2021-08-04 01:07 Loading vocabularies...\n",
      "2021-08-04 01:07 Done loading vocabularies.\n",
      "2021-08-04 01:07 Converting dataset to tensors...\n",
      "2021-08-04 01:07 Done Loading Dev. set.\n",
      "/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "2021-08-04 01:07 device: cpu, and we recognize 10 gpu(s) in total.\n",
      "2021-08-04 01:07 Total parameters: 443600\n",
      "2021-08-04 01:07 situation_encoder.conv_1.weight : [50, 17, 1, 1]\n",
      "2021-08-04 01:07 situation_encoder.conv_1.bias : [50]\n",
      "2021-08-04 01:07 situation_encoder.conv_2.weight : [50, 17, 5, 5]\n",
      "2021-08-04 01:07 situation_encoder.conv_2.bias : [50]\n",
      "2021-08-04 01:07 situation_encoder.conv_3.weight : [50, 17, 7, 7]\n",
      "2021-08-04 01:07 situation_encoder.conv_3.bias : [50]\n",
      "2021-08-04 01:07 visual_attention.key_layer.weight : [100, 150]\n",
      "2021-08-04 01:07 visual_attention.query_layer.weight : [100, 100]\n",
      "2021-08-04 01:07 visual_attention.energy_layer.weight : [1, 100]\n",
      "2021-08-04 01:07 encoder.embedding.weight : [20, 25]\n",
      "2021-08-04 01:07 encoder.lstm.weight_ih_l0 : [400, 25]\n",
      "2021-08-04 01:07 encoder.lstm.weight_hh_l0 : [400, 100]\n",
      "2021-08-04 01:07 encoder.lstm.bias_ih_l0 : [400]\n",
      "2021-08-04 01:07 encoder.lstm.bias_hh_l0 : [400]\n",
      "2021-08-04 01:07 encoder.lstm.weight_ih_l0_reverse : [400, 25]\n",
      "2021-08-04 01:07 encoder.lstm.weight_hh_l0_reverse : [400, 100]\n",
      "2021-08-04 01:07 encoder.lstm.bias_ih_l0_reverse : [400]\n",
      "2021-08-04 01:07 encoder.lstm.bias_hh_l0_reverse : [400]\n",
      "2021-08-04 01:07 enc_hidden_to_dec_hidden.weight : [100, 100]\n",
      "2021-08-04 01:07 enc_hidden_to_dec_hidden.bias : [100]\n",
      "2021-08-04 01:07 textual_attention.key_layer.weight : [100, 100]\n",
      "2021-08-04 01:07 textual_attention.query_layer.weight : [100, 100]\n",
      "2021-08-04 01:07 textual_attention.energy_layer.weight : [1, 100]\n",
      "2021-08-04 01:07 attention_decoder.queries_to_keys.weight : [100, 200]\n",
      "2021-08-04 01:07 attention_decoder.queries_to_keys.bias : [100]\n",
      "2021-08-04 01:07 attention_decoder.embedding.weight : [7, 100]\n",
      "2021-08-04 01:07 attention_decoder.lstm.weight_ih_l0 : [400, 300]\n",
      "2021-08-04 01:07 attention_decoder.lstm.weight_hh_l0 : [400, 100]\n",
      "2021-08-04 01:07 attention_decoder.lstm.bias_ih_l0 : [400]\n",
      "2021-08-04 01:07 attention_decoder.lstm.bias_hh_l0 : [400]\n",
      "2021-08-04 01:07 attention_decoder.output_to_hidden.weight : [100, 400]\n",
      "2021-08-04 01:07 attention_decoder.hidden_to_output.weight : [7, 100]\n",
      "2021-08-04 01:07 ==== WARNING ====\n",
      "2021-08-04 01:07 MAX_DECODING_STEPS for Training: 40\n",
      "2021-08-04 01:07 ==== WARNING ====\n",
      "2021-08-04 01:07 Training starts..\n",
      "/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "2021-08-04 01:07 Iteration 00000001, loss   0.0024, accuracy 12.42, exact match  0.00, learning_rate 0.00100, aux. accuracy target pos  0.00\n",
      "2021-08-04 01:07 Iteration 00000002, loss   0.0024, accuracy 38.33, exact match  0.00, learning_rate 0.00100, aux. accuracy target pos  0.00\n",
      "2021-08-04 01:07 Iteration 00000003, loss   0.0022, accuracy 47.06, exact match  0.00, learning_rate 0.00100, aux. accuracy target pos  0.00\n",
      "2021-08-04 01:07 Iteration 00000004, loss   0.0018, accuracy 46.38, exact match  0.00, learning_rate 0.00100, aux. accuracy target pos  0.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 100\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
