{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "\n",
    "from seq2seq.model import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(data_iterator):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "        yield (input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_iterator,\n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx,\n",
    "    sos_idx,\n",
    "    eos_idx,\n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    "):\n",
    "    accuracies = []\n",
    "    target_accuracies = []\n",
    "    exact_match = 0\n",
    "    for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "            data_iterator=data_iterator, model=model, max_decoding_steps=max_decoding_steps, pad_idx=pad_idx,\n",
    "            sos_idx=sos_idx, eos_idx=eos_idx, max_examples_to_evaluate=max_examples_to_evaluate, device=device):\n",
    "        accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "        if accuracy == 100:\n",
    "            exact_match += 1\n",
    "        accuracies.append(accuracy)\n",
    "        target_accuracies.append(aux_acc_target)\n",
    "    return (float(np.mean(np.array(accuracies))), (exact_match / len(accuracies)) * 100,\n",
    "            float(np.mean(np.array(target_accuracies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_path: str, \n",
    "    data_directory: str, \n",
    "    generate_vocabularies: bool, \n",
    "    input_vocab_path: str,   \n",
    "    target_vocab_path: str, \n",
    "    embedding_dimension: int, \n",
    "    num_encoder_layers: int, \n",
    "    encoder_dropout_p: float,\n",
    "    encoder_bidirectional: bool, \n",
    "    training_batch_size: int, \n",
    "    test_batch_size: int, \n",
    "    max_decoding_steps: int,\n",
    "    num_decoder_layers: int, \n",
    "    decoder_dropout_p: float, \n",
    "    cnn_kernel_size: int, \n",
    "    cnn_dropout_p: float,\n",
    "    cnn_hidden_num_channels: int, \n",
    "    simple_situation_representation: bool, \n",
    "    decoder_hidden_size: int,\n",
    "    encoder_hidden_size: int, \n",
    "    learning_rate: float, \n",
    "    adam_beta_1: float, \n",
    "    adam_beta_2: float, \n",
    "    lr_decay: float,\n",
    "    lr_decay_steps: int, \n",
    "    resume_from_file: str, \n",
    "    max_training_iterations: int, \n",
    "    output_directory: str,\n",
    "    print_every: int, \n",
    "    evaluate_every: int, \n",
    "    conditional_attention: bool, \n",
    "    auxiliary_task: bool,\n",
    "    weight_target_loss: float, \n",
    "    attention_type: str, \n",
    "    k: int, \n",
    "    is_wandb: bool,\n",
    "    max_training_examples=None, \n",
    "    seed=42, **kwargs\n",
    "):\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    from pathlib import Path\n",
    "    # the output directory name is generated on-the-fly.\n",
    "    dataset_name = data_directory.strip(\"/\").split(\"/\")[-1]\n",
    "    run_name = f\"mmlstm_{dataset_name}_seed_{seed}_lr_{learning_rate}\"\n",
    "    output_directory = os.path.join(output_directory, run_name)\n",
    "    cfg[\"output_directory\"] = output_directory\n",
    "    logger.info(f\"Create the output directory if not exist: {output_directory}\")\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # initialize w&b in the beginning.\n",
    "    if is_wandb:\n",
    "        logger.warning(\"Enabling wandb for tensorboard logging...\")\n",
    "        import wandb\n",
    "        run = wandb.init(\n",
    "            project=\"ReaSCAN-Causal\", \n",
    "            entity=\"wuzhengx\",\n",
    "            name=run_name,\n",
    "        )\n",
    "        wandb.config.update(args)\n",
    "    \n",
    "    logger.info(\"Loading all data into memory...\")\n",
    "    logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "    data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "    logger.info(\"Loading Training set...\")\n",
    "    training_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"train\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=k\n",
    "    )\n",
    "    training_set.read_dataset(\n",
    "        max_examples=max_training_examples,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "    logger.info(\"Done Loading Training set.\")\n",
    "    logger.info(\"  Loaded {} training examples.\".format(training_set.num_examples))\n",
    "    logger.info(\"  Input vocabulary size training set: {}\".format(training_set.input_vocabulary_size))\n",
    "    logger.info(\"  Most common input words: {}\".format(training_set.input_vocabulary.most_common(5)))\n",
    "    logger.info(\"  Output vocabulary size training set: {}\".format(training_set.target_vocabulary_size))\n",
    "    logger.info(\"  Most common target words: {}\".format(training_set.target_vocabulary.most_common(5)))\n",
    "\n",
    "    if generate_vocabularies:\n",
    "        training_set.save_vocabularies(input_vocab_path, target_vocab_path)\n",
    "        logger.info(\"Saved vocabularies to {} for input and {} for target.\".format(input_vocab_path, target_vocab_path))\n",
    "\n",
    "    logger.info(\"Loading Dev. set...\")\n",
    "    test_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"dev\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=0\n",
    "    )\n",
    "    test_set.read_dataset(\n",
    "        max_examples=None,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "\n",
    "    # Shuffle the test set to make sure that if we only evaluate max_testing_examples we get a random part of the set.\n",
    "    test_set.shuffle_data()\n",
    "    logger.info(\"Done Loading Dev. set.\")\n",
    "    \n",
    "    # some important variables.\n",
    "    grid_size = training_set.grid_size\n",
    "    target_position_size = 2*grid_size - 1\n",
    "    \n",
    "    # create modell based on our dataset.\n",
    "    model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "                  target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "                  num_cnn_channels=training_set.image_channels,\n",
    "                  input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "                  target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "                  target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "                  target_position_size=target_position_size,\n",
    "                  intervene_dimension_size=25, # this is dummy.\n",
    "                  **cfg)\n",
    "    \n",
    "    # gpu setups\n",
    "    use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "\n",
    "    # optimizer\n",
    "    log_parameters(model)\n",
    "    trainable_parameters = [parameter for parameter in model.parameters() if parameter.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_parameters, lr=learning_rate, betas=(adam_beta_1, adam_beta_2))\n",
    "    scheduler = LambdaLR(optimizer,\n",
    "                         lr_lambda=lambda t: lr_decay ** (t / lr_decay_steps))\n",
    "    \n",
    "    \n",
    "    # Load model and vocabularies if resuming.\n",
    "    start_iteration = 1\n",
    "    best_iteration = 1\n",
    "    best_accuracy = 0\n",
    "    best_exact_match = -99\n",
    "    best_loss = float('inf')\n",
    "    if resume_from_file:\n",
    "        assert os.path.isfile(resume_from_file), \"No checkpoint found at {}\".format(resume_from_file)\n",
    "        logger.info(\"Loading checkpoint from file at '{}'\".format(resume_from_file))\n",
    "        optimizer_state_dict = model.load_model(resume_from_file)\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "        start_iteration = model.trained_iterations\n",
    "        logger.info(\"Loaded checkpoint '{}' (iter {})\".format(resume_from_file, start_iteration))\n",
    "    \n",
    "    # Loading dataset and preprocessing a bit.\n",
    "    train_data, _ = training_set.get_dataset()\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.training_batch_size)\n",
    "    test_data, _ = test_set.get_dataset()\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False)\n",
    "    \n",
    "    if use_cuda and n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # graphical model\n",
    "    train_max_decoding_steps = int(training_set.get_max_seq_length_target())\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    logger.info(f\"MAX_DECODING_STEPS for Training: {train_max_decoding_steps}\")\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "\n",
    "    logger.info(\"Training starts..\")\n",
    "    training_iteration = start_iteration\n",
    "    while training_iteration < max_training_iterations:\n",
    "\n",
    "        # Shuffle the dataset and loop over it.\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            input_batch, target_batch, situation_batch, \\\n",
    "                agent_positions_batch, target_positions_batch, \\\n",
    "                input_lengths_batch, target_lengths_batch = batch\n",
    "            is_best = False\n",
    "            model.train()\n",
    "            \n",
    "            input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "            target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            situation_batch = situation_batch.to(device)\n",
    "            agent_positions_batch = agent_positions_batch.to(device)\n",
    "            target_positions_batch = target_positions_batch.to(device)\n",
    "            input_lengths_batch = input_lengths_batch.to(device)\n",
    "            target_lengths_batch = target_lengths_batch.to(device)\n",
    "            \n",
    "            # we use the main hidden to track.\n",
    "            encoded_image = model(\n",
    "                situations_input=situation_batch,\n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "            hidden, encoder_outputs = model(\n",
    "                commands_input=input_batch, \n",
    "                commands_lengths=input_lengths_batch,\n",
    "                tag=\"command_input_encode_no_dict\"\n",
    "            )\n",
    "            hidden = model(\n",
    "                command_hidden=hidden,\n",
    "                tag=\"initialize_hidden\"\n",
    "            )\n",
    "            projected_keys_visual = model(\n",
    "                encoded_situations=encoded_image,\n",
    "                tag=\"projected_keys_visual\"\n",
    "            )\n",
    "            projected_keys_textual = model(\n",
    "                command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "                tag=\"projected_keys_textual\"\n",
    "            )\n",
    "            outputs = []\n",
    "            for j in range(train_max_decoding_steps):\n",
    "                token = target_batch[:,j]\n",
    "                (output, hidden) = model(\n",
    "                    lstm_input_tokens_sorted=token,\n",
    "                    lstm_hidden=hidden,\n",
    "                    lstm_projected_keys_textual=projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "                output = F.log_softmax(output, dim=-1)\n",
    "                outputs += [output]\n",
    "            target_scores = torch.stack(outputs, dim=1)\n",
    "            loss = model(\n",
    "                loss_target_scores=target_scores, \n",
    "                loss_target_batch=target_batch,\n",
    "                tag=\"loss\"\n",
    "            )\n",
    "                \n",
    "            if use_cuda and n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            # we need to average over actual length to get rid of padding losses.\n",
    "            # loss /= target_lengths_batch.sum()\n",
    "            if auxiliary_task:\n",
    "                target_loss = 0\n",
    "                pass\n",
    "            else:\n",
    "                target_loss = 0\n",
    "            loss += weight_target_loss * target_loss\n",
    "            # Backward pass and update model parameters.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model(\n",
    "                is_best=is_best,\n",
    "                tag=\"update_state\"\n",
    "            )\n",
    "            \n",
    "            # Print current metrics.\n",
    "            if training_iteration % print_every == 0:\n",
    "                accuracy, exact_match = model(\n",
    "                    loss_target_scores=target_scores, \n",
    "                    loss_target_batch=target_batch,\n",
    "                    tag=\"get_metrics\"\n",
    "                )\n",
    "                if auxiliary_task:\n",
    "                    pass\n",
    "                else:\n",
    "                    auxiliary_accuracy_target = 0.\n",
    "                learning_rate = scheduler.get_lr()[0]\n",
    "                logger.info(\"Iteration %08d, loss %8.4f, accuracy %5.2f, exact match %5.2f, learning_rate %.5f,\"\n",
    "                            \" aux. accuracy target pos %5.2f\" % (training_iteration, loss, accuracy, exact_match,\n",
    "                                                                 learning_rate, auxiliary_accuracy_target))\n",
    "                if is_wandb:\n",
    "                    wandb.log({'train/training_iteration': training_iteration})\n",
    "                    wandb.log({'train/task_loss': loss})\n",
    "                    wandb.log({'train/task_accuracy': accuracy})\n",
    "                    wandb.log({'train/task_exact_match': exact_match})\n",
    "                    \n",
    "            # Evaluate on test set.\n",
    "            if training_iteration % evaluate_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    logger.info(\"Evaluating..\")\n",
    "                    accuracy, exact_match, target_accuracy = evaluate(\n",
    "                        test_dataloader, model=model,\n",
    "                        max_decoding_steps=max_decoding_steps, pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                        sos_idx=test_set.target_vocabulary.sos_idx,\n",
    "                        eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                        max_examples_to_evaluate=kwargs[\"max_testing_examples\"],\n",
    "                        device=device\n",
    "                    )\n",
    "                    logger.info(\"  Evaluation Accuracy: %5.2f Exact Match: %5.2f \"\n",
    "                                \" Target Accuracy: %5.2f\" % (accuracy, exact_match, target_accuracy))\n",
    "                    if is_wandb:\n",
    "                        wandb.log({'eval/accuracy': accuracy})\n",
    "                        wandb.log({'eval/exact_match': exact_match})\n",
    "                    if exact_match > best_exact_match:\n",
    "                        is_best = True\n",
    "                        best_accuracy = accuracy\n",
    "                        best_exact_match = exact_match\n",
    "                        model(\n",
    "                            accuracy=accuracy, exact_match=exact_match, \n",
    "                            is_best=is_best,\n",
    "                            tag=\"update_state\"\n",
    "                        )\n",
    "                    file_name = f\"checkpoint-{training_iteration}.pth.tar\"\n",
    "                    model.save_checkpoint(file_name=file_name, is_best=is_best,\n",
    "                                          optimizer_state_dict=optimizer.state_dict())\n",
    "                \n",
    "            training_iteration += 1\n",
    "            if training_iteration > max_training_iterations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        train(data_path=data_path, **flags)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
