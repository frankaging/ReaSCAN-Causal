{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/gSCAN-Simple/data-compositional-splits.txt\"\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 200\n",
    "agent_positions_batch = []\n",
    "target_positions_batch = []\n",
    "target_commands = []\n",
    "for ex in data_json[\"examples\"][\"situational_1\"]:\n",
    "    target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    agent_positions_batch.append(agent)\n",
    "    target_positions_batch.append(target)\n",
    "    if len(agent_positions_batch) == NUM:\n",
    "        break\n",
    "agent_positions_batch = torch.stack(agent_positions_batch, dim=0)\n",
    "target_positions_batch = torch.stack(target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = hi_model(agent_positions_batch, target_positions_batch, tag=\"situation_encode\")\n",
    "actions = torch.zeros(hidden_states.size(0), 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = []\n",
    "actions_length = torch.zeros(hidden_states.size(0), 1).long()\n",
    "for i in range(1):\n",
    "    hidden_states, actions = hi_model(\n",
    "        hmm_states=hidden_states, \n",
    "        hmm_actions=actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    actions_length += (actions!=0).long()\n",
    "    actions_sequence += [actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "x_target = torch.zeros(hidden_states.shape[0], (grid_size*2-1)).long()\n",
    "y_target = torch.zeros(hidden_states.shape[0], (grid_size*2-1)).long()\n",
    "indices = hidden_states + 5\n",
    "x_target[range(x_target.shape[0]), indices[:,0]] = 1\n",
    "y_target[range(y_target.shape[0]), indices[:,1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = torch.cat(actions_sequence, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(actions_sequence.size(0)):\n",
    "    pred = (hi_model.actions_list_to_sequence(actions_sequence[i,:actions_length[i]].tolist()))\n",
    "    actual = target_commands[i]\n",
    "    assert pred == actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try some interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.load(open(path_to_data, \"r\"))\n",
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/gSCAN-Simple/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=100,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel(\n",
    "    # None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    dual_high_hidden_states = hi_model(\n",
    "        agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    dual_high_actions = torch.zeros(\n",
    "        dual_high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    break # just steal one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervene_time = 1\n",
    "intervene_attribute = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercepted dual hidden states.\n",
    "for j in range(intervene_time):\n",
    "    dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "        hmm_states=dual_high_hidden_states, \n",
    "        hmm_actions=dual_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_decoding_steps = 20\n",
    "# main intervene for loop.\n",
    "cf_high_hidden_states = high_hidden_states\n",
    "cf_high_actions = high_actions\n",
    "intervened_target_batch = [torch.ones(high_hidden_states.size(0), 1).long()] # SOS tokens\n",
    "intervened_target_lengths_batch = torch.zeros(high_hidden_states.size(0), 1).long()\n",
    "# we need to take of the SOS and EOS tokens.\n",
    "for j in range(train_max_decoding_steps-2):\n",
    "    # intercept like antra!\n",
    "    if j == intervene_time:\n",
    "        # only swap out this part.\n",
    "        cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "        # comment out two lines below if it is not for testing.\n",
    "        # cf_high_hidden_states = dual_high_hidden_states\n",
    "        # cf_high_actions = dual_high_actions\n",
    "    cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "        hmm_states=cf_high_hidden_states, \n",
    "        hmm_actions=cf_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    # record the output for loss calculation.\n",
    "    intervened_target_batch += [cf_high_actions]\n",
    "    intervened_target_lengths_batch += (cf_high_actions!=0).long()\n",
    "intervened_target_batch += [torch.zeros(high_hidden_states.size(0), 1).long()] # pad for extra eos\n",
    "intervened_target_lengths_batch += 2\n",
    "intervened_target_batch = torch.cat(intervened_target_batch, dim=-1)\n",
    "for i in range(high_hidden_states.size(0)):\n",
    "    intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(intervened_target_batch[:,:target_batch.size(1)] != target_batch).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervened_target_batch[:,:target_batch.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden states of high level model of the compositional generalization split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train hidden states\n",
    "NUM = 200\n",
    "agent_positions_batch = []\n",
    "target_positions_batch = []\n",
    "target_commands = []\n",
    "for ex in data_json[\"examples\"][\"train\"]:\n",
    "    target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    agent_positions_batch.append(agent)\n",
    "    target_positions_batch.append(target)\n",
    "    if len(agent_positions_batch) == NUM:\n",
    "        break\n",
    "agent_positions_batch = torch.stack(agent_positions_batch, dim=0)\n",
    "target_positions_batch = torch.stack(target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()\n",
    "hidden_states = hi_model(agent_positions_batch, target_positions_batch, tag=\"situation_encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 200\n",
    "cg_agent_positions_batch = []\n",
    "cg_target_positions_batch = []\n",
    "cg_target_commands = []\n",
    "for ex in data_json[\"examples\"][\"situational_1\"]:\n",
    "    cg_target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    cg_agent_positions_batch.append(agent)\n",
    "    cg_target_positions_batch.append(target)\n",
    "    if len(cg_agent_positions_batch) == NUM:\n",
    "        break\n",
    "cg_agent_positions_batch = torch.stack(cg_agent_positions_batch, dim=0)\n",
    "cg_target_positions_batch = torch.stack(cg_target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()\n",
    "hidden_states = hi_model(cg_agent_positions_batch, cg_target_positions_batch, tag=\"situation_encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"][\"situational_1\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first should be positive and the second should be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see, if your intervention produce any similar examples as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.load(open(path_to_data, \"r\"))\n",
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/gSCAN-Simple/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=1000,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "    \n",
    "    print(high_hidden_states)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    dual_high_hidden_states = hi_model(\n",
    "        agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    dual_high_actions = torch.zeros(\n",
    "        dual_high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    intervene_attribute = 1\n",
    "    intervene_time = random.choice([1,2,3])\n",
    "    \n",
    "    # get the intercepted dual hidden states.\n",
    "    for j in range(intervene_time):\n",
    "        dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "            hmm_states=dual_high_hidden_states, \n",
    "            hmm_actions=dual_high_actions, \n",
    "            tag=\"_hmm_step_fxn\"\n",
    "        )\n",
    "        \n",
    "    train_max_decoding_steps = 20\n",
    "    # main intervene for loop.\n",
    "    cf_high_hidden_states = high_hidden_states\n",
    "    cf_high_actions = high_actions\n",
    "    # we need to take of the SOS and EOS tokens.\n",
    "    for j in range(train_max_decoding_steps-1):\n",
    "        # intercept like antra!\n",
    "        if j == intervene_time:\n",
    "            # only swap out this part.\n",
    "            cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "            print(cf_high_hidden_states)\n",
    "            break\n",
    "        cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "            hmm_states=cf_high_hidden_states, \n",
    "            hmm_actions=cf_high_actions, \n",
    "            tag=\"_hmm_step_fxn\"\n",
    "        )\n",
    "    \n",
    "    cg_count = 0\n",
    "    for i in range(input_batch.size(0)):\n",
    "        if cf_high_hidden_states[i][0] > 0 and cf_high_hidden_states[i][1] < 0 and cf_high_hidden_states[i][2] == 0:\n",
    "            cg_count += 1\n",
    "    \n",
    "    print(f\"cg_count: {cg_count}/{input_batch.size(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections are for counterfactual training for new attribute splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/ReaSCAN-novel-attribute/data-compositional-splits.txt\"\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 00:52 Formulating the dataset from the passed in json file...\n",
      "2021-09-20 00:52 Loading vocabularies...\n",
      "2021-09-20 00:52 Done loading vocabularies.\n",
      "2021-09-20 00:52 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/ReaSCAN-novel-attribute/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=1000,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset(novel_attribute=True)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d110814ef98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdual_input_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_situation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdual_agent_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdual_input_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 14)"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    input_sequence, target_sequence, situation, \\\n",
    "        agent_positions, target_positions, \\\n",
    "        input_lengths, target_lengths, \\\n",
    "        dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "        dual_agent_positions, dual_target_positions, \\\n",
    "        dual_input_lengths, dual_target_lengths = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
