{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/gSCAN-Simple/data-compositional-splits.txt\"\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 200\n",
    "agent_positions_batch = []\n",
    "target_positions_batch = []\n",
    "target_commands = []\n",
    "for ex in data_json[\"examples\"][\"situational_1\"]:\n",
    "    target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    agent_positions_batch.append(agent)\n",
    "    target_positions_batch.append(target)\n",
    "    if len(agent_positions_batch) == NUM:\n",
    "        break\n",
    "agent_positions_batch = torch.stack(agent_positions_batch, dim=0)\n",
    "target_positions_batch = torch.stack(target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = hi_model(agent_positions_batch, target_positions_batch, tag=\"situation_encode\")\n",
    "actions = torch.zeros(hidden_states.size(0), 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = []\n",
    "actions_length = torch.zeros(hidden_states.size(0), 1).long()\n",
    "for i in range(1):\n",
    "    hidden_states, actions = hi_model(\n",
    "        hmm_states=hidden_states, \n",
    "        hmm_actions=actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    actions_length += (actions!=0).long()\n",
    "    actions_sequence += [actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 6\n",
    "x_target = torch.zeros(hidden_states.shape[0], (grid_size*2-1)).long()\n",
    "y_target = torch.zeros(hidden_states.shape[0], (grid_size*2-1)).long()\n",
    "indices = hidden_states + 5\n",
    "x_target[range(x_target.shape[0]), indices[:,0]] = 1\n",
    "y_target[range(y_target.shape[0]), indices[:,1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = torch.cat(actions_sequence, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(actions_sequence.size(0)):\n",
    "    pred = (hi_model.actions_list_to_sequence(actions_sequence[i,:actions_length[i]].tolist()))\n",
    "    actual = target_commands[i]\n",
    "    assert pred == actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try some interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.load(open(path_to_data, \"r\"))\n",
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/gSCAN-Simple/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=100,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel(\n",
    "    # None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    dual_high_hidden_states = hi_model(\n",
    "        agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    dual_high_actions = torch.zeros(\n",
    "        dual_high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    break # just steal one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervene_time = 1\n",
    "intervene_attribute = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercepted dual hidden states.\n",
    "for j in range(intervene_time):\n",
    "    dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "        hmm_states=dual_high_hidden_states, \n",
    "        hmm_actions=dual_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_decoding_steps = 20\n",
    "# main intervene for loop.\n",
    "cf_high_hidden_states = high_hidden_states\n",
    "cf_high_actions = high_actions\n",
    "intervened_target_batch = [torch.ones(high_hidden_states.size(0), 1).long()] # SOS tokens\n",
    "intervened_target_lengths_batch = torch.zeros(high_hidden_states.size(0), 1).long()\n",
    "# we need to take of the SOS and EOS tokens.\n",
    "for j in range(train_max_decoding_steps-2):\n",
    "    # intercept like antra!\n",
    "    if j == intervene_time:\n",
    "        # only swap out this part.\n",
    "        cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "        # comment out two lines below if it is not for testing.\n",
    "        # cf_high_hidden_states = dual_high_hidden_states\n",
    "        # cf_high_actions = dual_high_actions\n",
    "    cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "        hmm_states=cf_high_hidden_states, \n",
    "        hmm_actions=cf_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    # record the output for loss calculation.\n",
    "    intervened_target_batch += [cf_high_actions]\n",
    "    intervened_target_lengths_batch += (cf_high_actions!=0).long()\n",
    "intervened_target_batch += [torch.zeros(high_hidden_states.size(0), 1).long()] # pad for extra eos\n",
    "intervened_target_lengths_batch += 2\n",
    "intervened_target_batch = torch.cat(intervened_target_batch, dim=-1)\n",
    "for i in range(high_hidden_states.size(0)):\n",
    "    intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(intervened_target_batch[:,:target_batch.size(1)] != target_batch).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervened_target_batch[:,:target_batch.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden states of high level model of the compositional generalization split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train hidden states\n",
    "NUM = 200\n",
    "agent_positions_batch = []\n",
    "target_positions_batch = []\n",
    "target_commands = []\n",
    "for ex in data_json[\"examples\"][\"train\"]:\n",
    "    target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    agent_positions_batch.append(agent)\n",
    "    target_positions_batch.append(target)\n",
    "    if len(agent_positions_batch) == NUM:\n",
    "        break\n",
    "agent_positions_batch = torch.stack(agent_positions_batch, dim=0)\n",
    "target_positions_batch = torch.stack(target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()\n",
    "hidden_states = hi_model(agent_positions_batch, target_positions_batch, tag=\"situation_encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 200\n",
    "cg_agent_positions_batch = []\n",
    "cg_target_positions_batch = []\n",
    "cg_target_commands = []\n",
    "for ex in data_json[\"examples\"][\"situational_1\"]:\n",
    "    cg_target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    cg_agent_positions_batch.append(agent)\n",
    "    cg_target_positions_batch.append(target)\n",
    "    if len(cg_agent_positions_batch) == NUM:\n",
    "        break\n",
    "cg_agent_positions_batch = torch.stack(cg_agent_positions_batch, dim=0)\n",
    "cg_target_positions_batch = torch.stack(cg_target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()\n",
    "hidden_states = hi_model(cg_agent_positions_batch, cg_target_positions_batch, tag=\"situation_encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"][\"situational_1\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first should be positive and the second should be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see, if your intervention produce any similar examples as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = json.load(open(path_to_data, \"r\"))\n",
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/gSCAN-Simple/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=1000,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "    \n",
    "    print(high_hidden_states)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    dual_high_hidden_states = hi_model(\n",
    "        agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    dual_high_actions = torch.zeros(\n",
    "        dual_high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    intervene_attribute = 1\n",
    "    intervene_time = random.choice([1,2,3])\n",
    "    \n",
    "    # get the intercepted dual hidden states.\n",
    "    for j in range(intervene_time):\n",
    "        dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "            hmm_states=dual_high_hidden_states, \n",
    "            hmm_actions=dual_high_actions, \n",
    "            tag=\"_hmm_step_fxn\"\n",
    "        )\n",
    "        \n",
    "    train_max_decoding_steps = 20\n",
    "    # main intervene for loop.\n",
    "    cf_high_hidden_states = high_hidden_states\n",
    "    cf_high_actions = high_actions\n",
    "    # we need to take of the SOS and EOS tokens.\n",
    "    for j in range(train_max_decoding_steps-1):\n",
    "        # intercept like antra!\n",
    "        if j == intervene_time:\n",
    "            # only swap out this part.\n",
    "            cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "            print(cf_high_hidden_states)\n",
    "            break\n",
    "        cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "            hmm_states=cf_high_hidden_states, \n",
    "            hmm_actions=cf_high_actions, \n",
    "            tag=\"_hmm_step_fxn\"\n",
    "        )\n",
    "    \n",
    "    cg_count = 0\n",
    "    for i in range(input_batch.size(0)):\n",
    "        if cf_high_hidden_states[i][0] > 0 and cf_high_hidden_states[i][1] < 0 and cf_high_hidden_states[i][2] == 0:\n",
    "            cg_count += 1\n",
    "    \n",
    "    print(f\"cg_count: {cg_count}/{input_batch.size(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following sections are for counterfactual training for new attribute splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/ReaSCAN-novel-attribute/data-compositional-splits.txt\"\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random \n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', 'Reason-SCAN', 'code', 'dataset'))\n",
    "\n",
    "# we want to make this file device irrelevant,\n",
    "# and we decide where to store the data afterwards.\n",
    "# def isnotebook():\n",
    "#     try:\n",
    "#         shell = get_ipython().__class__.__name__\n",
    "#         if shell == 'ZMQInteractiveShell':\n",
    "#             return True   # Jupyter notebook or qtconsole\n",
    "#         elif shell == 'TerminalInteractiveShell':\n",
    "#             return False  # Terminal running IPython\n",
    "#         else:\n",
    "#             return False  # Other type (?)\n",
    "#     except NameError:\n",
    "#         return False      # Probably standard Python interpreter\n",
    "# if isnotebook():\n",
    "#     device = torch.device(\"cpu\")\n",
    "# else:\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from vocabulary import Vocabulary as ReaSCANVocabulary\n",
    "from object_vocabulary import *\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Object that maps words in string form to indices to be processed by numerical models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sos_token=\"<SOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\"):\n",
    "        \"\"\"\n",
    "        NB: <PAD> token is by construction idx 0.\n",
    "        \"\"\"\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self._idx_to_word = [pad_token, sos_token, eos_token]\n",
    "        self._word_to_idx = defaultdict(lambda: self._idx_to_word.index(self.pad_token))\n",
    "        self._word_to_idx[sos_token] = 1\n",
    "        self._word_to_idx[eos_token] = 2\n",
    "        self._word_frequencies = Counter()\n",
    "\n",
    "    def word_to_idx(self, word: str) -> int:\n",
    "        return self._word_to_idx[word]\n",
    "\n",
    "    def idx_to_word(self, idx: int) -> str:\n",
    "        return self._idx_to_word[idx]\n",
    "\n",
    "    def add_sentence(self, sentence: List[str]):\n",
    "        for word in sentence:\n",
    "            if word not in self._word_to_idx:\n",
    "                self._word_to_idx[word] = self.size\n",
    "                self._idx_to_word.append(word)\n",
    "            self._word_frequencies[word] += 1\n",
    "\n",
    "    def most_common(self, n=10):\n",
    "        return self._word_frequencies.most_common(n=n)\n",
    "\n",
    "    @property\n",
    "    def pad_idx(self):\n",
    "        return self.word_to_idx(self.pad_token)\n",
    "\n",
    "    @property\n",
    "    def sos_idx(self):\n",
    "        return self.word_to_idx(self.sos_token)\n",
    "\n",
    "    @property\n",
    "    def eos_idx(self):\n",
    "        return self.word_to_idx(self.eos_token)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._idx_to_word)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        assert os.path.exists(path), \"Trying to load a vocabulary from a non-existing file {}\".format(path)\n",
    "        with open(path, 'r') as infile:\n",
    "            all_data = json.load(infile)\n",
    "            sos_token = all_data[\"sos_token\"]\n",
    "            eos_token = all_data[\"eos_token\"]\n",
    "            pad_token = all_data[\"pad_token\"]\n",
    "            vocab = cls(sos_token=sos_token, eos_token=eos_token, pad_token=pad_token)\n",
    "            vocab._idx_to_word = all_data[\"idx_to_word\"]\n",
    "            vocab._word_to_idx = defaultdict(int)\n",
    "            for word, idx in all_data[\"word_to_idx\"].items():\n",
    "                vocab._word_to_idx[word] = idx\n",
    "            vocab._word_frequencies = Counter(all_data[\"word_frequencies\"])\n",
    "        return vocab\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"sos_token\": self.sos_token,\n",
    "            \"eos_token\": self.eos_token,\n",
    "            \"pad_token\": self.pad_token,\n",
    "            \"idx_to_word\": self._idx_to_word,\n",
    "            \"word_to_idx\": self._word_to_idx,\n",
    "            \"word_frequencies\": self._word_frequencies\n",
    "        }\n",
    "\n",
    "    def save(self, path: str) -> str:\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(self.to_dict(), outfile, indent=4)\n",
    "        return path\n",
    "    \n",
    "class ReaSCANDataset(object):\n",
    "    \"\"\"\n",
    "    Loads a GroundedScan instance from a specified location.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_json, save_directory: str, k: int, split=\"all\", input_vocabulary_file=\"\",\n",
    "                 target_vocabulary_file=\"\", generate_vocabulary=False):\n",
    "        if not generate_vocabulary:\n",
    "            assert os.path.exists(os.path.join(save_directory, input_vocabulary_file)) and os.path.exists(\n",
    "                os.path.join(save_directory, target_vocabulary_file)), \\\n",
    "                \"Trying to load vocabularies from non-existing files.\"\n",
    "        \n",
    "        # we simply load the json file.\n",
    "        logger.info(f\"Formulating the dataset from the passed in json file...\")\n",
    "        self.data_json = data_json\n",
    "        \n",
    "        if split == \"test\" and generate_vocabulary:\n",
    "            logger.warning(\"WARNING: generating a vocabulary from the test set.\")\n",
    "            \n",
    "        # some helper initialization\n",
    "        self.grid_size = self.data_json['grid_size']\n",
    "\n",
    "        intransitive_verbs = [\"walk\"]\n",
    "        transitive_verbs = [\"push\", \"pull\"]\n",
    "        adverbs = [\"while zigzagging\", \"while spinning\", \"cautiously\", \"hesitantly\"]\n",
    "        nouns = [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "        color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "        size_adjectives = [\"big\", \"small\"]\n",
    "        relative_pronouns = [\"that is\"]\n",
    "        relation_clauses = [\"in the same row as\", \n",
    "                            \"in the same column as\", \n",
    "                            \"in the same color as\", \n",
    "                            \"in the same shape as\", \n",
    "                            \"in the same size as\",\n",
    "                            \"inside of\"]\n",
    "        reaSCANVocabulary = ReaSCANVocabulary.initialize(intransitive_verbs=intransitive_verbs,\n",
    "                                                   transitive_verbs=transitive_verbs, adverbs=adverbs, nouns=nouns,\n",
    "                                                   color_adjectives=color_adjectives,\n",
    "                                                   size_adjectives=size_adjectives, \n",
    "                                                   relative_pronouns=relative_pronouns, \n",
    "                                                   relation_clauses=relation_clauses)\n",
    "        min_object_size = 1\n",
    "        max_object_size = 4\n",
    "        object_vocabulary = ObjectVocabulary(shapes=reaSCANVocabulary.get_semantic_shapes(),\n",
    "                                             colors=reaSCANVocabulary.get_semantic_colors(),\n",
    "                                             min_size=min_object_size, max_size=max_object_size)\n",
    "        \n",
    "        self._world = World(grid_size=self.grid_size, colors=reaSCANVocabulary.get_semantic_colors(),\n",
    "                            object_vocabulary=object_vocabulary,\n",
    "                            shapes=reaSCANVocabulary.get_semantic_shapes(),\n",
    "                            save_directory=save_directory)\n",
    "        self._world.clear_situation()\n",
    "            \n",
    "        self.image_dimensions = self._world.get_current_situation_image().shape[0] \n",
    "        self.image_channels = 3\n",
    "        self.split = split\n",
    "        self.directory = save_directory\n",
    "        self.k = k\n",
    "\n",
    "        # Keeping track of data.\n",
    "        self._examples = np.array([])\n",
    "        self._input_lengths = np.array([])\n",
    "        self._target_lengths = np.array([])\n",
    "        if generate_vocabulary:\n",
    "            logger.info(\"Generating vocabularies...\")\n",
    "            self.input_vocabulary = Vocabulary()\n",
    "            self.target_vocabulary = Vocabulary()\n",
    "            self.read_vocabularies()\n",
    "            logger.info(\"Done generating vocabularies.\")\n",
    "        else:\n",
    "            logger.info(\"Loading vocabularies...\")\n",
    "            self.input_vocabulary = Vocabulary.load(os.path.join(save_directory, input_vocabulary_file))\n",
    "            self.target_vocabulary = Vocabulary.load(os.path.join(save_directory, target_vocabulary_file))\n",
    "            logger.info(\"Done loading vocabularies.\")\n",
    "    \n",
    "        self.k=k # this is for the few-shot splits only!\n",
    "\n",
    "    @staticmethod\n",
    "    def command_repr(command: List[str]) -> str:\n",
    "        return ','.join(command)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_command_repr(command_repr: str) -> List[str]:\n",
    "        return command_repr.split(',')\n",
    "    \n",
    "    def initialize_world(self, situation: Situation, mission=\"\") -> {}:\n",
    "        \"\"\"\n",
    "        Initializes the world with the passed situation.\n",
    "        :param situation: class describing the current situation in the world, fully determined by a grid size,\n",
    "        agent position, agent direction, list of placed objects, an optional target object and optional carrying object.\n",
    "        :param mission: a string defining a command (e.g. \"Walk to a green circle.\")\n",
    "        \"\"\"\n",
    "        objects = []\n",
    "        for positioned_object in situation.placed_objects:\n",
    "            objects.append((positioned_object.object, positioned_object.position))\n",
    "        self._world.initialize(objects, agent_position=situation.agent_pos, agent_direction=situation.agent_direction,\n",
    "                               target_object=situation.target_object, carrying=situation.carrying)\n",
    "        if mission:\n",
    "            self._world.set_mission(mission)\n",
    "\n",
    "    def extract_size_color_shape(self, referred_target_str):\n",
    "        target_size_d = \"\"\n",
    "        target_color_d = \"\"\n",
    "        target_shape_d = \"\"\n",
    "        if \"small\" in referred_target_str:\n",
    "            target_size_d = \"small\"\n",
    "        elif \"big\" in referred_target_str:\n",
    "            target_size_d = \"big\"\n",
    "        else:\n",
    "            pass\n",
    "        if \"circle\" in referred_target_str:\n",
    "            target_shape_d = \"circle\"\n",
    "        elif \"cylinder\" in referred_target_str:\n",
    "            target_shape_d = \"cylinder\"\n",
    "        elif \"square\" in referred_target_str:\n",
    "            target_shape_d = \"square\"\n",
    "        elif \"box\" in referred_target_str:\n",
    "            assert False\n",
    "        elif \"object\" in referred_target_str:\n",
    "            assert False\n",
    "        else:\n",
    "            pass\n",
    "        if \"red\" in referred_target_str:\n",
    "            target_color_d = \"red\"\n",
    "        elif \"blue\" in referred_target_str:\n",
    "            target_color_d = \"blue\"\n",
    "        elif \"green\" in referred_target_str:\n",
    "            target_color_d = \"green\"\n",
    "        elif \"yellow\" in referred_target_str:\n",
    "            target_color_d = \"yellow\"\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return target_size_d, target_color_d, target_shape_d\n",
    "    \n",
    "    def get_examples_with_image(self, split=\"train\", simple_situation_representation=False) -> dict:\n",
    "        \"\"\"\n",
    "        Get data pairs with images in the form of np.ndarray's with RGB values or with 1 pixel per grid cell\n",
    "        (see encode in class Grid of minigrid.py for details on what such representation looks like).\n",
    "        :param split: string specifying which split to load.\n",
    "        :param simple_situation_representation:  whether to get the full RGB image or a simple representation.\n",
    "        :return: data examples.\n",
    "        \"\"\"\n",
    "        for example in self.data_json[\"examples\"][split]:\n",
    "            target_size_d, target_color_d, target_shape_d = self.extract_size_color_shape(example[\"referred_target\"])\n",
    "            target_serialized_str = \",\".join([target_size_d, target_color_d, target_shape_d])\n",
    "\n",
    "            command = self.parse_command_repr(example[\"command\"])\n",
    "            if example.get(\"meaning\"):\n",
    "                meaning = example[\"meaning\"]\n",
    "            else:\n",
    "                meaning = example[\"command\"]\n",
    "            meaning = self.parse_command_repr(meaning)\n",
    "            situation = Situation.from_representation(example[\"situation\"])\n",
    "            self._world.clear_situation()\n",
    "            self.initialize_world(situation)\n",
    "            if simple_situation_representation:\n",
    "                situation_image = self._world.get_current_situation_grid_repr()\n",
    "            else:\n",
    "                situation_image = self._world.get_current_situation_image()\n",
    "            target_commands = self.parse_command_repr(example[\"target_commands\"])\n",
    "            verb_in_command = \"\"\n",
    "            if \"verb_in_command\" in example:\n",
    "                verb_in_command = example[\"verb_in_command\"]\n",
    "            adverb_in_command = \"\"\n",
    "            if \"adverb_in_command\" in example:\n",
    "                adverb_in_command = example[\"adverb_in_command\"]\n",
    "            yield {\"input_command\": command, \"input_meaning\": meaning,\n",
    "                   \"derivation_representation\": example.get(\"derivation\"),\n",
    "                   \"situation_image\": situation_image, \"situation_representation\": example[\"situation\"],\n",
    "                   \"target_command\": target_commands, \"target_str\": target_serialized_str, \n",
    "                   \"verb_in_command\": verb_in_command, \n",
    "                   \"adverb_in_command\": adverb_in_command}\n",
    "    \n",
    "    def read_vocabularies(self) -> {}:\n",
    "        \"\"\"\n",
    "        Loop over all examples in the dataset and add the words in them to the vocabularies.\n",
    "        \"\"\"\n",
    "        logger.info(\"Populating vocabulary...\")\n",
    "        for i, example in enumerate(self.get_examples_with_image(self.split)):\n",
    "            self.input_vocabulary.add_sentence(example[\"input_command\"])\n",
    "            self.target_vocabulary.add_sentence(example[\"target_command\"])\n",
    "\n",
    "    def save_vocabularies(self, input_vocabulary_file: str, target_vocabulary_file: str):\n",
    "        self.input_vocabulary.save(os.path.join(self.directory, input_vocabulary_file))\n",
    "        self.target_vocabulary.save(os.path.join(self.directory, target_vocabulary_file))\n",
    "\n",
    "    def get_vocabulary(self, vocabulary: str) -> Vocabulary:\n",
    "        if vocabulary == \"input\":\n",
    "            vocab = self.input_vocabulary\n",
    "        elif vocabulary == \"target\":\n",
    "            vocab = self.target_vocabulary\n",
    "        else:\n",
    "            raise ValueError(\"Specified unknown vocabulary in sentence_to_array: {}\".format(vocabulary))\n",
    "        return vocab\n",
    "\n",
    "    def shuffle_data(self) -> {}:\n",
    "        \"\"\"\n",
    "        Reorder the data examples and reorder the lengths of the input and target commands accordingly.\n",
    "        \"\"\"\n",
    "        random_permutation = np.random.permutation(len(self._examples))\n",
    "        self._examples = self._examples[random_permutation]\n",
    "        self._target_lengths = self._target_lengths[random_permutation]\n",
    "        self._input_lengths = self._input_lengths[random_permutation]\n",
    "    \n",
    "    def get_max_seq_length_input(self):\n",
    "        input_lengths = self._input_lengths\n",
    "        max_input_length = np.max(input_lengths)\n",
    "        return max_input_length\n",
    "     \n",
    "    def get_max_seq_length_target(self):\n",
    "        target_lengths = self._target_lengths\n",
    "        max_target_length = np.max(target_lengths)\n",
    "        return max_target_length\n",
    "\n",
    "    def get_dual_dataset(self, novel_attribute=False):\n",
    "        \"\"\"\n",
    "        Function for getting dual dataset for\n",
    "        counterfactual training.\n",
    "        \"\"\"\n",
    "        examples = self._examples        \n",
    "        # get length.\n",
    "        input_lengths = self._input_lengths\n",
    "        target_lengths = self._target_lengths\n",
    "        \n",
    "        ## WARNING: DO NOT UNCOMMENT THE FOLLOWING LINES.\n",
    "        ## TODO: DEBUG ON THIS.\n",
    "        # we may want to random shuffle the examples\n",
    "        # to make sure have a richer space of composities.\n",
    "        # p = np.random.permutation(len(input_lengths))\n",
    "        # examples = examples[p]\n",
    "        # input_lengths = input_lengths[p]\n",
    "        # target_lengths = target_lengths[p]\n",
    "        \n",
    "        max_input_length = np.max(input_lengths)\n",
    "        max_target_length = np.max(target_lengths)\n",
    "        \n",
    "        # return structs\n",
    "        input_batch = []\n",
    "        target_batch = []\n",
    "        situation_batch = []\n",
    "        situation_representation_batch = []\n",
    "        derivation_representation_batch = []\n",
    "        agent_positions_batch = []\n",
    "        target_positions_batch = []\n",
    "        target_str_batch = [] \n",
    "        adverb_str_batch = []\n",
    "        verb_str_batch = []\n",
    "        \n",
    "        for example in examples:\n",
    "            to_pad_input = max_input_length - example[\"input_tensor\"].size(1)\n",
    "            to_pad_target = max_target_length - example[\"target_tensor\"].size(1)\n",
    "            padded_input = torch.cat([\n",
    "                example[\"input_tensor\"],\n",
    "                torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "            # padded_input = torch.cat([\n",
    "            #     torch.zeros_like(example[\"input_tensor\"], dtype=torch.long),\n",
    "            #     torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1) # TODO: change back\n",
    "            padded_target = torch.cat([\n",
    "                example[\"target_tensor\"],\n",
    "                torch.zeros(int(to_pad_target), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "            input_batch.append(padded_input)\n",
    "            target_batch.append(padded_target)\n",
    "            situation_batch.append(example[\"situation_tensor\"])\n",
    "            situation_representation_batch.append(example[\"situation_representation\"])\n",
    "            derivation_representation_batch.append(example[\"derivation_representation\"])\n",
    "            agent_positions_batch.append(example[\"agent_position\"])\n",
    "            target_positions_batch.append(example[\"target_position\"])\n",
    "            target_str_batch.append(example[\"target_str\"])\n",
    "            adverb_str_batch.append(example[\"adverb_in_command\"])\n",
    "            verb_str_batch.append(example[\"verb_in_command\"])\n",
    "        \n",
    "        # Main dataset.\n",
    "        main_input_batch = torch.cat(input_batch, dim=0)\n",
    "        main_target_batch = torch.cat(target_batch, dim=0)\n",
    "        main_situation_batch = torch.cat(situation_batch, dim=0)\n",
    "        main_agent_positions_batch = torch.cat(agent_positions_batch, dim=0)\n",
    "        main_target_positions_batch = torch.cat(target_positions_batch, dim=0)\n",
    "        main_input_lengths_batch = torch.tensor([[l] for l in input_lengths], dtype=torch.long)\n",
    "        main_target_lengths_batch = torch.tensor([[l] for l in target_lengths], dtype=torch.long)\n",
    "        \n",
    "        # Dual dataset for counterfactual training.\n",
    "        dual_input_batch = torch.cat(input_batch, dim=0)\n",
    "        dual_target_batch = torch.cat(target_batch, dim=0)\n",
    "        dual_situation_batch = torch.cat(situation_batch, dim=0)\n",
    "        dual_agent_positions_batch = torch.cat(agent_positions_batch, dim=0)\n",
    "        dual_target_positions_batch = torch.cat(target_positions_batch, dim=0)\n",
    "        dual_input_lengths_batch = torch.tensor([[l] for l in input_lengths], dtype=torch.long)\n",
    "        dual_target_lengths_batch = torch.tensor([[l] for l in target_lengths], dtype=torch.long)\n",
    "        # Randomly shifting the dataset.\n",
    "        # Later, we may need to have more complicated sampling strategies for\n",
    "        # getting the dual dataset for counterfactual training.\n",
    "        perm_idx = torch.randperm(dual_input_batch.size()[0])\n",
    "        dual_input_batch = dual_input_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_target_batch = dual_target_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_situation_batch = dual_situation_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_agent_positions_batch = dual_agent_positions_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_target_positions_batch = dual_target_positions_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_input_lengths_batch = dual_input_lengths_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_target_lengths_batch = dual_target_lengths_batch.index_select(dim=0, index=perm_idx)\n",
    "        dual_target_str_batch = [target_str_batch[i] for i in perm_idx.tolist()]\n",
    "        dual_situation_representation_batch = [situation_representation_batch[i] for i in perm_idx.tolist()]\n",
    "        dual_adverb_str_batch = [adverb_str_batch[i] for i in perm_idx.tolist()]\n",
    "        dual_verb_str_batch = [verb_str_batch[i] for i in perm_idx.tolist()]\n",
    "        # we need to do a little extra work here just to generate\n",
    "        # examples for novel attribute cases.\n",
    "        \n",
    "        if not novel_attribute:\n",
    "            main_dataset = TensorDataset(\n",
    "                # main dataset\n",
    "                main_input_batch, main_target_batch, main_situation_batch, main_agent_positions_batch, \n",
    "                main_target_positions_batch, main_input_lengths_batch, main_target_lengths_batch,\n",
    "                # dual dataset\n",
    "                dual_input_batch, dual_target_batch, dual_situation_batch, dual_agent_positions_batch,\n",
    "                dual_target_positions_batch, dual_input_lengths_batch, dual_target_lengths_batch,\n",
    "            )\n",
    "            # with non-tensorized outputs\n",
    "            return main_dataset, (situation_representation_batch, derivation_representation_batch)\n",
    "            # the last two items are deprecated. we need to fix them to make them usable.\n",
    "        \n",
    "        # here are the steps:\n",
    "        # 1. find avaliable attributes to swap in both example.\n",
    "        # 2. swap attribute, and get the updated action sequence, everything else stays the same.\n",
    "        # 3. we need to identify the swap index.\n",
    "        # 3.1. there can be two ways of swapping, either whole token swapping, or noun embedding\n",
    "        # slice swapping. Both should be provided I think.\n",
    "        # I think we then need to return two things, noun swapping index, and atribute swapping index\n",
    "        # those two could be the same, if we are swapping the noun.\n",
    "        batch_size = dual_input_batch.shape[0]\n",
    "        intervened_main_swap_index = []\n",
    "        intervened_dual_swap_index = []\n",
    "        intervened_main_shape_index = []\n",
    "        intervened_dual_shape_index = []\n",
    "        intervened_target_batch = []\n",
    "        intervened_target_lengths_batch = []\n",
    "        intervened_swap_attr = [] # 0, 1, 2 maps to size, color, shape\n",
    "        intervened_target_str = []\n",
    "        for i in range(0, batch_size):\n",
    "            if not novel_attribute:\n",
    "                # we put dummies\n",
    "                main_swap_index, dual_swap_index, main_shape_index, dual_shape_index = -1, -1, -1, -1\n",
    "                to_pad_target = max_target_length\n",
    "                intervened_padded_target = torch.zeros(int(to_pad_target), dtype=torch.long)\n",
    "                intervened_main_swap_index += [main_swap_index]\n",
    "                intervened_dual_swap_index += [dual_swap_index]\n",
    "                intervened_main_shape_index += [main_shape_index]\n",
    "                intervened_dual_shape_index += [dual_shape_index]\n",
    "                intervened_target_batch += [intervened_padded_target]\n",
    "                intervened_target_lengths_batch += [0]\n",
    "                continue\n",
    "            main_command_str = self.array_to_sentence(main_input_batch[i].tolist(), \"input\")\n",
    "            dual_command_str = self.array_to_sentence(dual_input_batch[i].tolist(), \"input\")\n",
    "            target_si, target_co, target_sh = target_str_batch[i].split(\",\")\n",
    "            dual_target_si, dual_target_co, dual_target_sh = dual_target_str_batch[i].split(\",\")\n",
    "            potential_swap_attr = [\"\"]\n",
    "            if target_si != \"\" and dual_target_si != \"\":\n",
    "                potential_swap_attr += [\"size\"]\n",
    "            if target_co != \"\" and dual_target_co != \"\":\n",
    "                potential_swap_attr += [\"color\"]\n",
    "            if target_sh != \"\" and dual_target_sh != \"\":\n",
    "                potential_swap_attr += [\"shape\"]\n",
    "            swap_attr = random.choice(potential_swap_attr)\n",
    "            \n",
    "            if swap_attr == \"size\":\n",
    "                swap_attr_main = target_si\n",
    "                swap_attr_dual = dual_target_si\n",
    "                new_composites = [dual_target_si, target_co, target_sh]\n",
    "                intervened_swap_attr += [0]\n",
    "            elif swap_attr == \"color\":\n",
    "                swap_attr_main = target_co\n",
    "                swap_attr_dual = dual_target_co\n",
    "                new_composites = [target_si, dual_target_co, target_sh]\n",
    "                intervened_swap_attr += [1]\n",
    "            elif swap_attr == \"shape\":\n",
    "                swap_attr_main = target_sh\n",
    "                swap_attr_dual = dual_target_sh\n",
    "                new_composites = [target_si, target_co, dual_target_sh]\n",
    "                intervened_swap_attr += [2]\n",
    "            else:\n",
    "                intervened_swap_attr += [-1]\n",
    "            \n",
    "            new_target_id = -1\n",
    "            if swap_attr != \"\":\n",
    "                id_size_tuples = []\n",
    "                for k, v in situation_representation_batch[i][\"placed_objects\"].items():\n",
    "                    if v[\"object\"][\"shape\"] == new_composites[2]:\n",
    "                        if new_composites[1] != \"\":\n",
    "                            if v[\"object\"][\"color\"] == new_composites[1]:\n",
    "                                id_size_tuples.append((k, int(v[\"object\"][\"size\"])))\n",
    "                        else:\n",
    "                            id_size_tuples.append((k, int(v[\"object\"][\"size\"])))\n",
    "                if new_composites[0] != \"\":\n",
    "                    # we need to ground size relatively?\n",
    "                    size_set = set([])\n",
    "                    for ss in id_size_tuples:\n",
    "                        size_set.add(ss[1])\n",
    "                    if len(id_size_tuples) == 2 and len(size_set) == 2:\n",
    "                        id_size_tuples = sorted(id_size_tuples, key=lambda x: x[1])\n",
    "                        # only more than 2 we can have relative stuffs.\n",
    "                        if new_composites[0] == \"big\":\n",
    "                            new_target_id = id_size_tuples[-1][0]\n",
    "                        elif new_composites[0] == \"small\":\n",
    "                            new_target_id = id_size_tuples[0][0]\n",
    "                else:\n",
    "                    if len(id_size_tuples) == 1:\n",
    "                        new_target_id = id_size_tuples[0][0]\n",
    "            \n",
    "            if new_target_id == -1:\n",
    "                # we don't have a new target, we need to use some dummy data!\n",
    "                main_swap_index, dual_swap_index, main_shape_index, dual_shape_index = -1, -1, -1, -1\n",
    "                to_pad_target = max_target_length\n",
    "                intervened_padded_target = torch.zeros(int(to_pad_target), dtype=torch.long)\n",
    "                intervened_target_lengths_batch += [0]\n",
    "            else:\n",
    "                new_target_shape = situation_representation_batch[i][\"placed_objects\"][new_target_id]['object']['shape']\n",
    "                new_target_color = situation_representation_batch[i][\"placed_objects\"][new_target_id]['object']['color']\n",
    "                assert new_target_shape == new_composites[2]\n",
    "                if new_composites[1] != \"\":\n",
    "                    assert new_target_color == new_composites[1]\n",
    "\n",
    "                # we have a new target, let us generate the action sequence as well.\n",
    "                new_target_pos = situation_representation_batch[i][\"placed_objects\"][new_target_id][\"position\"]\n",
    "                self._world.clear_situation()\n",
    "                for obj_idx, obj in situation_representation_batch[i][\"placed_objects\"].items():\n",
    "                    self._world.place_object(\n",
    "                        Object(size=int(obj[\"object\"][\"size\"]), color=obj[\"object\"][\"color\"], shape=obj[\"object\"][\"shape\"]),\n",
    "                        position=Position(row=int(obj[\"position\"][\"row\"]), column=int(obj[\"position\"][\"column\"]))\n",
    "                    )\n",
    "                self._world.place_agent_at(\n",
    "                    Position(\n",
    "                        row=int(situation_representation_batch[i][\"agent_position\"][\"row\"]),\n",
    "                        column=int(situation_representation_batch[i][\"agent_position\"][\"column\"])\n",
    "                ))\n",
    "                row = int(new_target_pos[\"row\"])\n",
    "                column = int(new_target_pos[\"column\"])\n",
    "                new_target_position = Position(\n",
    "                    row=row,\n",
    "                    column=column\n",
    "                )\n",
    "                self._world.go_to_position(\n",
    "                    position=new_target_position, \n",
    "                    manner=adverb_str_batch[i], \n",
    "                    primitive_command=\"walk\"\n",
    "                )\n",
    "\n",
    "                if len(adverb_str_batch[i].split(\" \")) > 1:\n",
    "                    assert adverb_str_batch[i].split(\" \")[-1] in main_command_str\n",
    "                elif len(adverb_str_batch[i]) != 0:\n",
    "                    assert adverb_str_batch[i] in main_command_str\n",
    "\n",
    "                if verb_str_batch[i] != \"walk\":\n",
    "                    self._world.move_object_to_wall(action=verb_str_batch[i], manner=adverb_str_batch[i])\n",
    "                target_commands, _ = self._world.get_current_observations()\n",
    "                target_array = self.sentence_to_array(target_commands, vocabulary=\"target\")\n",
    "                self._world.clear_situation()\n",
    "                \n",
    "                # now, we need to get the index of words in the sequence that need to be swapped.\n",
    "                main_swap_index = main_command_str.index(swap_attr_main)\n",
    "                dual_swap_index = dual_command_str.index(swap_attr_dual)\n",
    "                main_shape_index = main_command_str.index(target_sh)\n",
    "                dual_shape_index = dual_command_str.index(dual_target_sh)\n",
    "                \n",
    "                assert main_command_str[main_shape_index] in [\"circle\", \"square\", \"cylinder\"]\n",
    "                assert dual_command_str[dual_shape_index] in [\"circle\", \"square\", \"cylinder\"]\n",
    "                if intervened_swap_attr[-1] == 0:\n",
    "                    assert main_command_str[main_swap_index] in [\"\", \"small\", \"big\"]\n",
    "                    assert dual_command_str[dual_swap_index] in [\"\", \"small\", \"big\"]\n",
    "                elif intervened_swap_attr[-1] == 1:\n",
    "                    assert main_command_str[main_swap_index] in [\"\", \"red\", \"blue\", \"green\", \"yellow\"]\n",
    "                    assert dual_command_str[dual_swap_index] in [\"\", \"red\", \"blue\", \"green\", \"yellow\"]\n",
    "                elif intervened_swap_attr[-1] == 2:\n",
    "                    assert main_command_str[main_swap_index] in [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "                    assert dual_command_str[dual_swap_index] in [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "                \n",
    "                if len(target_array) <= max_target_length:\n",
    "                    # only these are valid!\n",
    "                    target_array = torch.tensor(target_array, dtype=torch.long)\n",
    "                    to_pad_target = max_target_length - target_array.size(0)\n",
    "                    intervened_target_lengths_batch += [target_array.size(0)]\n",
    "                    intervened_padded_target = torch.cat([\n",
    "                        target_array,\n",
    "                        torch.zeros(int(to_pad_target), dtype=torch.long)], dim=-1)\n",
    "                    intervened_target_str += [\",\".join(new_composites)]\n",
    "                else:\n",
    "                    # we don't have a valid action sequence, we need to use some dummy data!\n",
    "                    main_swap_index, dual_swap_index, main_shape_index, dual_shape_index = -1, -1, -1, -1\n",
    "                    to_pad_target = max_target_length\n",
    "                    intervened_padded_target = torch.zeros(int(to_pad_target), dtype=torch.long)\n",
    "                    intervened_target_lengths_batch += [0]\n",
    "            \n",
    "            # we now consolidate everything.\n",
    "            intervened_main_swap_index += [main_swap_index]\n",
    "            intervened_dual_swap_index += [dual_swap_index]\n",
    "            intervened_main_shape_index += [main_shape_index]\n",
    "            intervened_dual_shape_index += [dual_shape_index]\n",
    "            intervened_target_batch += [intervened_padded_target]\n",
    "        \n",
    "        intervened_main_swap_index = torch.tensor(intervened_main_swap_index, dtype=torch.long)\n",
    "        intervened_dual_swap_index = torch.tensor(intervened_dual_swap_index, dtype=torch.long)\n",
    "        intervened_main_shape_index = torch.tensor(intervened_main_shape_index, dtype=torch.long)\n",
    "        intervened_dual_shape_index = torch.tensor(intervened_dual_shape_index, dtype=torch.long)\n",
    "        intervened_target_batch = torch.stack(intervened_target_batch, dim=0)\n",
    "        intervened_swap_attr = torch.tensor(intervened_swap_attr, dtype=torch.long)\n",
    "        intervened_target_lengths_batch = torch.tensor(intervened_target_lengths_batch, dtype=torch.long)\n",
    "        \n",
    "        assert novel_attribute == True\n",
    "        main_dataset = TensorDataset(\n",
    "            # main dataset\n",
    "            main_input_batch, main_target_batch, main_situation_batch, main_agent_positions_batch, \n",
    "            main_target_positions_batch, main_input_lengths_batch, main_target_lengths_batch,\n",
    "            # dual dataset\n",
    "            dual_input_batch, dual_target_batch, dual_situation_batch, dual_agent_positions_batch,\n",
    "            dual_target_positions_batch, dual_input_lengths_batch, dual_target_lengths_batch,\n",
    "            # intervened dataset for novel attribute\n",
    "            intervened_main_swap_index, intervened_dual_swap_index, intervened_main_shape_index, \n",
    "            intervened_dual_shape_index, intervened_target_batch, intervened_swap_attr, \n",
    "            intervened_target_lengths_batch\n",
    "        )  \n",
    "        # with non-tensorized outputs\n",
    "        return main_dataset, (situation_representation_batch, derivation_representation_batch)\n",
    "        # the last two items are deprecated. we need to fix them to make them usable.\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        examples = self._examples\n",
    "        \n",
    "        # get length.\n",
    "        input_lengths = self._input_lengths\n",
    "        target_lengths = self._target_lengths\n",
    "        max_input_length = np.max(input_lengths)\n",
    "        max_target_length = np.max(target_lengths)\n",
    "        \n",
    "        # return structs\n",
    "        input_batch = []\n",
    "        target_batch = []\n",
    "        situation_batch = []\n",
    "        situation_representation_batch = []\n",
    "        derivation_representation_batch = []\n",
    "        agent_positions_batch = []\n",
    "        target_positions_batch = []\n",
    "\n",
    "        for example in examples:\n",
    "            to_pad_input = max_input_length - example[\"input_tensor\"].size(1)\n",
    "            to_pad_target = max_target_length - example[\"target_tensor\"].size(1)\n",
    "            padded_input = torch.cat([\n",
    "                example[\"input_tensor\"],\n",
    "                torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "            # padded_input = torch.cat([\n",
    "            #     torch.zeros_like(example[\"input_tensor\"], dtype=torch.long),\n",
    "            #     torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1) # TODO: change back\n",
    "            padded_target = torch.cat([\n",
    "                example[\"target_tensor\"],\n",
    "                torch.zeros(int(to_pad_target), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "            input_batch.append(padded_input)\n",
    "            target_batch.append(padded_target)\n",
    "            situation_batch.append(example[\"situation_tensor\"])\n",
    "            situation_representation_batch.append(example[\"situation_representation\"])\n",
    "            derivation_representation_batch.append(example[\"derivation_representation\"])\n",
    "            agent_positions_batch.append(example[\"agent_position\"])\n",
    "            target_positions_batch.append(example[\"target_position\"])\n",
    "        \n",
    "        input_batch = torch.cat(input_batch, dim=0)\n",
    "        target_batch = torch.cat(target_batch, dim=0)\n",
    "        situation_batch = torch.cat(situation_batch, dim=0)\n",
    "        agent_positions_batch = torch.cat(agent_positions_batch, dim=0)\n",
    "        target_positions_batch = torch.cat(target_positions_batch, dim=0)\n",
    "        input_lengths_batch = torch.tensor([[l] for l in input_lengths], dtype=torch.long)\n",
    "        target_lengths_batch = torch.tensor([[l] for l in target_lengths], dtype=torch.long)\n",
    "        main_dataset = TensorDataset(\n",
    "            input_batch, target_batch, situation_batch, agent_positions_batch, \n",
    "            target_positions_batch, input_lengths_batch, target_lengths_batch\n",
    "        )\n",
    "\n",
    "        # with non-tensorized outputs\n",
    "        return main_dataset, (situation_representation_batch, derivation_representation_batch)\n",
    "        \n",
    "            \n",
    "    \"\"\"\n",
    "    Deprecated. We may want to deprecate this function, so we want use multi-gpu settings.\n",
    "    \"\"\"\n",
    "    def get_data_iterator(self, batch_size=10) -> Tuple[torch.Tensor, List[int], torch.Tensor, List[dict],\n",
    "                                                        torch.Tensor, List[int], torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Iterate over batches of example tensors, pad them to the max length in the batch and yield.\n",
    "        :param batch_size: how many examples to put in each batch.\n",
    "        :param auxiliary_task: if true, also batches agent and target positions (flattened, so\n",
    "        agent row * agent columns = agent_position)\n",
    "        :return: tuple of input commands batch, corresponding input lengths, situation image batch,\n",
    "        list of corresponding situation representations, target commands batch and corresponding target lengths.\n",
    "        \"\"\"\n",
    "        for example_i in range(0, len(self._examples), batch_size):\n",
    "            if example_i + batch_size > len(self._examples):\n",
    "                batch_size = len(self._examples) - example_i\n",
    "            examples = self._examples[example_i:example_i + batch_size]\n",
    "            input_lengths = self._input_lengths[example_i:example_i + batch_size]\n",
    "            target_lengths = self._target_lengths[example_i:example_i + batch_size]\n",
    "            max_input_length = np.max(input_lengths)\n",
    "            max_target_length = np.max(target_lengths)\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "            situation_batch = []\n",
    "            situation_representation_batch = []\n",
    "            derivation_representation_batch = []\n",
    "            agent_positions_batch = []\n",
    "            target_positions_batch = []\n",
    "            for example in examples:\n",
    "                to_pad_input = max_input_length - example[\"input_tensor\"].size(1)\n",
    "                to_pad_target = max_target_length - example[\"target_tensor\"].size(1)\n",
    "                padded_input = torch.cat([\n",
    "                    example[\"input_tensor\"],\n",
    "                    torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "                # padded_input = torch.cat([\n",
    "                #     torch.zeros_like(example[\"input_tensor\"], dtype=torch.long),\n",
    "                #     torch.zeros(int(to_pad_input), dtype=torch.long).unsqueeze(0)], dim=1) # TODO: change back\n",
    "                padded_target = torch.cat([\n",
    "                    example[\"target_tensor\"],\n",
    "                    torch.zeros(int(to_pad_target), dtype=torch.long).unsqueeze(0)], dim=1)\n",
    "                input_batch.append(padded_input)\n",
    "                target_batch.append(padded_target)\n",
    "                situation_batch.append(example[\"situation_tensor\"])\n",
    "                situation_representation_batch.append(example[\"situation_representation\"])\n",
    "                derivation_representation_batch.append(example[\"derivation_representation\"])\n",
    "                agent_positions_batch.append(example[\"agent_position\"])\n",
    "                target_positions_batch.append(example[\"target_position\"])\n",
    "\n",
    "            yield (torch.cat(input_batch, dim=0), input_lengths, derivation_representation_batch,\n",
    "                   torch.cat(situation_batch, dim=0), situation_representation_batch, torch.cat(target_batch, dim=0),\n",
    "                   target_lengths, torch.cat(agent_positions_batch, dim=0), torch.cat(target_positions_batch, dim=0))\n",
    "\n",
    "    def read_dataset(self, max_examples=None, simple_situation_representation=True) -> {}:\n",
    "        \"\"\"\n",
    "        Loop over the data examples in GroundedScan and convert them to tensors, also save the lengths\n",
    "        for input and target sequences that are needed for padding.\n",
    "        :param max_examples: how many examples to read maximally, read all if None.\n",
    "        :param simple_situation_representation: whether to read the full situation image in RGB or the simplified\n",
    "        smaller representation.\n",
    "        \"\"\"\n",
    "        few_shots_ids = []\n",
    "        logger.info(\"Converting dataset to tensors...\")\n",
    "        if self.split == \"few_shot_single_clause_logic\" and self.k != 0:\n",
    "            logger.info(\"Removing examples for few-shots training test set...\")\n",
    "            path_to_few_shot_data = os.path.join(self.directory, f\"few-shot-inoculations-{self.k}.txt\")\n",
    "            logger.info(f\"Reading few-shot inoculation from file: {path_to_few_shot_data}...\")\n",
    "            few_shots_ids = json.load(open(path_to_few_shot_data, \"r\"))\n",
    "        \n",
    "        for i, example in enumerate(self.get_examples_with_image(self.split, simple_situation_representation)):\n",
    "            if i in few_shots_ids: # this is just for few-shot experiments.\n",
    "                continue\n",
    "            if max_examples:\n",
    "                if len(self._examples) > max_examples - 1:\n",
    "                    break\n",
    "            empty_example = {}\n",
    "            input_commands = example[\"input_command\"]\n",
    "            target_commands = example[\"target_command\"]\n",
    "            #equivalent_target_commands = example[\"equivalent_target_command\"]\n",
    "            situation_image = example[\"situation_image\"]\n",
    "            if i == 0:\n",
    "                self.image_dimensions = situation_image.shape[0]\n",
    "                self.image_channels = situation_image.shape[-1]\n",
    "            situation_repr = example[\"situation_representation\"]\n",
    "            input_array = self.sentence_to_array(input_commands, vocabulary=\"input\")\n",
    "            target_array = self.sentence_to_array(target_commands, vocabulary=\"target\")\n",
    "            #equivalent_target_array = self.sentence_to_array(equivalent_target_commands, vocabulary=\"target\")\n",
    "            empty_example[\"input_tensor\"] = torch.tensor(input_array, dtype=torch.long).unsqueeze(\n",
    "                dim=0)\n",
    "            empty_example[\"target_tensor\"] = torch.tensor(target_array, dtype=torch.long).unsqueeze(\n",
    "                dim=0)\n",
    "            #empty_example[\"equivalent_target_tensor\"] = torch.tensor(equivalent_target_array, dtype=torch.long).unsqueeze(dim=0)\n",
    "            empty_example[\"situation_tensor\"] = torch.tensor(situation_image, dtype=torch.float).unsqueeze(dim=0)\n",
    "            empty_example[\"situation_representation\"] = situation_repr\n",
    "            empty_example[\"derivation_representation\"] = example[\"derivation_representation\"]\n",
    "            empty_example[\"agent_position\"] = torch.tensor(\n",
    "                (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "            empty_example[\"target_position\"] = torch.tensor(\n",
    "                (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                int(situation_repr[\"target_object\"][\"position\"][\"column\"]),\n",
    "                dtype=torch.long).unsqueeze(dim=0)\n",
    "            empty_example[\"target_str\"] = example[\"target_str\"]\n",
    "            empty_example[\"adverb_in_command\"] = example[\"adverb_in_command\"]\n",
    "            empty_example[\"verb_in_command\"] = example[\"verb_in_command\"]\n",
    "            self._input_lengths = np.append(self._input_lengths, [len(input_array)])\n",
    "            self._target_lengths = np.append(self._target_lengths, [len(target_array)])\n",
    "            self._examples = np.append(self._examples, [empty_example])\n",
    "        \n",
    "        # we also need to load few-shots examples in case k is not 0.\n",
    "        if self.k != 0:\n",
    "            logger.info(\"Loading few examples into the training set for few-shots learning...\")\n",
    "            # Let us also record the few shots examples index, so in evaluation,\n",
    "            # we can move them out!\n",
    "            few_shot_single_clause_logic = self.data_json[\"examples\"][\"few_shot_single_clause_logic\"]\n",
    "            few_shots_ids = [i for i in range(len(few_shot_single_clause_logic))]\n",
    "            few_shots_ids = random.sample(few_shots_ids, self.k)\n",
    "            logger.info(\"The following idx examples are selected for few-shot learning:\")\n",
    "            logger.info(few_shots_ids)\n",
    "            with open(os.path.join(self.directory, f\"few-shot-inoculations-{self.k}.txt\"), \"w\") as fd:\n",
    "                json.dump(few_shots_ids, fd, indent=4)\n",
    "                \n",
    "            all_examples_few_shots_selected = []\n",
    "            for i, example in enumerate(\n",
    "                self.get_examples_with_image(\n",
    "                    \"few_shot_single_clause_logic\", simple_situation_representation\n",
    "                )\n",
    "            ):\n",
    "                if i in few_shots_ids:\n",
    "                    all_examples_few_shots_selected.append(example)\n",
    "            for i, example in enumerate(all_examples_few_shots_selected):\n",
    "                empty_example = {}\n",
    "                input_commands = example[\"input_command\"]\n",
    "                target_commands = example[\"target_command\"]\n",
    "                #equivalent_target_commands = example[\"equivalent_target_command\"]\n",
    "                situation_image = example[\"situation_image\"]\n",
    "                if i == 0:\n",
    "                    self.image_dimensions = situation_image.shape[0]\n",
    "                    self.image_channels = situation_image.shape[-1]\n",
    "                situation_repr = example[\"situation_representation\"]\n",
    "                input_array = self.sentence_to_array(input_commands, vocabulary=\"input\")\n",
    "                target_array = self.sentence_to_array(target_commands, vocabulary=\"target\")\n",
    "                #equivalent_target_array = self.sentence_to_array(equivalent_target_commands, vocabulary=\"target\")\n",
    "                empty_example[\"input_tensor\"] = torch.tensor(input_array, dtype=torch.long).unsqueeze(\n",
    "                    dim=0)\n",
    "                empty_example[\"target_tensor\"] = torch.tensor(target_array, dtype=torch.long).unsqueeze(\n",
    "                    dim=0)\n",
    "                #empty_example[\"equivalent_target_tensor\"] = torch.tensor(equivalent_target_array, dtype=torch.long).unsqueeze(dim=0)\n",
    "                empty_example[\"situation_tensor\"] = torch.tensor(situation_image, dtype=torch.float).unsqueeze(dim=0)\n",
    "                empty_example[\"situation_representation\"] = situation_repr\n",
    "                empty_example[\"derivation_representation\"] = example[\"derivation_representation\"]\n",
    "                empty_example[\"agent_position\"] = torch.tensor(\n",
    "                    (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                    int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "                empty_example[\"target_position\"] = torch.tensor(\n",
    "                    (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                    int(situation_repr[\"target_object\"][\"position\"][\"column\"]),\n",
    "                    dtype=torch.long).unsqueeze(dim=0)\n",
    "                self._input_lengths = np.append(self._input_lengths, [len(input_array)])\n",
    "                self._target_lengths = np.append(self._target_lengths, [len(target_array)])\n",
    "                self._examples = np.append(self._examples, [empty_example])\n",
    "\n",
    "    def sentence_to_array(self, sentence: List[str], vocabulary: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert each string word in a sentence to the corresponding integer from the vocabulary and append\n",
    "        a start-of-sequence and end-of-sequence token.\n",
    "        :param sentence: the sentence in words (strings)\n",
    "        :param vocabulary: whether to use the input or target vocabulary.\n",
    "        :return: the sentence in integers.\n",
    "        \"\"\"\n",
    "        vocab = self.get_vocabulary(vocabulary)\n",
    "        sentence_array = [vocab.sos_idx]\n",
    "        for word in sentence:\n",
    "            sentence_array.append(vocab.word_to_idx(word))\n",
    "        sentence_array.append(vocab.eos_idx)\n",
    "        return sentence_array\n",
    "\n",
    "    def array_to_sentence(self, sentence_array: List[int], vocabulary: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Translate each integer in a sentence array to the corresponding word.\n",
    "        :param sentence_array: array with integers representing words from the vocabulary.\n",
    "        :param vocabulary: whether to use the input or target vocabulary.\n",
    "        :return: the sentence in words.\n",
    "        \"\"\"\n",
    "        vocab = self.get_vocabulary(vocabulary)\n",
    "        return [vocab.idx_to_word(word_idx) for word_idx in sentence_array]\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return len(self._examples)\n",
    "\n",
    "    @property\n",
    "    def input_vocabulary_size(self):\n",
    "        return self.input_vocabulary.size\n",
    "\n",
    "    @property\n",
    "    def target_vocabulary_size(self):\n",
    "        return self.target_vocabulary.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 01:59 Formulating the dataset from the passed in json file...\n",
      "2021-09-27 01:59 Loading vocabularies...\n",
      "2021-09-27 01:59 Done loading vocabularies.\n",
      "2021-09-27 01:59 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/ReaSCAN-novel-attribute/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=100,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 1), ('1', 2)]\n",
      "[('5', 4), ('12', 2)]\n",
      "[('0', 4), ('1', 1)]\n",
      "[('5', 3), ('9', 4)]\n",
      "[('0', 3), ('1', 1)]\n",
      "[('0', 1), ('1', 4)]\n",
      "[('0', 4), ('1', 1)]\n",
      "[('0', 4), ('1', 1)]\n",
      "[('0', 2), ('1', 1)]\n",
      "[('0', 2), ('1', 3)]\n",
      "[('0', 4), ('1', 3)]\n",
      "[('0', 3), ('1', 2)]\n",
      "[('3', 4), ('7', 3)]\n",
      "[('0', 1), ('1', 4)]\n",
      "[('6', 2), ('11', 4)]\n",
      "[('0', 4), ('1', 3)]\n",
      "[('0', 4), ('1', 1)]\n",
      "[('2', 4), ('8', 2)]\n",
      "[('0', 4), ('1', 2)]\n",
      "[('0', 3), ('1', 4)]\n",
      "[('0', 4), ('1', 3)]\n",
      "[('0', 4), ('1', 3)]\n",
      "[('0', 4), ('1', 3)]\n",
      "[('0', 2), ('1', 3)]\n",
      "[('5', 2), ('11', 3)]\n",
      "[('0', 3), ('1', 2)]\n"
     ]
    }
   ],
   "source": [
    "train_data, _ = training_set.get_dual_dataset(novel_attribute=True)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d110814ef98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdual_input_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_situation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdual_agent_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdual_input_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_target_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 14)"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    input_sequence, target_sequence, situation, \\\n",
    "        agent_positions, target_positions, \\\n",
    "        input_lengths, target_lengths, \\\n",
    "        dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "        dual_agent_positions, dual_target_positions, \\\n",
    "        dual_input_lengths, dual_target_lengths = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
