{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from decode_graphical_models import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../../data-files/gSCAN-Simple/data-compositional-splits.txt\"\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 200\n",
    "agent_positions_batch = []\n",
    "target_positions_batch = []\n",
    "target_commands = []\n",
    "for ex in data_json[\"examples\"][\"train\"]:\n",
    "    target_commands += [ex[\"target_commands\"]]\n",
    "    situation_repr = ex['situation']\n",
    "    agent = torch.tensor(\n",
    "        (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    target = torch.tensor(\n",
    "        (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "        int(situation_repr[\"target_object\"][\"position\"][\"column\"]), dtype=torch.long).unsqueeze(dim=0)\n",
    "    agent_positions_batch.append(agent)\n",
    "    target_positions_batch.append(target)\n",
    "    if len(agent_positions_batch) == NUM:\n",
    "        break\n",
    "agent_positions_batch = torch.stack(agent_positions_batch, dim=0)\n",
    "target_positions_batch = torch.stack(target_positions_batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = hi_model(agent_positions_batch, target_positions_batch, tag=\"situation_encode\")\n",
    "actions = torch.zeros(hidden_states.size(0), 1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = []\n",
    "actions_length = torch.zeros(hidden_states.size(0), 1).long()\n",
    "for i in range(20):\n",
    "    hidden_states, actions = hi_model(\n",
    "        hmm_states=hidden_states, \n",
    "        hmm_actions=actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    actions_length += (actions!=0).long()\n",
    "    actions_sequence += [actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_sequence = torch.cat(actions_sequence, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(actions_sequence.size(0)):\n",
    "    pred = (hi_model.actions_list_to_sequence(actions_sequence[i,:actions_length[i]].tolist()))\n",
    "    actual = target_commands[i]\n",
    "    assert pred == actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try some interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 02:39 Formulating the dataset from the passed in json file...\n",
      "2021-08-21 02:39 Loading vocabularies...\n",
      "2021-08-21 02:39 Done loading vocabularies.\n",
      "2021-08-21 02:39 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "data_json = json.load(open(path_to_data, \"r\"))\n",
    "training_set = ReaSCANDataset(\n",
    "    data_json, \n",
    "    \"../../../data-files/gSCAN-Simple/\", split=\"train\",\n",
    "    input_vocabulary_file=\"input_vocabulary.txt\",\n",
    "    target_vocabulary_file=\"target_vocabulary.txt\",\n",
    "    generate_vocabulary=False, k=0\n",
    ")\n",
    "training_set.read_dataset(\n",
    "    max_examples=100,\n",
    "    simple_situation_representation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = training_set.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_model = HighLevelModel(\n",
    "    # None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and loop over it.\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # main batch\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "    high_hidden_states = hi_model(\n",
    "        agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    high_actions = torch.zeros(\n",
    "        high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    dual_high_hidden_states = hi_model(\n",
    "        agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "        target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "        tag=\"situation_encode\"\n",
    "    )\n",
    "    dual_high_actions = torch.zeros(\n",
    "        dual_high_hidden_states.size(0), 1\n",
    "    ).long()\n",
    "\n",
    "    break # just steal one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervene_time = 1\n",
    "intervene_attribute = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercepted dual hidden states.\n",
    "for j in range(intervene_time):\n",
    "    dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "        hmm_states=dual_high_hidden_states, \n",
    "        hmm_actions=dual_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_decoding_steps = 20\n",
    "# main intervene for loop.\n",
    "cf_high_hidden_states = high_hidden_states\n",
    "cf_high_actions = high_actions\n",
    "intervened_target_batch = [torch.ones(high_hidden_states.size(0), 1).long()] # SOS tokens\n",
    "intervened_target_lengths_batch = torch.zeros(high_hidden_states.size(0), 1).long()\n",
    "# we need to take of the SOS and EOS tokens.\n",
    "for j in range(train_max_decoding_steps-2):\n",
    "    # intercept like antra!\n",
    "    if j == intervene_time:\n",
    "        # only swap out this part.\n",
    "        cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "        # comment out two lines below if it is not for testing.\n",
    "        # cf_high_hidden_states = dual_high_hidden_states\n",
    "        # cf_high_actions = dual_high_actions\n",
    "    cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "        hmm_states=cf_high_hidden_states, \n",
    "        hmm_actions=cf_high_actions, \n",
    "        tag=\"_hmm_step_fxn\"\n",
    "    )\n",
    "    # record the output for loss calculation.\n",
    "    intervened_target_batch += [cf_high_actions]\n",
    "    intervened_target_lengths_batch += (cf_high_actions!=0).long()\n",
    "intervened_target_batch += [torch.zeros(high_hidden_states.size(0), 1).long()] # pad for extra eos\n",
    "intervened_target_lengths_batch += 2\n",
    "intervened_target_batch = torch.cat(intervened_target_batch, dim=-1)\n",
    "for i in range(high_hidden_states.size(0)):\n",
    "    intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 4, 6, 3, 6, 5, 5, 6, 6, 7, 4, 3, 6, 0, 3, 4, 0, 6, 3, 6, 7, 3, 4,\n",
       "        6, 4, 3, 3, 5, 3, 0, 5, 3, 5, 7, 7, 0, 4, 4, 5, 3, 3, 3, 0, 4, 3, 4, 7,\n",
       "        5, 4])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(intervened_target_batch[:,:target_batch.size(1)] != target_batch).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 2],\n",
       "        [1, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 2],\n",
       "        [1, 4, 4, 4, 5, 4, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 5, 4, 4, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 5, 4, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2],\n",
       "        [1, 4, 5, 4, 4, 4, 4, 4, 3, 4, 4, 2, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 4, 5, 4, 4, 2, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 4, 4, 3, 4, 4, 4, 2],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4],\n",
       "        [1, 4, 4, 3, 4, 4, 4, 4, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 5, 4, 4, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 5, 4, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 4, 4, 4, 3, 4, 4, 4, 4, 2, 0, 0],\n",
       "        [1, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 3, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 5, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4],\n",
       "        [1, 3, 5, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 5, 4, 4, 4, 4, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 4, 2],\n",
       "        [1, 4, 5, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 5, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 5, 4, 4, 4, 4, 4, 2, 0],\n",
       "        [1, 4, 3, 4, 4, 3, 4, 4, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 5, 4, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 3, 4, 4, 2, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 3, 4, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 3, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 3, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 5, 4, 4, 2, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 4, 4, 4, 4, 5, 4, 4, 4, 2, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4],\n",
       "        [1, 4, 5, 4, 4, 4, 3, 4, 4, 4, 4, 2, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 4, 5, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 4, 3, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 5, 4, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 5, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 5, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 3, 4, 2, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 4, 5, 4, 2, 0, 0],\n",
       "        [1, 4, 5, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 3, 5, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 4, 4, 4, 5, 4, 4, 4, 4, 2],\n",
       "        [1, 3, 4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 4, 3, 4, 3, 4, 4, 4, 2, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervened_target_batch[:,:target_batch.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0017, 0.2133, 0.3266],\n",
       "        [0.2905, 0.4855, 0.1845],\n",
       "        [0.8512, 0.1628, 0.3731]])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7027, 0.4703, 0.8446],\n",
       "        [0.9190, 0.3526, 0.1672],\n",
       "        [0.5256, 0.4236, 0.5546]])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,1]=b[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0017, 0.4703, 0.3266],\n",
       "        [0.2905, 0.3526, 0.1845],\n",
       "        [0.8512, 0.4236, 0.3731]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
