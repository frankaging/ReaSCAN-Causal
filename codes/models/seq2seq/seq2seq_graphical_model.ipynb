{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../../data-files/ReaSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-08 20:57 Formulating the dataset from the passed in json file...\n",
      "2021-07-08 20:57 Loading vocabularies...\n",
      "2021-07-08 20:57 Done loading vocabularies.\n"
     ]
    }
   ],
   "source": [
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/ReaSCAN-Simple/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (situation_encoder): ConvolutionalNet(\n",
       "    (conv_1): Conv2d(3, 50, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_2): Conv2d(3, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv_3): Conv2d(3, 50, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (relu): ReLU()\n",
       "    (layers): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (visual_attention): Attention(\n",
       "    (key_layer): Linear(in_features=150, out_features=100, bias=False)\n",
       "    (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "  )\n",
       "  (encoder): EncoderRNN(\n",
       "    EncoderRNN\n",
       "     bidirectional=True \n",
       "     num_layers=1\n",
       "     hidden_size=100\n",
       "     dropout=0.3\n",
       "     n_input_symbols=20\n",
       "    \n",
       "    (embedding): Embedding(20, 25, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(25, 100, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (enc_hidden_to_dec_hidden): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (textual_attention): Attention(\n",
       "    (key_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "  )\n",
       "  (attention_decoder): BahdanauAttentionDecoderRNN(\n",
       "    AttentionDecoderRNN\n",
       "     num_layers=1\n",
       "     hidden_size=100\n",
       "     dropout=0.3\n",
       "     num_output_symbols=7\n",
       "    \n",
       "    (queries_to_keys): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (embedding): Embedding(7, 100, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (lstm): LSTM(300, 100, dropout=0.3)\n",
       "    (textual_attention): Attention(\n",
       "      (key_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (visual_attention): Attention(\n",
       "      (key_layer): Linear(in_features=150, out_features=100, bias=False)\n",
       "      (query_layer): Linear(in_features=100, out_features=100, bias=False)\n",
       "      (energy_layer): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (output_to_hidden): Linear(in_features=400, out_features=100, bias=False)\n",
       "    (hidden_to_output): Linear(in_features=100, out_features=7, bias=False)\n",
       "  )\n",
       "  (loss_criterion): NLLLoss()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
