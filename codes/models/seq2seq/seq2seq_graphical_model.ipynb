{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import json\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from antra.antra import *\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# setting up the seeds.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ReaSCAN dataset to load config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-22 01:49 Formulating the dataset from the passed in json file...\n",
      "2021-07-22 01:49 Loading vocabularies...\n",
      "2021-07-22 01:49 Done loading vocabularies.\n",
      "2021-07-22 01:49 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"../../../data-files/ReaSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\"\n",
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")\n",
    "# Loading a couple of example from ReaSCAN.\n",
    "dataset.read_dataset(\n",
    "    max_examples=10,\n",
    "    simple_situation_representation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define computatinal graph of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_lstm_step_fxn(step_module, i):\n",
    "    \"\"\" \n",
    "    Generate a function for a layer in lstm.\n",
    "    \"\"\"\n",
    "\n",
    "    def _lstm_step_fxn(hidden_states):\n",
    "        (output, hidden, context_situation, attention_weights_commands,\n",
    "         attention_weights_situations) = step_module(\n",
    "            hidden_states[\"input_tokens_sorted\"][:, i], \n",
    "            hidden_states[\"hidden\"], \n",
    "            hidden_states[\"projected_keys_textual\"], \n",
    "            hidden_states[\"commands_lengths\"], \n",
    "            hidden_states[\"projected_keys_visual\"],\n",
    "        )\n",
    "        hidden_states[\"hidden\"] = hidden\n",
    "        hidden_states[\"return_lstm_output\"] += [output.unsqueeze(0)]\n",
    "        hidden_states[\"return_attention_weights\"] += [attention_weights_situations.unsqueeze(0)]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "    return _lstm_step_fxn\n",
    "\n",
    "def generate_compute_graph(model):\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input preparation.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Command Inputs.\n",
    "    \"\"\"\n",
    "    command_world_inputs = [\"commands_input\", \"commands_lengths\"]\n",
    "    command_world_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in command_world_inputs\n",
    "    ]\n",
    "    @GraphNode(*command_world_input_leaves, cache_results=False)\n",
    "    def command_input_preparation(\n",
    "        commands_input, commands_lengths,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"commands_input\": commands_input,\n",
    "            \"commands_lengths\": commands_lengths,\n",
    "        }\n",
    "        # We may not need the following fields. But we leave it here in case we need these\n",
    "        # to generate other inputs.\n",
    "        batch_size = input_dict[\"commands_input\"].shape[0]\n",
    "        device = input_dict[\"commands_input\"].device\n",
    "        return input_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation Inputs.\n",
    "    \"\"\"\n",
    "    situation_inputs = [\"situations_input\"]\n",
    "    situation_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in situation_inputs\n",
    "    ]\n",
    "    @GraphNode(*situation_input_leaves, cache_results=False)\n",
    "    def situation_input_preparation(\n",
    "        situations_input,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"situations_input\": situations_input,\n",
    "        }\n",
    "        return input_dict\n",
    "        \n",
    "    \"\"\"\n",
    "    Target Inputs\n",
    "    \"\"\"\n",
    "    target_sequence_inputs = [\"target_batch\", \"target_lengths\"]\n",
    "    target_sequence_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in target_sequence_inputs\n",
    "    ]\n",
    "    @GraphNode(*target_sequence_input_leaves, cache_results=False)\n",
    "    def target_sequence_input_preparation(\n",
    "        target_batch, target_lengths\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"target_batch\": target_batch,\n",
    "            \"target_lengths\": target_lengths,\n",
    "        }\n",
    "        return input_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input encoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Situation Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(situation_input_preparation)\n",
    "    def situation_encode(input_dict):\n",
    "        encoded_image = model.situation_encoder(\n",
    "            input_images=input_dict[\"situations_input\"]\n",
    "        )\n",
    "        return encoded_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Language Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_preparation)\n",
    "    def command_input_encode(input_dict):\n",
    "        hidden, encoder_outputs = model.encoder(\n",
    "            input_batch=input_dict[\"commands_input\"], \n",
    "            input_lengths=input_dict[\"commands_lengths\"],\n",
    "        )\n",
    "        output_dict = {\n",
    "            \"command_hidden\" : hidden,\n",
    "            \"command_encoder_outputs\" : encoder_outputs[\"encoder_outputs\"],\n",
    "            \"command_sequence_lengths\" : encoder_outputs[\"sequence_lengths\"],\n",
    "        }\n",
    "        return output_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Decoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Preparation of Decoding Data structure.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_encode, situation_encode, target_sequence_input_preparation)\n",
    "    def decode_input_preparation(c_encode, s_encode, target_sequence):\n",
    "        \"\"\"\n",
    "        The decoding step can be represented as:\n",
    "        h_T = f(h_T-1, C)\n",
    "        where h_i is the recurring hidden states, and C\n",
    "        is the static state representations.\n",
    "        \n",
    "        In this function, we want to abstract the C.\n",
    "        \"\"\"\n",
    "        \n",
    "        initial_hidden = model.attention_decoder.initialize_hidden(\n",
    "            model.tanh(model.enc_hidden_to_dec_hidden(c_encode[\"command_hidden\"])))\n",
    "        \n",
    "        \"\"\"\n",
    "        Renaming.\n",
    "        \"\"\"\n",
    "        input_tokens, input_lengths = target_sequence[\"target_batch\"], target_sequence[\"target_lengths\"]\n",
    "        init_hidden = initial_hidden\n",
    "        encoded_commands = c_encode[\"command_encoder_outputs\"]\n",
    "        commands_lengths = c_encode[\"command_sequence_lengths\"]\n",
    "        encoded_situations = s_encode\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping as well as getting the context-guided attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, max_time = input_tokens.size()\n",
    "        # Sort the sequences by length in descending order\n",
    "        input_lengths = torch.tensor(input_lengths, dtype=torch.long, device=device)\n",
    "        input_lengths, perm_idx = torch.sort(input_lengths, descending=True)\n",
    "        input_tokens_sorted = input_tokens.index_select(dim=0, index=perm_idx)\n",
    "        initial_h, initial_c = init_hidden\n",
    "        hidden = (initial_h.index_select(dim=1, index=perm_idx),\n",
    "                  initial_c.index_select(dim=1, index=perm_idx))\n",
    "        encoded_commands = encoded_commands.index_select(dim=1, index=perm_idx)\n",
    "        commands_lengths = torch.tensor(commands_lengths, device=device)\n",
    "        commands_lengths = commands_lengths.index_select(dim=0, index=perm_idx)\n",
    "        encoded_situations = encoded_situations.index_select(dim=0, index=perm_idx)\n",
    "\n",
    "        # For efficiency\n",
    "        projected_keys_visual = model.visual_attention.key_layer(\n",
    "            encoded_situations)  # [batch_size, situation_length, dec_hidden_dim]\n",
    "        projected_keys_textual = model.textual_attention.key_layer(\n",
    "            encoded_commands)  # [max_input_length, batch_size, dec_hidden_dim]\n",
    "        \n",
    "        return {\n",
    "            \"return_lstm_output\":[],\n",
    "            \"return_attention_weights\":[],\n",
    "            \"hidden\":hidden,\n",
    "            \"input_tokens_sorted\":input_tokens_sorted,\n",
    "            \"projected_keys_textual\":projected_keys_textual,\n",
    "            \"commands_lengths\":commands_lengths,\n",
    "            \"projected_keys_visual\":projected_keys_visual,\n",
    "            \"perm_idx\":perm_idx,\n",
    "            \"seq_lengths\":input_lengths,\n",
    "        }\n",
    "\n",
    "    hidden_layer = decode_input_preparation\n",
    "    \"\"\"\n",
    "    Here, we set to a static bound of decoding steps.\n",
    "    \"\"\"\n",
    "    max_time = 4\n",
    "    for i in range(max_time):\n",
    "        f = _generate_lstm_step_fxn(model.attention_decoder.forward_step, i)\n",
    "        hidden_layer = GraphNode(hidden_layer,\n",
    "                                 name=f\"lstm_step_{i}\",\n",
    "                                 forward=f)\n",
    "    \"\"\"\n",
    "    Formulating outputs.\n",
    "    \"\"\"\n",
    "    @GraphNode(hidden_layer)\n",
    "    def output_preparation(hidden_states):\n",
    "        hidden_states[\"return_lstm_output\"] = torch.cat(\n",
    "            hidden_states[\"return_lstm_output\"], dim=0)\n",
    "        hidden_states[\"return_attention_weights\"] = torch.cat(\n",
    "            hidden_states[\"return_attention_weights\"], dim=0)\n",
    "        \n",
    "        _, unperm_idx = hidden_states[\"perm_idx\"].sort(0)\n",
    "        hidden_states[\"return_lstm_output\"] = hidden_states[\"return_lstm_output\"].index_select(dim=1, index=unperm_idx)  # [max_time, batch_size, output_size]\n",
    "        hidden_states[\"seq_lengths\"] = hidden_states[\"seq_lengths\"][unperm_idx].tolist()\n",
    "        hidden_states[\"return_attention_weights\"] = hidden_states[\"return_attention_weights\"].index_select(dim=1, index=unperm_idx)\n",
    "        \n",
    "        decoder_output_batched = hidden_states[\"return_lstm_output\"]\n",
    "        context_situation = hidden_states[\"return_attention_weights\"]\n",
    "        decoder_output_batched = F.log_softmax(decoder_output_batched, dim=-1)\n",
    "        \n",
    "        if model.auxiliary_task:\n",
    "            pass # Not implemented yet.\n",
    "        else:\n",
    "            target_position_scores = torch.zeros(1), torch.zeros(1)\n",
    "            # We are not returning this as well, since it is not used...\n",
    "        \n",
    "        return decoder_output_batched.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "    \n",
    "    root = output_preparation # TODO: removing this and continue.\n",
    "    \n",
    "    return root\n",
    "    \n",
    "class ReaSCANMultiModalLSTMCompGraph(ComputationGraph):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        root = generate_compute_graph(model)\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model to the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/ReaSCAN-Simple/\"\n",
    ")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "g = ReaSCANMultiModalLSTMCompGraph(\n",
    "     model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some examples to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (input_batch, input_lengths, _, situation_batch, _, target_batch,\n",
    "     target_lengths, agent_positions, target_positions) in dataset.get_data_iterator(batch_size=2):\n",
    "#     target_scores, target_position_scores = model(\n",
    "#         commands_input=input_batch, commands_lengths=input_lengths,\n",
    "#         situations_input=situation_batch, target_batch=target_batch,\n",
    "#         target_lengths=target_lengths\n",
    "#     )\n",
    "    # print(target_scores)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3,  4,  5,  6,  7,  8,  9, 10,  2],\n",
       "        [ 1,  3,  4,  5, 11, 12,  2,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"commands_input\": input_batch, \n",
    "    \"commands_lengths\": input_lengths,\n",
    "    \"situations_input\": situation_batch,\n",
    "    \"target_batch\": target_batch,\n",
    "    \"target_lengths\": target_lengths,\n",
    "}\n",
    "all_in = GraphInput(input_dict, batched=True, batch_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[-2.1942e-01,  3.5031e-01,  2.8302e-01,  6.3546e-02, -3.9658e-02,\n",
      "          8.2039e-02, -2.6031e-01,  1.1407e-01,  3.5423e-02, -7.3205e-02,\n",
      "          2.2751e-02,  6.2365e-02, -5.6642e-02,  5.4872e-02,  1.8291e-01,\n",
      "          4.5804e-02, -1.5299e-01, -8.8536e-02,  1.8954e-01,  3.9032e-02,\n",
      "         -8.3567e-03,  1.2233e-02, -1.3982e-01, -1.5779e-03,  6.6031e-02,\n",
      "          1.9188e-01, -8.6329e-02,  1.6921e-01, -2.5367e-01, -1.3325e-01,\n",
      "          1.3773e-01, -1.4950e-01, -2.3418e-02, -9.7508e-02,  6.1760e-02,\n",
      "          1.4037e-02,  3.7396e-02,  1.9038e-01, -3.8300e-02,  3.8696e-02,\n",
      "          6.4354e-02, -1.0324e-01,  1.3277e-01, -5.0206e-02,  2.3116e-02,\n",
      "         -2.3093e-01, -6.3682e-02,  1.5918e-01,  3.4607e-02, -2.0302e-01,\n",
      "         -6.9659e-02,  1.5170e-01, -5.5256e-02,  3.2395e-02,  1.2660e-01,\n",
      "         -1.5917e-03,  2.2692e-01,  6.4513e-02,  8.0817e-02,  8.7244e-02,\n",
      "          7.2151e-02,  1.5747e-01, -1.2346e-01, -4.0435e-02, -9.8149e-02,\n",
      "         -7.3107e-02,  2.1308e-01, -5.9193e-02,  5.9529e-02,  3.0365e-03,\n",
      "         -8.9808e-02, -9.7464e-02,  3.1892e-03,  1.3669e-01, -2.6280e-01,\n",
      "         -1.3899e-02, -4.9243e-02,  8.3155e-02,  6.7136e-02, -2.1818e-01,\n",
      "         -1.2322e-01,  2.6834e-02, -8.3359e-02, -1.2199e-01,  1.9188e-02,\n",
      "         -8.0164e-02,  3.9502e-03,  1.9290e-01, -1.7055e-01,  3.4352e-01,\n",
      "          2.3982e-02, -8.6839e-02,  8.0277e-02, -5.2677e-02,  2.6666e-04,\n",
      "          2.7901e-02,  2.3013e-01, -2.1092e-01,  1.6005e-01, -2.9160e-01]],\n",
      "       grad_fn=<SplitBackward>), tensor([[-0.2167,  0.2841,  0.2368,  0.0029,  0.0511,  0.1607, -0.2183,  0.1068,\n",
      "         -0.0209, -0.0436,  0.0580, -0.0419, -0.0004,  0.0287,  0.2363,  0.0802,\n",
      "         -0.0353, -0.0977,  0.1777,  0.1432,  0.1163, -0.0557, -0.1670, -0.0137,\n",
      "          0.1110,  0.2199, -0.0771,  0.1274, -0.2078, -0.1311,  0.1479, -0.1123,\n",
      "          0.0019, -0.1519,  0.0560,  0.0255, -0.0230,  0.1252,  0.0116, -0.0407,\n",
      "          0.0064, -0.1254,  0.1936, -0.1037, -0.0810, -0.1549,  0.0195,  0.0760,\n",
      "          0.1818, -0.2647, -0.1018,  0.1212, -0.1083,  0.0478,  0.0835, -0.0630,\n",
      "          0.2067,  0.0541,  0.0855,  0.0405,  0.0419,  0.1963, -0.1213, -0.0597,\n",
      "         -0.0415, -0.0777,  0.2265, -0.1168,  0.1231,  0.0045, -0.1376, -0.1299,\n",
      "          0.0179,  0.1817, -0.2571, -0.0012, -0.0064,  0.0604,  0.1571, -0.1937,\n",
      "         -0.1429, -0.0149, -0.1732, -0.1023, -0.0344, -0.0312,  0.0263,  0.2177,\n",
      "         -0.1752,  0.3269, -0.0512, -0.0888, -0.0072, -0.0305, -0.0056,  0.0174,\n",
      "          0.1910, -0.2266,  0.1042, -0.3179]], grad_fn=<SplitBackward>)),)\n",
      "((tensor([[[-0.1536,  0.1988,  0.2890,  0.0570,  0.1060,  0.0217, -0.1083,\n",
      "          -0.0173,  0.0609,  0.0480,  0.1385, -0.0908, -0.0266,  0.0456,\n",
      "           0.1931, -0.0689,  0.0565, -0.0190,  0.1395,  0.0075,  0.1049,\n",
      "          -0.0371, -0.0543,  0.1172,  0.2593,  0.1764, -0.1141, -0.0206,\n",
      "          -0.2410, -0.2235,  0.1692, -0.1114, -0.0913, -0.0874,  0.0413,\n",
      "          -0.0308,  0.0705,  0.1980,  0.0154, -0.0658,  0.0518, -0.1405,\n",
      "           0.0985, -0.1382, -0.0867, -0.0833, -0.0757, -0.0098,  0.0993,\n",
      "          -0.0294, -0.0601,  0.0706, -0.1416, -0.0262,  0.0998,  0.0788,\n",
      "           0.0679,  0.0596,  0.0197,  0.0730, -0.0419,  0.2292, -0.0682,\n",
      "          -0.0533, -0.0873, -0.0618,  0.1654, -0.0673,  0.1079, -0.0233,\n",
      "          -0.1364, -0.2070,  0.0020, -0.0007, -0.1820, -0.0340, -0.0955,\n",
      "          -0.0496,  0.1254, -0.2485, -0.1088,  0.0341,  0.0350, -0.1082,\n",
      "          -0.0161, -0.1799,  0.0532,  0.1786, -0.0839,  0.1770,  0.0616,\n",
      "          -0.2994, -0.0199,  0.0458,  0.0938,  0.1987,  0.3312, -0.2266,\n",
      "          -0.0332, -0.1663],\n",
      "         [-0.1425,  0.1946,  0.2786,  0.0540,  0.1299,  0.0254, -0.1004,\n",
      "          -0.0134,  0.0600,  0.0391,  0.1278, -0.0871,  0.0064,  0.0478,\n",
      "           0.1994, -0.0635,  0.0544, -0.0137,  0.1357,  0.0036,  0.0998,\n",
      "          -0.0379, -0.0379,  0.1109,  0.2521,  0.1761, -0.1122, -0.0358,\n",
      "          -0.2283, -0.2241,  0.1695, -0.1115, -0.0957, -0.0738,  0.0535,\n",
      "          -0.0427,  0.0709,  0.2041,  0.0196, -0.0643,  0.0418, -0.1376,\n",
      "           0.1074, -0.1392, -0.0938, -0.0881, -0.0643, -0.0158,  0.1043,\n",
      "          -0.0288, -0.0478,  0.0726, -0.1388, -0.0472,  0.0914,  0.0775,\n",
      "           0.0637,  0.0602,  0.0223,  0.0697, -0.0451,  0.2319, -0.0485,\n",
      "          -0.0319, -0.0752, -0.0436,  0.1569, -0.0682,  0.1044, -0.0101,\n",
      "          -0.1549, -0.2103, -0.0005,  0.0083, -0.1671, -0.0396, -0.0989,\n",
      "          -0.0340,  0.1425, -0.2530, -0.1089,  0.0388,  0.0393, -0.1011,\n",
      "          -0.0201, -0.1833,  0.0509,  0.1860, -0.0843,  0.1834,  0.0709,\n",
      "          -0.3046, -0.0161,  0.0559,  0.0995,  0.1954,  0.3313, -0.2412,\n",
      "          -0.0296, -0.1688]]], grad_fn=<SplitBackward>), tensor([[[-0.0238,  0.0049,  0.2779, -0.0148,  0.1340,  0.0739, -0.2180,\n",
      "          -0.0187,  0.0755,  0.0462,  0.1535, -0.0124, -0.0156, -0.0647,\n",
      "           0.2163,  0.0732,  0.1503,  0.1072,  0.1460, -0.0342,  0.1804,\n",
      "           0.0562,  0.0602, -0.0468,  0.2282,  0.0977, -0.0882, -0.1079,\n",
      "          -0.1233,  0.0663,  0.1275,  0.0703,  0.0208, -0.0466,  0.0360,\n",
      "           0.1833, -0.0233,  0.1852,  0.1534, -0.1220, -0.1977, -0.1703,\n",
      "          -0.0167, -0.1990,  0.0166,  0.1343, -0.1515, -0.2478, -0.0537,\n",
      "          -0.0330, -0.0058,  0.1272, -0.0538, -0.3251,  0.0657,  0.0554,\n",
      "           0.2397,  0.1344,  0.0510,  0.1426, -0.1109,  0.2504, -0.0895,\n",
      "          -0.0104,  0.0961, -0.2205,  0.2350,  0.1347,  0.0247,  0.2047,\n",
      "          -0.1065, -0.1802, -0.0445, -0.0143, -0.0666, -0.2266, -0.2278,\n",
      "          -0.0815,  0.2300, -0.1720, -0.0389,  0.1163,  0.1478,  0.0540,\n",
      "           0.0593, -0.0596,  0.0403,  0.1712, -0.0710,  0.2018, -0.1483,\n",
      "          -0.0167, -0.0216, -0.0530,  0.1510,  0.2816,  0.1993, -0.2147,\n",
      "          -0.0971, -0.1576],\n",
      "         [-0.0073, -0.0026,  0.2577, -0.0164,  0.1693,  0.0778, -0.2004,\n",
      "          -0.0102,  0.0785,  0.0220,  0.1406, -0.0061,  0.0348, -0.0659,\n",
      "           0.2266,  0.0837,  0.1470,  0.1155,  0.1384, -0.0411,  0.1710,\n",
      "           0.0519,  0.0881, -0.0524,  0.2159,  0.0996, -0.0818, -0.1335,\n",
      "          -0.1169,  0.0595,  0.1270,  0.0657,  0.0161, -0.0249,  0.0488,\n",
      "           0.1668, -0.0237,  0.1975,  0.1644, -0.1179, -0.2187, -0.1576,\n",
      "          -0.0017, -0.2043,  0.0057,  0.1293, -0.1375, -0.2581, -0.0481,\n",
      "          -0.0326,  0.0109,  0.1341, -0.0483, -0.3655,  0.0497,  0.0509,\n",
      "           0.2341,  0.1316,  0.0534,  0.1358, -0.1165,  0.2553, -0.0627,\n",
      "           0.0195,  0.1188, -0.1990,  0.2223,  0.1307,  0.0204,  0.2207,\n",
      "          -0.1398, -0.1889, -0.0462, -0.0028, -0.0416, -0.2318, -0.2326,\n",
      "          -0.0600,  0.2581, -0.1771, -0.0350,  0.1311,  0.1571,  0.0600,\n",
      "           0.0503, -0.0591,  0.0344,  0.1791, -0.0726,  0.2125, -0.1323,\n",
      "          -0.0213, -0.0197, -0.0369,  0.1675,  0.2711,  0.1969, -0.2324,\n",
      "          -0.0866, -0.1627]]], grad_fn=<SplitBackward>), tensor([[[ 5.9868e-03,  1.1730e-01,  3.0074e-01,  8.6623e-02,  9.9349e-02,\n",
      "          -1.1474e-01, -9.9970e-02, -2.3363e-01,  7.1900e-03,  1.4696e-01,\n",
      "           1.9174e-01, -1.4763e-02, -1.4947e-01, -7.4005e-02,  1.1708e-01,\n",
      "          -2.7275e-02,  2.6914e-01,  1.7192e-01, -6.4846e-02, -1.2357e-01,\n",
      "           2.5746e-01, -1.2323e-01, -1.1246e-02,  6.7544e-02,  2.6971e-01,\n",
      "          -1.4108e-01, -2.1993e-02, -2.0159e-01,  5.0181e-02,  9.9150e-02,\n",
      "           5.9924e-02, -7.0866e-02,  4.0421e-02,  2.0823e-02,  6.8742e-02,\n",
      "           1.5390e-01,  1.0575e-02, -5.4437e-02, -1.2209e-01, -1.1415e-01,\n",
      "          -3.8002e-02, -1.8440e-02, -9.5728e-02, -8.3756e-02,  2.3755e-01,\n",
      "           1.0407e-01,  2.5417e-02, -4.5129e-01, -1.0941e-01,  6.5923e-02,\n",
      "          -4.8753e-02,  6.4191e-02,  1.4824e-01, -2.2613e-01,  1.6996e-01,\n",
      "           1.3517e-01,  1.8422e-01,  2.9441e-01,  1.4253e-01,  6.7773e-02,\n",
      "          -2.3337e-01,  3.1689e-01, -1.2674e-01, -8.9996e-02, -1.2300e-01,\n",
      "           1.6775e-02,  2.0648e-01,  1.5094e-01, -2.3010e-02,  2.7931e-01,\n",
      "          -1.8900e-01, -1.9099e-01, -1.1246e-02,  4.6959e-02, -3.0385e-02,\n",
      "          -4.7083e-02, -2.0152e-02,  6.6104e-02,  2.0449e-01, -9.1356e-02,\n",
      "           3.6579e-02, -1.8125e-02,  1.8550e-01,  2.1989e-01,  7.8916e-02,\n",
      "           1.3918e-01, -2.8744e-03,  3.4228e-02,  1.6968e-01, -2.4704e-02,\n",
      "          -4.0726e-03,  1.1486e-01,  1.1372e-01, -1.6840e-01,  9.4102e-02,\n",
      "           1.7898e-01,  2.2346e-01, -1.3092e-01, -6.3555e-02,  1.3051e-01],\n",
      "         [ 1.8526e-02,  1.0106e-01,  2.5510e-01,  8.7304e-02,  1.4877e-01,\n",
      "          -1.1273e-01, -8.3935e-02, -2.1609e-01,  1.9928e-02,  8.0319e-02,\n",
      "           1.7519e-01,  1.4689e-02, -8.0270e-02, -8.3360e-02,  1.3010e-01,\n",
      "           1.9786e-04,  2.6307e-01,  1.7935e-01, -8.5006e-02, -1.3020e-01,\n",
      "           2.4570e-01, -1.3741e-01,  1.9919e-02,  6.2857e-02,  2.4618e-01,\n",
      "          -1.3354e-01, -9.3866e-03, -2.3436e-01,  5.0370e-02,  7.8886e-02,\n",
      "           5.5183e-02, -8.3343e-02,  3.7179e-02,  4.2487e-02,  9.3873e-02,\n",
      "           1.2916e-01,  6.3738e-03, -3.2551e-02, -9.9239e-02, -1.0302e-01,\n",
      "          -7.6504e-02, -5.4750e-04, -7.4640e-02, -9.6957e-02,  2.2575e-01,\n",
      "           9.3423e-02,  4.5547e-02, -4.6907e-01, -1.0363e-01,  6.3726e-02,\n",
      "          -2.4547e-02,  8.2090e-02,  1.6162e-01, -2.8638e-01,  1.3011e-01,\n",
      "           1.2005e-01,  1.7618e-01,  2.8456e-01,  1.4923e-01,  5.7387e-02,\n",
      "          -2.4295e-01,  3.2564e-01, -6.6675e-02, -5.8015e-02, -8.1539e-02,\n",
      "           4.7099e-02,  1.8903e-01,  1.3646e-01, -3.1878e-02,  2.7905e-01,\n",
      "          -2.1942e-01, -1.9496e-01, -1.1568e-02,  6.3637e-02,  9.7969e-03,\n",
      "          -5.4493e-02, -2.8469e-02,  8.8376e-02,  2.3868e-01, -1.0430e-01,\n",
      "           4.6451e-02,  1.1638e-02,  2.0422e-01,  2.2568e-01,  6.1617e-02,\n",
      "           1.7048e-01, -1.0013e-02,  4.5515e-02,  1.6356e-01, -1.1756e-02,\n",
      "           2.6341e-02,  1.1359e-01,  1.1173e-01, -1.4125e-01,  1.1527e-01,\n",
      "           1.5998e-01,  2.1468e-01, -1.5993e-01, -4.0814e-02,  1.1831e-01]]],\n",
      "       grad_fn=<SplitBackward>), tensor([[[-5.7297e-02, -2.7712e-02,  1.9893e-01,  1.2598e-03,  1.7501e-02,\n",
      "          -6.9164e-02, -4.3143e-02, -4.7449e-02, -5.2341e-03,  4.7123e-02,\n",
      "          -1.8698e-02, -6.7803e-02, -4.9229e-02,  9.6911e-02, -1.2219e-01,\n",
      "          -1.1813e-01,  2.7583e-01,  2.5334e-02,  4.4365e-03, -9.9438e-02,\n",
      "           2.5955e-01, -5.9768e-02,  1.4313e-02,  9.7207e-02,  4.7668e-02,\n",
      "          -1.3986e-01, -4.7068e-03, -2.0139e-02,  2.4094e-01,  1.6126e-01,\n",
      "          -2.7330e-02, -6.5991e-02,  2.8876e-02, -5.0405e-03, -2.7316e-02,\n",
      "           1.1712e-01, -5.9483e-02, -1.7706e-01, -1.9321e-01, -2.5209e-02,\n",
      "           1.2053e-02,  1.3343e-01, -6.6623e-02, -6.7496e-02,  1.2774e-01,\n",
      "           1.1436e-02,  1.5293e-01, -9.1172e-02, -4.3348e-02,  1.7649e-02,\n",
      "           4.7212e-02, -6.2935e-02,  1.6963e-01, -1.6317e-01,  1.3794e-01,\n",
      "           3.7784e-02,  6.4471e-02,  1.1000e-01,  6.6344e-02,  1.5273e-01,\n",
      "          -5.2763e-02,  1.4382e-01, -7.2903e-02, -3.7556e-02, -1.7760e-01,\n",
      "           3.5215e-02, -1.6864e-02,  5.2697e-02,  6.1663e-02,  5.9512e-02,\n",
      "          -1.0958e-01,  4.8095e-02, -5.9025e-03,  1.4954e-01,  1.0893e-02,\n",
      "          -5.8979e-02,  2.2946e-02, -1.2100e-02, -5.1187e-03, -5.2104e-02,\n",
      "          -3.0298e-02, -5.2376e-02,  1.9359e-02,  2.2448e-01, -1.4184e-01,\n",
      "           1.1444e-01, -1.3547e-01, -1.4954e-01,  2.1371e-01, -2.6572e-02,\n",
      "           3.4884e-02,  3.4353e-02,  2.3255e-01,  6.0241e-03, -1.0292e-01,\n",
      "           1.1531e-01, -2.8796e-02,  1.9740e-01,  2.2376e-02,  1.6602e-01],\n",
      "         [-4.5440e-02, -5.5119e-02,  7.3273e-02,  4.4213e-03,  7.7216e-02,\n",
      "          -7.9252e-02, -1.2712e-02, -9.9054e-03,  2.0484e-02, -1.7437e-02,\n",
      "          -4.3586e-02,  9.3168e-03,  2.9064e-02,  8.1098e-02, -1.1436e-01,\n",
      "          -6.5512e-02,  2.6347e-01,  3.3552e-02, -4.0104e-02, -1.0188e-01,\n",
      "           2.3285e-01, -8.6532e-02,  7.4049e-02,  9.3066e-02, -6.0833e-03,\n",
      "          -1.2206e-01,  1.7269e-02, -9.4892e-02,  1.9394e-01,  9.9549e-02,\n",
      "          -4.4559e-02, -9.3487e-02,  3.1686e-02,  1.5856e-02,  1.6831e-02,\n",
      "           5.8887e-02, -7.0148e-02, -1.5058e-01, -1.4203e-01,  1.3883e-03,\n",
      "          -6.1816e-02,  1.8826e-01, -7.1714e-03, -1.1131e-01,  1.0915e-01,\n",
      "           8.9588e-03,  1.6577e-01, -1.1338e-01, -4.1609e-02,  5.1947e-03,\n",
      "           6.5051e-02, -2.4293e-02,  1.7841e-01, -2.4748e-01,  8.6556e-02,\n",
      "          -1.6208e-02,  5.3716e-02,  9.6347e-02,  7.5645e-02,  1.3539e-01,\n",
      "          -8.8023e-02,  1.6029e-01,  7.2743e-02, -6.5386e-03, -9.5756e-02,\n",
      "           7.9782e-02, -2.6340e-02,  2.4291e-02,  5.4422e-02,  3.2862e-02,\n",
      "          -1.4288e-01,  5.4564e-02, -3.2359e-03,  1.8158e-01,  4.2697e-02,\n",
      "          -6.3748e-02, -2.4828e-04,  1.5335e-02,  7.9828e-02, -8.1017e-02,\n",
      "          -8.2353e-03, -3.9862e-03,  5.5406e-02,  2.3006e-01, -1.8112e-01,\n",
      "           2.2378e-01, -1.3158e-01, -1.4024e-01,  2.0129e-01, -2.0462e-02,\n",
      "           8.1167e-02,  3.2912e-02,  2.2096e-01,  7.3498e-02, -7.8306e-02,\n",
      "           7.3816e-02, -5.6716e-02,  1.5367e-01,  7.2454e-02,  1.2878e-01]]],\n",
      "       grad_fn=<SplitBackward>), tensor([[[-0.1229,  0.1445,  0.2233, -0.0962, -0.0362, -0.1078, -0.1147,\n",
      "          -0.0883, -0.0581, -0.0365, -0.1024,  0.0344, -0.1254,  0.1058,\n",
      "          -0.0597, -0.0549,  0.2305,  0.0716,  0.1420, -0.0027,  0.3183,\n",
      "          -0.0905, -0.0425,  0.0191,  0.0516,  0.1485, -0.0588,  0.1636,\n",
      "           0.2605,  0.0314,  0.0513, -0.0642, -0.2640, -0.1219, -0.1667,\n",
      "           0.1332,  0.0267,  0.0269, -0.2480, -0.0924, -0.0227,  0.0040,\n",
      "           0.0152, -0.1956,  0.0969, -0.1131,  0.2222, -0.0563,  0.0021,\n",
      "           0.0159,  0.0526, -0.1167,  0.1974,  0.0476,  0.1883, -0.0247,\n",
      "           0.0833,  0.1249,  0.1269,  0.1671,  0.0098,  0.0562, -0.1261,\n",
      "           0.1142, -0.2367,  0.0318,  0.0356,  0.1012,  0.1330,  0.0361,\n",
      "          -0.2191, -0.0448, -0.1235,  0.0842, -0.1018, -0.0348, -0.0210,\n",
      "           0.0889, -0.1143, -0.1409, -0.0591, -0.0880, -0.2635,  0.0555,\n",
      "           0.0788,  0.0107, -0.0922,  0.1693,  0.0612,  0.1991,  0.0125,\n",
      "           0.2387,  0.0678,  0.0074, -0.1102,  0.0890,  0.1098,  0.0257,\n",
      "           0.0942, -0.0227],\n",
      "         [-0.0884,  0.1312, -0.1324,  0.0254,  0.1081, -0.0732, -0.1349,\n",
      "          -0.1405,  0.0917, -0.0772, -0.1575,  0.1514,  0.1033,  0.0830,\n",
      "          -0.0437,  0.0960,  0.2499,  0.0252,  0.1405, -0.0205,  0.0758,\n",
      "          -0.0431, -0.0112,  0.0798, -0.0409,  0.0208,  0.0080, -0.1859,\n",
      "           0.0841,  0.0612,  0.0059, -0.1573, -0.1809, -0.0689,  0.0532,\n",
      "           0.1002, -0.1607, -0.0255, -0.1523,  0.0970, -0.2017,  0.0692,\n",
      "           0.0598, -0.1471,  0.0321, -0.0986,  0.2219, -0.1262,  0.0449,\n",
      "          -0.0178, -0.0650,  0.1696,  0.1774, -0.1176, -0.0627, -0.0996,\n",
      "           0.1967, -0.0530,  0.1789,  0.1337, -0.0917,  0.1297, -0.0020,\n",
      "           0.0430, -0.0454,  0.1813,  0.1093, -0.0244,  0.1074, -0.1275,\n",
      "          -0.2336,  0.0964, -0.1592,  0.1460, -0.1122,  0.0133, -0.0121,\n",
      "           0.0582,  0.1700, -0.1774, -0.0980, -0.1342, -0.1408,  0.0603,\n",
      "           0.0547,  0.1001,  0.0461, -0.0740, -0.0566,  0.3051,  0.0580,\n",
      "           0.2439,  0.1009,  0.0952, -0.1495, -0.0213, -0.0519, -0.0279,\n",
      "           0.0923, -0.2016]]], grad_fn=<SplitBackward>), tensor([[[-0.0600, -0.0385,  0.2363, -0.0435, -0.1472, -0.0617, -0.0042,\n",
      "           0.1390, -0.0035, -0.1360, -0.1523,  0.1083, -0.1586, -0.0523,\n",
      "           0.0651,  0.0466,  0.0992,  0.1339,  0.0638, -0.1668,  0.1248,\n",
      "          -0.0762, -0.2474,  0.0251,  0.0827,  0.0996, -0.0953,  0.0196,\n",
      "           0.2238,  0.0941,  0.0228,  0.0207, -0.2245, -0.0288, -0.1006,\n",
      "           0.0734,  0.1125, -0.0405, -0.0841, -0.0370,  0.0751,  0.0276,\n",
      "           0.0231, -0.2678,  0.0676, -0.0864, -0.0055, -0.0997,  0.1303,\n",
      "          -0.1683,  0.0056, -0.0265,  0.0751, -0.0889, -0.0417, -0.1314,\n",
      "           0.1878,  0.1044,  0.0548, -0.0266, -0.2332,  0.1024, -0.1662,\n",
      "          -0.0208, -0.1831,  0.0637, -0.0514, -0.0596, -0.0176,  0.0867,\n",
      "          -0.1143,  0.0062, -0.0439,  0.0872, -0.0988, -0.2199, -0.1017,\n",
      "           0.0202, -0.0371, -0.1215, -0.1470,  0.1246, -0.1180, -0.0159,\n",
      "          -0.0221, -0.0753,  0.0851,  0.0609, -0.1919,  0.2194,  0.0174,\n",
      "          -0.0040,  0.2269, -0.0720,  0.0290,  0.1309,  0.1217, -0.0286,\n",
      "           0.1175, -0.1055],\n",
      "         [-0.2871, -0.0866,  0.0831, -0.1773,  0.0774,  0.0006, -0.1391,\n",
      "           0.1036, -0.1098,  0.0964, -0.0625, -0.0672, -0.0505, -0.0036,\n",
      "           0.0144,  0.2232,  0.0586,  0.0262,  0.0636, -0.0474,  0.1314,\n",
      "           0.0442, -0.0527,  0.1833,  0.0401,  0.2018, -0.0175, -0.0132,\n",
      "           0.1619,  0.2442, -0.0748, -0.0014,  0.0652, -0.1295, -0.2255,\n",
      "          -0.0080, -0.1468, -0.1619, -0.1323,  0.0187,  0.0466,  0.1691,\n",
      "           0.1490, -0.1169, -0.0516,  0.0105,  0.1513, -0.1165,  0.1976,\n",
      "          -0.1282, -0.1045, -0.0086, -0.0342, -0.0014, -0.0242, -0.1330,\n",
      "           0.0538,  0.1018,  0.2333,  0.1244, -0.1550,  0.0088, -0.0480,\n",
      "           0.0346,  0.0302,  0.1005,  0.0541, -0.0388,  0.0902, -0.0250,\n",
      "          -0.0012, -0.0760,  0.0726, -0.1317, -0.1137, -0.0840,  0.1007,\n",
      "          -0.0366,  0.2359, -0.0360,  0.0173, -0.0371, -0.2151, -0.0780,\n",
      "          -0.0942,  0.1529, -0.0207, -0.0563,  0.0025,  0.2459, -0.0978,\n",
      "           0.2596,  0.0999,  0.0600, -0.0248, -0.0641, -0.1142, -0.0877,\n",
      "           0.0536, -0.0522]]], grad_fn=<SplitBackward>), tensor([[[-0.1576, -0.0774,  0.3355,  0.1818, -0.1875,  0.0885,  0.0591,\n",
      "           0.1269, -0.1176,  0.0200,  0.0402,  0.0326, -0.1719, -0.1317,\n",
      "           0.0030, -0.0700,  0.0465,  0.1664,  0.0353, -0.2551, -0.0240,\n",
      "           0.0314, -0.1204,  0.1742,  0.0802,  0.1084, -0.0731, -0.1418,\n",
      "           0.1376,  0.0796,  0.0062,  0.0980,  0.0519,  0.0557, -0.1444,\n",
      "           0.1304,  0.1700, -0.2989, -0.2395, -0.0990,  0.0260, -0.0359,\n",
      "          -0.0179, -0.1574,  0.0032, -0.1070,  0.0883, -0.0868,  0.1391,\n",
      "           0.0013, -0.0170,  0.0807, -0.0348, -0.1230,  0.2077,  0.0843,\n",
      "           0.2344,  0.0116,  0.0498, -0.1160, -0.1592, -0.0795, -0.1361,\n",
      "          -0.0480,  0.0839,  0.1280, -0.0265,  0.1100, -0.0534,  0.1874,\n",
      "           0.0237,  0.0338, -0.0118,  0.0557,  0.0762, -0.0533, -0.0291,\n",
      "           0.1383,  0.1925, -0.2908,  0.0918,  0.1093,  0.0940, -0.0568,\n",
      "          -0.0426,  0.1936,  0.0267,  0.0090, -0.0029,  0.3326,  0.1121,\n",
      "           0.0502,  0.1265, -0.1167,  0.0725,  0.1661,  0.0040,  0.0578,\n",
      "           0.1324, -0.0869],\n",
      "         [-0.2032,  0.1826,  0.0520, -0.0747, -0.0814, -0.0768, -0.1252,\n",
      "           0.0653, -0.0888, -0.0362, -0.0011,  0.0087,  0.0752,  0.0841,\n",
      "           0.0638,  0.0927, -0.0852, -0.0679,  0.1001,  0.0702,  0.1829,\n",
      "          -0.0269,  0.0203,  0.1038, -0.0818,  0.1033, -0.0279, -0.0256,\n",
      "          -0.0395,  0.0168, -0.0114, -0.0887, -0.0616, -0.1537, -0.1762,\n",
      "          -0.0310, -0.1291, -0.0039, -0.0218,  0.0742, -0.1389,  0.0801,\n",
      "           0.2280, -0.0187, -0.0579, -0.1098,  0.1920,  0.1763,  0.2184,\n",
      "          -0.1526, -0.0292,  0.0567, -0.0336, -0.0510, -0.0920, -0.2004,\n",
      "           0.0356,  0.0233,  0.0681, -0.0007,  0.1112,  0.0692, -0.1021,\n",
      "          -0.0378, -0.1039,  0.0678,  0.0848,  0.0019,  0.1174,  0.0018,\n",
      "          -0.0302, -0.0188, -0.0066,  0.0975, -0.2697, -0.0093,  0.0497,\n",
      "           0.0421,  0.0075, -0.1333, -0.1886, -0.0697, -0.2024, -0.0211,\n",
      "          -0.0400,  0.0137, -0.0240,  0.0953, -0.0490,  0.2639, -0.0722,\n",
      "           0.2137,  0.1269,  0.0629, -0.0463, -0.0941,  0.0124, -0.0927,\n",
      "           0.1145, -0.2155]]], grad_fn=<SplitBackward>), tensor([[[-0.1535, -0.0896,  0.3423,  0.0059, -0.1961,  0.0353,  0.0369,\n",
      "           0.3263, -0.1492,  0.0455,  0.0607, -0.1747, -0.1782,  0.0227,\n",
      "          -0.1379,  0.0330,  0.1107,  0.2735,  0.0812, -0.2108, -0.0052,\n",
      "          -0.1009, -0.1163,  0.2113, -0.0913,  0.0321, -0.0731,  0.0938,\n",
      "           0.1702,  0.2635, -0.1124,  0.0845, -0.0943,  0.0133, -0.0639,\n",
      "          -0.0595,  0.2874, -0.1535, -0.1168, -0.0845,  0.0478, -0.0291,\n",
      "          -0.0658, -0.1383,  0.0792, -0.1205,  0.1021,  0.0353, -0.0354,\n",
      "           0.0547,  0.0054, -0.0122,  0.0922, -0.2589,  0.1755, -0.0755,\n",
      "           0.0201,  0.0474,  0.0171,  0.0578, -0.1536, -0.0164, -0.0948,\n",
      "           0.1538, -0.0868,  0.0861, -0.0496,  0.0289, -0.1181,  0.1814,\n",
      "          -0.0377,  0.0765,  0.0516,  0.1041,  0.0071, -0.2781, -0.0799,\n",
      "           0.1204,  0.0150, -0.0778, -0.1081,  0.1163,  0.1878, -0.0620,\n",
      "           0.1210,  0.0931,  0.0772,  0.1533,  0.2169,  0.1969,  0.2038,\n",
      "          -0.0775,  0.0810, -0.2817, -0.0284, -0.0449,  0.1576,  0.0312,\n",
      "          -0.0013,  0.0324],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000]]], grad_fn=<SplitBackward>), tensor([[[-0.3156,  0.2054,  0.2021,  0.0176, -0.1128, -0.1058, -0.1291,\n",
      "           0.0638, -0.0672,  0.0189, -0.0730,  0.0343, -0.0138,  0.1248,\n",
      "          -0.0482,  0.0754, -0.0520,  0.0723, -0.0815, -0.1622,  0.0238,\n",
      "           0.0896, -0.0890,  0.0792, -0.0735,  0.1130,  0.0613, -0.2007,\n",
      "           0.1066,  0.1393, -0.1324, -0.1493,  0.0492,  0.0768, -0.0138,\n",
      "          -0.0105,  0.1329,  0.0040, -0.0065,  0.0944, -0.2012,  0.1473,\n",
      "           0.1912, -0.0300,  0.0989, -0.2427,  0.0306,  0.1540,  0.0184,\n",
      "           0.0568, -0.1011,  0.1890,  0.0486, -0.2652,  0.1061, -0.0207,\n",
      "           0.1406, -0.0219,  0.0909,  0.0978, -0.0420, -0.1058,  0.1228,\n",
      "           0.0510,  0.0205,  0.2811, -0.0005,  0.0146, -0.0184, -0.0633,\n",
      "          -0.0100,  0.0997, -0.0358,  0.1319, -0.1060,  0.0201, -0.0099,\n",
      "           0.1611,  0.0177, -0.1641, -0.0624,  0.0821,  0.1095,  0.0962,\n",
      "          -0.0229,  0.2010, -0.0443, -0.1308,  0.0253,  0.4138,  0.1514,\n",
      "           0.1808,  0.2276, -0.0988, -0.0881, -0.0231, -0.1777, -0.0035,\n",
      "           0.1899,  0.0672],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000]]], grad_fn=<SplitBackward>), tensor([[[-0.1949,  0.2446,  0.0878, -0.0170, -0.1483, -0.1518, -0.1593,\n",
      "           0.0764, -0.0335, -0.0747, -0.0471,  0.1167,  0.0520,  0.1125,\n",
      "           0.0166,  0.0637, -0.2050, -0.0535,  0.1082, -0.0378,  0.0531,\n",
      "           0.0402,  0.0638,  0.1097, -0.1340,  0.0749, -0.0352,  0.0010,\n",
      "          -0.0727,  0.0140, -0.0212, -0.1260, -0.0913, -0.0857, -0.1582,\n",
      "          -0.0543, -0.0683,  0.0674, -0.0675,  0.1552, -0.0909,  0.1051,\n",
      "           0.1760,  0.0339,  0.0392, -0.1908,  0.1202,  0.2535,  0.0762,\n",
      "          -0.0903,  0.0153,  0.0892,  0.0223, -0.0874, -0.0573, -0.1403,\n",
      "           0.0516,  0.0343,  0.0660,  0.0428,  0.1383,  0.0330, -0.0846,\n",
      "           0.0029, -0.1484,  0.0906,  0.0629,  0.0586,  0.0504,  0.0135,\n",
      "          -0.0009,  0.0103, -0.0239,  0.0616, -0.2604, -0.0277,  0.0035,\n",
      "           0.0806, -0.0654, -0.1623, -0.1690, -0.0232, -0.1082, -0.0338,\n",
      "           0.0096, -0.0386, -0.0486,  0.0778, -0.0448,  0.2868,  0.0122,\n",
      "           0.2105,  0.2183,  0.0509, -0.0348, -0.0870,  0.0516, -0.0916,\n",
      "           0.1739, -0.1916],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000]]], grad_fn=<SplitBackward>)),)\n",
      "([[10], [7]],)\n"
     ]
    }
   ],
   "source": [
    "target_scores = g.compute_node(\"command_input_encode\", all_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('commands_input', (1, 3, 4, 5, 6, 7, 8, 9, 10, 2)),\n",
       "  ('commands_lengths', 10.0),\n",
       "  ('situations_input',\n",
       "   (((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)))),\n",
       "  ('target_batch',\n",
       "   (1,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    4,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    4,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    5,\n",
       "    4,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    4,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    3,\n",
       "    4,\n",
       "    2)),\n",
       "  ('target_lengths',\n",
       "   28.0)): {'command_hidden': (tensor([[-2.1942e-01,  3.5031e-01,  2.8302e-01,  6.3546e-02, -3.9658e-02,\n",
       "             8.2039e-02, -2.6031e-01,  1.1407e-01,  3.5423e-02, -7.3205e-02,\n",
       "             2.2751e-02,  6.2365e-02, -5.6642e-02,  5.4872e-02,  1.8291e-01,\n",
       "             4.5804e-02, -1.5299e-01, -8.8536e-02,  1.8954e-01,  3.9032e-02,\n",
       "            -8.3567e-03,  1.2233e-02, -1.3982e-01, -1.5779e-03,  6.6031e-02,\n",
       "             1.9188e-01, -8.6329e-02,  1.6921e-01, -2.5367e-01, -1.3325e-01,\n",
       "             1.3773e-01, -1.4950e-01, -2.3418e-02, -9.7508e-02,  6.1760e-02,\n",
       "             1.4037e-02,  3.7396e-02,  1.9038e-01, -3.8300e-02,  3.8696e-02,\n",
       "             6.4354e-02, -1.0324e-01,  1.3277e-01, -5.0206e-02,  2.3116e-02,\n",
       "            -2.3093e-01, -6.3682e-02,  1.5918e-01,  3.4607e-02, -2.0302e-01,\n",
       "            -6.9659e-02,  1.5170e-01, -5.5256e-02,  3.2395e-02,  1.2660e-01,\n",
       "            -1.5917e-03,  2.2692e-01,  6.4513e-02,  8.0817e-02,  8.7244e-02,\n",
       "             7.2151e-02,  1.5747e-01, -1.2346e-01, -4.0435e-02, -9.8149e-02,\n",
       "            -7.3107e-02,  2.1308e-01, -5.9193e-02,  5.9529e-02,  3.0365e-03,\n",
       "            -8.9808e-02, -9.7464e-02,  3.1892e-03,  1.3669e-01, -2.6280e-01,\n",
       "            -1.3899e-02, -4.9243e-02,  8.3155e-02,  6.7136e-02, -2.1818e-01,\n",
       "            -1.2322e-01,  2.6834e-02, -8.3359e-02, -1.2199e-01,  1.9188e-02,\n",
       "            -8.0164e-02,  3.9502e-03,  1.9290e-01, -1.7055e-01,  3.4352e-01,\n",
       "             2.3982e-02, -8.6839e-02,  8.0277e-02, -5.2677e-02,  2.6666e-04,\n",
       "             2.7901e-02,  2.3013e-01, -2.1092e-01,  1.6005e-01, -2.9160e-01]],\n",
       "          grad_fn=<SplitBackward>), tensor([[-0.2167,  0.2841,  0.2368,  0.0029,  0.0511,  0.1607, -0.2183,  0.1068,\n",
       "            -0.0209, -0.0436,  0.0580, -0.0419, -0.0004,  0.0287,  0.2363,  0.0802,\n",
       "            -0.0353, -0.0977,  0.1777,  0.1432,  0.1163, -0.0557, -0.1670, -0.0137,\n",
       "             0.1110,  0.2199, -0.0771,  0.1274, -0.2078, -0.1311,  0.1479, -0.1123,\n",
       "             0.0019, -0.1519,  0.0560,  0.0255, -0.0230,  0.1252,  0.0116, -0.0407,\n",
       "             0.0064, -0.1254,  0.1936, -0.1037, -0.0810, -0.1549,  0.0195,  0.0760,\n",
       "             0.1818, -0.2647, -0.1018,  0.1212, -0.1083,  0.0478,  0.0835, -0.0630,\n",
       "             0.2067,  0.0541,  0.0855,  0.0405,  0.0419,  0.1963, -0.1213, -0.0597,\n",
       "            -0.0415, -0.0777,  0.2265, -0.1168,  0.1231,  0.0045, -0.1376, -0.1299,\n",
       "             0.0179,  0.1817, -0.2571, -0.0012, -0.0064,  0.0604,  0.1571, -0.1937,\n",
       "            -0.1429, -0.0149, -0.1732, -0.1023, -0.0344, -0.0312,  0.0263,  0.2177,\n",
       "            -0.1752,  0.3269, -0.0512, -0.0888, -0.0072, -0.0305, -0.0056,  0.0174,\n",
       "             0.1910, -0.2266,  0.1042, -0.3179]], grad_fn=<SplitBackward>))},\n",
       " (('commands_input', (1, 3, 4, 5, 11, 12, 2, 0, 0, 0)),\n",
       "  ('commands_lengths', 7.0),\n",
       "  ('situations_input',\n",
       "   (((0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      1.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)),\n",
       "    ((0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0),\n",
       "     (0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0,\n",
       "      0.0)))),\n",
       "  ('target_batch',\n",
       "   (1,\n",
       "    4,\n",
       "    5,\n",
       "    4,\n",
       "    4,\n",
       "    2,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0)),\n",
       "  ('target_lengths',\n",
       "   6.0)): {'command_hidden': (tensor([[[-0.1536,  0.1988,  0.2890,  0.0570,  0.1060,  0.0217, -0.1083,\n",
       "             -0.0173,  0.0609,  0.0480,  0.1385, -0.0908, -0.0266,  0.0456,\n",
       "              0.1931, -0.0689,  0.0565, -0.0190,  0.1395,  0.0075,  0.1049,\n",
       "             -0.0371, -0.0543,  0.1172,  0.2593,  0.1764, -0.1141, -0.0206,\n",
       "             -0.2410, -0.2235,  0.1692, -0.1114, -0.0913, -0.0874,  0.0413,\n",
       "             -0.0308,  0.0705,  0.1980,  0.0154, -0.0658,  0.0518, -0.1405,\n",
       "              0.0985, -0.1382, -0.0867, -0.0833, -0.0757, -0.0098,  0.0993,\n",
       "             -0.0294, -0.0601,  0.0706, -0.1416, -0.0262,  0.0998,  0.0788,\n",
       "              0.0679,  0.0596,  0.0197,  0.0730, -0.0419,  0.2292, -0.0682,\n",
       "             -0.0533, -0.0873, -0.0618,  0.1654, -0.0673,  0.1079, -0.0233,\n",
       "             -0.1364, -0.2070,  0.0020, -0.0007, -0.1820, -0.0340, -0.0955,\n",
       "             -0.0496,  0.1254, -0.2485, -0.1088,  0.0341,  0.0350, -0.1082,\n",
       "             -0.0161, -0.1799,  0.0532,  0.1786, -0.0839,  0.1770,  0.0616,\n",
       "             -0.2994, -0.0199,  0.0458,  0.0938,  0.1987,  0.3312, -0.2266,\n",
       "             -0.0332, -0.1663],\n",
       "            [-0.1425,  0.1946,  0.2786,  0.0540,  0.1299,  0.0254, -0.1004,\n",
       "             -0.0134,  0.0600,  0.0391,  0.1278, -0.0871,  0.0064,  0.0478,\n",
       "              0.1994, -0.0635,  0.0544, -0.0137,  0.1357,  0.0036,  0.0998,\n",
       "             -0.0379, -0.0379,  0.1109,  0.2521,  0.1761, -0.1122, -0.0358,\n",
       "             -0.2283, -0.2241,  0.1695, -0.1115, -0.0957, -0.0738,  0.0535,\n",
       "             -0.0427,  0.0709,  0.2041,  0.0196, -0.0643,  0.0418, -0.1376,\n",
       "              0.1074, -0.1392, -0.0938, -0.0881, -0.0643, -0.0158,  0.1043,\n",
       "             -0.0288, -0.0478,  0.0726, -0.1388, -0.0472,  0.0914,  0.0775,\n",
       "              0.0637,  0.0602,  0.0223,  0.0697, -0.0451,  0.2319, -0.0485,\n",
       "             -0.0319, -0.0752, -0.0436,  0.1569, -0.0682,  0.1044, -0.0101,\n",
       "             -0.1549, -0.2103, -0.0005,  0.0083, -0.1671, -0.0396, -0.0989,\n",
       "             -0.0340,  0.1425, -0.2530, -0.1089,  0.0388,  0.0393, -0.1011,\n",
       "             -0.0201, -0.1833,  0.0509,  0.1860, -0.0843,  0.1834,  0.0709,\n",
       "             -0.3046, -0.0161,  0.0559,  0.0995,  0.1954,  0.3313, -0.2412,\n",
       "             -0.0296, -0.1688]]], grad_fn=<SplitBackward>), tensor([[[-0.0238,  0.0049,  0.2779, -0.0148,  0.1340,  0.0739, -0.2180,\n",
       "             -0.0187,  0.0755,  0.0462,  0.1535, -0.0124, -0.0156, -0.0647,\n",
       "              0.2163,  0.0732,  0.1503,  0.1072,  0.1460, -0.0342,  0.1804,\n",
       "              0.0562,  0.0602, -0.0468,  0.2282,  0.0977, -0.0882, -0.1079,\n",
       "             -0.1233,  0.0663,  0.1275,  0.0703,  0.0208, -0.0466,  0.0360,\n",
       "              0.1833, -0.0233,  0.1852,  0.1534, -0.1220, -0.1977, -0.1703,\n",
       "             -0.0167, -0.1990,  0.0166,  0.1343, -0.1515, -0.2478, -0.0537,\n",
       "             -0.0330, -0.0058,  0.1272, -0.0538, -0.3251,  0.0657,  0.0554,\n",
       "              0.2397,  0.1344,  0.0510,  0.1426, -0.1109,  0.2504, -0.0895,\n",
       "             -0.0104,  0.0961, -0.2205,  0.2350,  0.1347,  0.0247,  0.2047,\n",
       "             -0.1065, -0.1802, -0.0445, -0.0143, -0.0666, -0.2266, -0.2278,\n",
       "             -0.0815,  0.2300, -0.1720, -0.0389,  0.1163,  0.1478,  0.0540,\n",
       "              0.0593, -0.0596,  0.0403,  0.1712, -0.0710,  0.2018, -0.1483,\n",
       "             -0.0167, -0.0216, -0.0530,  0.1510,  0.2816,  0.1993, -0.2147,\n",
       "             -0.0971, -0.1576],\n",
       "            [-0.0073, -0.0026,  0.2577, -0.0164,  0.1693,  0.0778, -0.2004,\n",
       "             -0.0102,  0.0785,  0.0220,  0.1406, -0.0061,  0.0348, -0.0659,\n",
       "              0.2266,  0.0837,  0.1470,  0.1155,  0.1384, -0.0411,  0.1710,\n",
       "              0.0519,  0.0881, -0.0524,  0.2159,  0.0996, -0.0818, -0.1335,\n",
       "             -0.1169,  0.0595,  0.1270,  0.0657,  0.0161, -0.0249,  0.0488,\n",
       "              0.1668, -0.0237,  0.1975,  0.1644, -0.1179, -0.2187, -0.1576,\n",
       "             -0.0017, -0.2043,  0.0057,  0.1293, -0.1375, -0.2581, -0.0481,\n",
       "             -0.0326,  0.0109,  0.1341, -0.0483, -0.3655,  0.0497,  0.0509,\n",
       "              0.2341,  0.1316,  0.0534,  0.1358, -0.1165,  0.2553, -0.0627,\n",
       "              0.0195,  0.1188, -0.1990,  0.2223,  0.1307,  0.0204,  0.2207,\n",
       "             -0.1398, -0.1889, -0.0462, -0.0028, -0.0416, -0.2318, -0.2326,\n",
       "             -0.0600,  0.2581, -0.1771, -0.0350,  0.1311,  0.1571,  0.0600,\n",
       "              0.0503, -0.0591,  0.0344,  0.1791, -0.0726,  0.2125, -0.1323,\n",
       "             -0.0213, -0.0197, -0.0369,  0.1675,  0.2711,  0.1969, -0.2324,\n",
       "             -0.0866, -0.1627]]], grad_fn=<SplitBackward>), tensor([[[ 5.9868e-03,  1.1730e-01,  3.0074e-01,  8.6623e-02,  9.9349e-02,\n",
       "             -1.1474e-01, -9.9970e-02, -2.3363e-01,  7.1900e-03,  1.4696e-01,\n",
       "              1.9174e-01, -1.4763e-02, -1.4947e-01, -7.4005e-02,  1.1708e-01,\n",
       "             -2.7275e-02,  2.6914e-01,  1.7192e-01, -6.4846e-02, -1.2357e-01,\n",
       "              2.5746e-01, -1.2323e-01, -1.1246e-02,  6.7544e-02,  2.6971e-01,\n",
       "             -1.4108e-01, -2.1993e-02, -2.0159e-01,  5.0181e-02,  9.9150e-02,\n",
       "              5.9924e-02, -7.0866e-02,  4.0421e-02,  2.0823e-02,  6.8742e-02,\n",
       "              1.5390e-01,  1.0575e-02, -5.4437e-02, -1.2209e-01, -1.1415e-01,\n",
       "             -3.8002e-02, -1.8440e-02, -9.5728e-02, -8.3756e-02,  2.3755e-01,\n",
       "              1.0407e-01,  2.5417e-02, -4.5129e-01, -1.0941e-01,  6.5923e-02,\n",
       "             -4.8753e-02,  6.4191e-02,  1.4824e-01, -2.2613e-01,  1.6996e-01,\n",
       "              1.3517e-01,  1.8422e-01,  2.9441e-01,  1.4253e-01,  6.7773e-02,\n",
       "             -2.3337e-01,  3.1689e-01, -1.2674e-01, -8.9996e-02, -1.2300e-01,\n",
       "              1.6775e-02,  2.0648e-01,  1.5094e-01, -2.3010e-02,  2.7931e-01,\n",
       "             -1.8900e-01, -1.9099e-01, -1.1246e-02,  4.6959e-02, -3.0385e-02,\n",
       "             -4.7083e-02, -2.0152e-02,  6.6104e-02,  2.0449e-01, -9.1356e-02,\n",
       "              3.6579e-02, -1.8125e-02,  1.8550e-01,  2.1989e-01,  7.8916e-02,\n",
       "              1.3918e-01, -2.8744e-03,  3.4228e-02,  1.6968e-01, -2.4704e-02,\n",
       "             -4.0726e-03,  1.1486e-01,  1.1372e-01, -1.6840e-01,  9.4102e-02,\n",
       "              1.7898e-01,  2.2346e-01, -1.3092e-01, -6.3555e-02,  1.3051e-01],\n",
       "            [ 1.8526e-02,  1.0106e-01,  2.5510e-01,  8.7304e-02,  1.4877e-01,\n",
       "             -1.1273e-01, -8.3935e-02, -2.1609e-01,  1.9928e-02,  8.0319e-02,\n",
       "              1.7519e-01,  1.4689e-02, -8.0270e-02, -8.3360e-02,  1.3010e-01,\n",
       "              1.9786e-04,  2.6307e-01,  1.7935e-01, -8.5006e-02, -1.3020e-01,\n",
       "              2.4570e-01, -1.3741e-01,  1.9919e-02,  6.2857e-02,  2.4618e-01,\n",
       "             -1.3354e-01, -9.3866e-03, -2.3436e-01,  5.0370e-02,  7.8886e-02,\n",
       "              5.5183e-02, -8.3343e-02,  3.7179e-02,  4.2487e-02,  9.3873e-02,\n",
       "              1.2916e-01,  6.3738e-03, -3.2551e-02, -9.9239e-02, -1.0302e-01,\n",
       "             -7.6504e-02, -5.4750e-04, -7.4640e-02, -9.6957e-02,  2.2575e-01,\n",
       "              9.3423e-02,  4.5547e-02, -4.6907e-01, -1.0363e-01,  6.3726e-02,\n",
       "             -2.4547e-02,  8.2090e-02,  1.6162e-01, -2.8638e-01,  1.3011e-01,\n",
       "              1.2005e-01,  1.7618e-01,  2.8456e-01,  1.4923e-01,  5.7387e-02,\n",
       "             -2.4295e-01,  3.2564e-01, -6.6675e-02, -5.8015e-02, -8.1539e-02,\n",
       "              4.7099e-02,  1.8903e-01,  1.3646e-01, -3.1878e-02,  2.7905e-01,\n",
       "             -2.1942e-01, -1.9496e-01, -1.1568e-02,  6.3637e-02,  9.7969e-03,\n",
       "             -5.4493e-02, -2.8469e-02,  8.8376e-02,  2.3868e-01, -1.0430e-01,\n",
       "              4.6451e-02,  1.1638e-02,  2.0422e-01,  2.2568e-01,  6.1617e-02,\n",
       "              1.7048e-01, -1.0013e-02,  4.5515e-02,  1.6356e-01, -1.1756e-02,\n",
       "              2.6341e-02,  1.1359e-01,  1.1173e-01, -1.4125e-01,  1.1527e-01,\n",
       "              1.5998e-01,  2.1468e-01, -1.5993e-01, -4.0814e-02,  1.1831e-01]]],\n",
       "          grad_fn=<SplitBackward>), tensor([[[-5.7297e-02, -2.7712e-02,  1.9893e-01,  1.2598e-03,  1.7501e-02,\n",
       "             -6.9164e-02, -4.3143e-02, -4.7449e-02, -5.2341e-03,  4.7123e-02,\n",
       "             -1.8698e-02, -6.7803e-02, -4.9229e-02,  9.6911e-02, -1.2219e-01,\n",
       "             -1.1813e-01,  2.7583e-01,  2.5334e-02,  4.4365e-03, -9.9438e-02,\n",
       "              2.5955e-01, -5.9768e-02,  1.4313e-02,  9.7207e-02,  4.7668e-02,\n",
       "             -1.3986e-01, -4.7068e-03, -2.0139e-02,  2.4094e-01,  1.6126e-01,\n",
       "             -2.7330e-02, -6.5991e-02,  2.8876e-02, -5.0405e-03, -2.7316e-02,\n",
       "              1.1712e-01, -5.9483e-02, -1.7706e-01, -1.9321e-01, -2.5209e-02,\n",
       "              1.2053e-02,  1.3343e-01, -6.6623e-02, -6.7496e-02,  1.2774e-01,\n",
       "              1.1436e-02,  1.5293e-01, -9.1172e-02, -4.3348e-02,  1.7649e-02,\n",
       "              4.7212e-02, -6.2935e-02,  1.6963e-01, -1.6317e-01,  1.3794e-01,\n",
       "              3.7784e-02,  6.4471e-02,  1.1000e-01,  6.6344e-02,  1.5273e-01,\n",
       "             -5.2763e-02,  1.4382e-01, -7.2903e-02, -3.7556e-02, -1.7760e-01,\n",
       "              3.5215e-02, -1.6864e-02,  5.2697e-02,  6.1663e-02,  5.9512e-02,\n",
       "             -1.0958e-01,  4.8095e-02, -5.9025e-03,  1.4954e-01,  1.0893e-02,\n",
       "             -5.8979e-02,  2.2946e-02, -1.2100e-02, -5.1187e-03, -5.2104e-02,\n",
       "             -3.0298e-02, -5.2376e-02,  1.9359e-02,  2.2448e-01, -1.4184e-01,\n",
       "              1.1444e-01, -1.3547e-01, -1.4954e-01,  2.1371e-01, -2.6572e-02,\n",
       "              3.4884e-02,  3.4353e-02,  2.3255e-01,  6.0241e-03, -1.0292e-01,\n",
       "              1.1531e-01, -2.8796e-02,  1.9740e-01,  2.2376e-02,  1.6602e-01],\n",
       "            [-4.5440e-02, -5.5119e-02,  7.3273e-02,  4.4213e-03,  7.7216e-02,\n",
       "             -7.9252e-02, -1.2712e-02, -9.9054e-03,  2.0484e-02, -1.7437e-02,\n",
       "             -4.3586e-02,  9.3168e-03,  2.9064e-02,  8.1098e-02, -1.1436e-01,\n",
       "             -6.5512e-02,  2.6347e-01,  3.3552e-02, -4.0104e-02, -1.0188e-01,\n",
       "              2.3285e-01, -8.6532e-02,  7.4049e-02,  9.3066e-02, -6.0833e-03,\n",
       "             -1.2206e-01,  1.7269e-02, -9.4892e-02,  1.9394e-01,  9.9549e-02,\n",
       "             -4.4559e-02, -9.3487e-02,  3.1686e-02,  1.5856e-02,  1.6831e-02,\n",
       "              5.8887e-02, -7.0148e-02, -1.5058e-01, -1.4203e-01,  1.3883e-03,\n",
       "             -6.1816e-02,  1.8826e-01, -7.1714e-03, -1.1131e-01,  1.0915e-01,\n",
       "              8.9588e-03,  1.6577e-01, -1.1338e-01, -4.1609e-02,  5.1947e-03,\n",
       "              6.5051e-02, -2.4293e-02,  1.7841e-01, -2.4748e-01,  8.6556e-02,\n",
       "             -1.6208e-02,  5.3716e-02,  9.6347e-02,  7.5645e-02,  1.3539e-01,\n",
       "             -8.8023e-02,  1.6029e-01,  7.2743e-02, -6.5386e-03, -9.5756e-02,\n",
       "              7.9782e-02, -2.6340e-02,  2.4291e-02,  5.4422e-02,  3.2862e-02,\n",
       "             -1.4288e-01,  5.4564e-02, -3.2359e-03,  1.8158e-01,  4.2697e-02,\n",
       "             -6.3748e-02, -2.4828e-04,  1.5335e-02,  7.9828e-02, -8.1017e-02,\n",
       "             -8.2353e-03, -3.9862e-03,  5.5406e-02,  2.3006e-01, -1.8112e-01,\n",
       "              2.2378e-01, -1.3158e-01, -1.4024e-01,  2.0129e-01, -2.0462e-02,\n",
       "              8.1167e-02,  3.2912e-02,  2.2096e-01,  7.3498e-02, -7.8306e-02,\n",
       "              7.3816e-02, -5.6716e-02,  1.5367e-01,  7.2454e-02,  1.2878e-01]]],\n",
       "          grad_fn=<SplitBackward>), tensor([[[-0.1229,  0.1445,  0.2233, -0.0962, -0.0362, -0.1078, -0.1147,\n",
       "             -0.0883, -0.0581, -0.0365, -0.1024,  0.0344, -0.1254,  0.1058,\n",
       "             -0.0597, -0.0549,  0.2305,  0.0716,  0.1420, -0.0027,  0.3183,\n",
       "             -0.0905, -0.0425,  0.0191,  0.0516,  0.1485, -0.0588,  0.1636,\n",
       "              0.2605,  0.0314,  0.0513, -0.0642, -0.2640, -0.1219, -0.1667,\n",
       "              0.1332,  0.0267,  0.0269, -0.2480, -0.0924, -0.0227,  0.0040,\n",
       "              0.0152, -0.1956,  0.0969, -0.1131,  0.2222, -0.0563,  0.0021,\n",
       "              0.0159,  0.0526, -0.1167,  0.1974,  0.0476,  0.1883, -0.0247,\n",
       "              0.0833,  0.1249,  0.1269,  0.1671,  0.0098,  0.0562, -0.1261,\n",
       "              0.1142, -0.2367,  0.0318,  0.0356,  0.1012,  0.1330,  0.0361,\n",
       "             -0.2191, -0.0448, -0.1235,  0.0842, -0.1018, -0.0348, -0.0210,\n",
       "              0.0889, -0.1143, -0.1409, -0.0591, -0.0880, -0.2635,  0.0555,\n",
       "              0.0788,  0.0107, -0.0922,  0.1693,  0.0612,  0.1991,  0.0125,\n",
       "              0.2387,  0.0678,  0.0074, -0.1102,  0.0890,  0.1098,  0.0257,\n",
       "              0.0942, -0.0227],\n",
       "            [-0.0884,  0.1312, -0.1324,  0.0254,  0.1081, -0.0732, -0.1349,\n",
       "             -0.1405,  0.0917, -0.0772, -0.1575,  0.1514,  0.1033,  0.0830,\n",
       "             -0.0437,  0.0960,  0.2499,  0.0252,  0.1405, -0.0205,  0.0758,\n",
       "             -0.0431, -0.0112,  0.0798, -0.0409,  0.0208,  0.0080, -0.1859,\n",
       "              0.0841,  0.0612,  0.0059, -0.1573, -0.1809, -0.0689,  0.0532,\n",
       "              0.1002, -0.1607, -0.0255, -0.1523,  0.0970, -0.2017,  0.0692,\n",
       "              0.0598, -0.1471,  0.0321, -0.0986,  0.2219, -0.1262,  0.0449,\n",
       "             -0.0178, -0.0650,  0.1696,  0.1774, -0.1176, -0.0627, -0.0996,\n",
       "              0.1967, -0.0530,  0.1789,  0.1337, -0.0917,  0.1297, -0.0020,\n",
       "              0.0430, -0.0454,  0.1813,  0.1093, -0.0244,  0.1074, -0.1275,\n",
       "             -0.2336,  0.0964, -0.1592,  0.1460, -0.1122,  0.0133, -0.0121,\n",
       "              0.0582,  0.1700, -0.1774, -0.0980, -0.1342, -0.1408,  0.0603,\n",
       "              0.0547,  0.1001,  0.0461, -0.0740, -0.0566,  0.3051,  0.0580,\n",
       "              0.2439,  0.1009,  0.0952, -0.1495, -0.0213, -0.0519, -0.0279,\n",
       "              0.0923, -0.2016]]], grad_fn=<SplitBackward>), tensor([[[-0.0600, -0.0385,  0.2363, -0.0435, -0.1472, -0.0617, -0.0042,\n",
       "              0.1390, -0.0035, -0.1360, -0.1523,  0.1083, -0.1586, -0.0523,\n",
       "              0.0651,  0.0466,  0.0992,  0.1339,  0.0638, -0.1668,  0.1248,\n",
       "             -0.0762, -0.2474,  0.0251,  0.0827,  0.0996, -0.0953,  0.0196,\n",
       "              0.2238,  0.0941,  0.0228,  0.0207, -0.2245, -0.0288, -0.1006,\n",
       "              0.0734,  0.1125, -0.0405, -0.0841, -0.0370,  0.0751,  0.0276,\n",
       "              0.0231, -0.2678,  0.0676, -0.0864, -0.0055, -0.0997,  0.1303,\n",
       "             -0.1683,  0.0056, -0.0265,  0.0751, -0.0889, -0.0417, -0.1314,\n",
       "              0.1878,  0.1044,  0.0548, -0.0266, -0.2332,  0.1024, -0.1662,\n",
       "             -0.0208, -0.1831,  0.0637, -0.0514, -0.0596, -0.0176,  0.0867,\n",
       "             -0.1143,  0.0062, -0.0439,  0.0872, -0.0988, -0.2199, -0.1017,\n",
       "              0.0202, -0.0371, -0.1215, -0.1470,  0.1246, -0.1180, -0.0159,\n",
       "             -0.0221, -0.0753,  0.0851,  0.0609, -0.1919,  0.2194,  0.0174,\n",
       "             -0.0040,  0.2269, -0.0720,  0.0290,  0.1309,  0.1217, -0.0286,\n",
       "              0.1175, -0.1055],\n",
       "            [-0.2871, -0.0866,  0.0831, -0.1773,  0.0774,  0.0006, -0.1391,\n",
       "              0.1036, -0.1098,  0.0964, -0.0625, -0.0672, -0.0505, -0.0036,\n",
       "              0.0144,  0.2232,  0.0586,  0.0262,  0.0636, -0.0474,  0.1314,\n",
       "              0.0442, -0.0527,  0.1833,  0.0401,  0.2018, -0.0175, -0.0132,\n",
       "              0.1619,  0.2442, -0.0748, -0.0014,  0.0652, -0.1295, -0.2255,\n",
       "             -0.0080, -0.1468, -0.1619, -0.1323,  0.0187,  0.0466,  0.1691,\n",
       "              0.1490, -0.1169, -0.0516,  0.0105,  0.1513, -0.1165,  0.1976,\n",
       "             -0.1282, -0.1045, -0.0086, -0.0342, -0.0014, -0.0242, -0.1330,\n",
       "              0.0538,  0.1018,  0.2333,  0.1244, -0.1550,  0.0088, -0.0480,\n",
       "              0.0346,  0.0302,  0.1005,  0.0541, -0.0388,  0.0902, -0.0250,\n",
       "             -0.0012, -0.0760,  0.0726, -0.1317, -0.1137, -0.0840,  0.1007,\n",
       "             -0.0366,  0.2359, -0.0360,  0.0173, -0.0371, -0.2151, -0.0780,\n",
       "             -0.0942,  0.1529, -0.0207, -0.0563,  0.0025,  0.2459, -0.0978,\n",
       "              0.2596,  0.0999,  0.0600, -0.0248, -0.0641, -0.1142, -0.0877,\n",
       "              0.0536, -0.0522]]], grad_fn=<SplitBackward>), tensor([[[-0.1576, -0.0774,  0.3355,  0.1818, -0.1875,  0.0885,  0.0591,\n",
       "              0.1269, -0.1176,  0.0200,  0.0402,  0.0326, -0.1719, -0.1317,\n",
       "              0.0030, -0.0700,  0.0465,  0.1664,  0.0353, -0.2551, -0.0240,\n",
       "              0.0314, -0.1204,  0.1742,  0.0802,  0.1084, -0.0731, -0.1418,\n",
       "              0.1376,  0.0796,  0.0062,  0.0980,  0.0519,  0.0557, -0.1444,\n",
       "              0.1304,  0.1700, -0.2989, -0.2395, -0.0990,  0.0260, -0.0359,\n",
       "             -0.0179, -0.1574,  0.0032, -0.1070,  0.0883, -0.0868,  0.1391,\n",
       "              0.0013, -0.0170,  0.0807, -0.0348, -0.1230,  0.2077,  0.0843,\n",
       "              0.2344,  0.0116,  0.0498, -0.1160, -0.1592, -0.0795, -0.1361,\n",
       "             -0.0480,  0.0839,  0.1280, -0.0265,  0.1100, -0.0534,  0.1874,\n",
       "              0.0237,  0.0338, -0.0118,  0.0557,  0.0762, -0.0533, -0.0291,\n",
       "              0.1383,  0.1925, -0.2908,  0.0918,  0.1093,  0.0940, -0.0568,\n",
       "             -0.0426,  0.1936,  0.0267,  0.0090, -0.0029,  0.3326,  0.1121,\n",
       "              0.0502,  0.1265, -0.1167,  0.0725,  0.1661,  0.0040,  0.0578,\n",
       "              0.1324, -0.0869],\n",
       "            [-0.2032,  0.1826,  0.0520, -0.0747, -0.0814, -0.0768, -0.1252,\n",
       "              0.0653, -0.0888, -0.0362, -0.0011,  0.0087,  0.0752,  0.0841,\n",
       "              0.0638,  0.0927, -0.0852, -0.0679,  0.1001,  0.0702,  0.1829,\n",
       "             -0.0269,  0.0203,  0.1038, -0.0818,  0.1033, -0.0279, -0.0256,\n",
       "             -0.0395,  0.0168, -0.0114, -0.0887, -0.0616, -0.1537, -0.1762,\n",
       "             -0.0310, -0.1291, -0.0039, -0.0218,  0.0742, -0.1389,  0.0801,\n",
       "              0.2280, -0.0187, -0.0579, -0.1098,  0.1920,  0.1763,  0.2184,\n",
       "             -0.1526, -0.0292,  0.0567, -0.0336, -0.0510, -0.0920, -0.2004,\n",
       "              0.0356,  0.0233,  0.0681, -0.0007,  0.1112,  0.0692, -0.1021,\n",
       "             -0.0378, -0.1039,  0.0678,  0.0848,  0.0019,  0.1174,  0.0018,\n",
       "             -0.0302, -0.0188, -0.0066,  0.0975, -0.2697, -0.0093,  0.0497,\n",
       "              0.0421,  0.0075, -0.1333, -0.1886, -0.0697, -0.2024, -0.0211,\n",
       "             -0.0400,  0.0137, -0.0240,  0.0953, -0.0490,  0.2639, -0.0722,\n",
       "              0.2137,  0.1269,  0.0629, -0.0463, -0.0941,  0.0124, -0.0927,\n",
       "              0.1145, -0.2155]]], grad_fn=<SplitBackward>), tensor([[[-0.1535, -0.0896,  0.3423,  0.0059, -0.1961,  0.0353,  0.0369,\n",
       "              0.3263, -0.1492,  0.0455,  0.0607, -0.1747, -0.1782,  0.0227,\n",
       "             -0.1379,  0.0330,  0.1107,  0.2735,  0.0812, -0.2108, -0.0052,\n",
       "             -0.1009, -0.1163,  0.2113, -0.0913,  0.0321, -0.0731,  0.0938,\n",
       "              0.1702,  0.2635, -0.1124,  0.0845, -0.0943,  0.0133, -0.0639,\n",
       "             -0.0595,  0.2874, -0.1535, -0.1168, -0.0845,  0.0478, -0.0291,\n",
       "             -0.0658, -0.1383,  0.0792, -0.1205,  0.1021,  0.0353, -0.0354,\n",
       "              0.0547,  0.0054, -0.0122,  0.0922, -0.2589,  0.1755, -0.0755,\n",
       "              0.0201,  0.0474,  0.0171,  0.0578, -0.1536, -0.0164, -0.0948,\n",
       "              0.1538, -0.0868,  0.0861, -0.0496,  0.0289, -0.1181,  0.1814,\n",
       "             -0.0377,  0.0765,  0.0516,  0.1041,  0.0071, -0.2781, -0.0799,\n",
       "              0.1204,  0.0150, -0.0778, -0.1081,  0.1163,  0.1878, -0.0620,\n",
       "              0.1210,  0.0931,  0.0772,  0.1533,  0.2169,  0.1969,  0.2038,\n",
       "             -0.0775,  0.0810, -0.2817, -0.0284, -0.0449,  0.1576,  0.0312,\n",
       "             -0.0013,  0.0324],\n",
       "            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000]]], grad_fn=<SplitBackward>), tensor([[[-0.3156,  0.2054,  0.2021,  0.0176, -0.1128, -0.1058, -0.1291,\n",
       "              0.0638, -0.0672,  0.0189, -0.0730,  0.0343, -0.0138,  0.1248,\n",
       "             -0.0482,  0.0754, -0.0520,  0.0723, -0.0815, -0.1622,  0.0238,\n",
       "              0.0896, -0.0890,  0.0792, -0.0735,  0.1130,  0.0613, -0.2007,\n",
       "              0.1066,  0.1393, -0.1324, -0.1493,  0.0492,  0.0768, -0.0138,\n",
       "             -0.0105,  0.1329,  0.0040, -0.0065,  0.0944, -0.2012,  0.1473,\n",
       "              0.1912, -0.0300,  0.0989, -0.2427,  0.0306,  0.1540,  0.0184,\n",
       "              0.0568, -0.1011,  0.1890,  0.0486, -0.2652,  0.1061, -0.0207,\n",
       "              0.1406, -0.0219,  0.0909,  0.0978, -0.0420, -0.1058,  0.1228,\n",
       "              0.0510,  0.0205,  0.2811, -0.0005,  0.0146, -0.0184, -0.0633,\n",
       "             -0.0100,  0.0997, -0.0358,  0.1319, -0.1060,  0.0201, -0.0099,\n",
       "              0.1611,  0.0177, -0.1641, -0.0624,  0.0821,  0.1095,  0.0962,\n",
       "             -0.0229,  0.2010, -0.0443, -0.1308,  0.0253,  0.4138,  0.1514,\n",
       "              0.1808,  0.2276, -0.0988, -0.0881, -0.0231, -0.1777, -0.0035,\n",
       "              0.1899,  0.0672],\n",
       "            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000]]], grad_fn=<SplitBackward>), tensor([[[-0.1949,  0.2446,  0.0878, -0.0170, -0.1483, -0.1518, -0.1593,\n",
       "              0.0764, -0.0335, -0.0747, -0.0471,  0.1167,  0.0520,  0.1125,\n",
       "              0.0166,  0.0637, -0.2050, -0.0535,  0.1082, -0.0378,  0.0531,\n",
       "              0.0402,  0.0638,  0.1097, -0.1340,  0.0749, -0.0352,  0.0010,\n",
       "             -0.0727,  0.0140, -0.0212, -0.1260, -0.0913, -0.0857, -0.1582,\n",
       "             -0.0543, -0.0683,  0.0674, -0.0675,  0.1552, -0.0909,  0.1051,\n",
       "              0.1760,  0.0339,  0.0392, -0.1908,  0.1202,  0.2535,  0.0762,\n",
       "             -0.0903,  0.0153,  0.0892,  0.0223, -0.0874, -0.0573, -0.1403,\n",
       "              0.0516,  0.0343,  0.0660,  0.0428,  0.1383,  0.0330, -0.0846,\n",
       "              0.0029, -0.1484,  0.0906,  0.0629,  0.0586,  0.0504,  0.0135,\n",
       "             -0.0009,  0.0103, -0.0239,  0.0616, -0.2604, -0.0277,  0.0035,\n",
       "              0.0806, -0.0654, -0.1623, -0.1690, -0.0232, -0.1082, -0.0338,\n",
       "              0.0096, -0.0386, -0.0486,  0.0778, -0.0448,  0.2868,  0.0122,\n",
       "              0.2105,  0.2183,  0.0509, -0.0348, -0.0870,  0.0516, -0.0916,\n",
       "              0.1739, -0.1916],\n",
       "            [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "              0.0000,  0.0000]]], grad_fn=<SplitBackward>))}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[\"command_input_encode\"].base_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command_hidden': tensor([[-2.1942e-01,  3.5031e-01,  2.8302e-01,  6.3546e-02, -3.9658e-02,\n",
       "           8.2039e-02, -2.6031e-01,  1.1407e-01,  3.5423e-02, -7.3205e-02,\n",
       "           2.2751e-02,  6.2365e-02, -5.6642e-02,  5.4872e-02,  1.8291e-01,\n",
       "           4.5804e-02, -1.5299e-01, -8.8536e-02,  1.8954e-01,  3.9032e-02,\n",
       "          -8.3567e-03,  1.2233e-02, -1.3982e-01, -1.5779e-03,  6.6031e-02,\n",
       "           1.9188e-01, -8.6329e-02,  1.6921e-01, -2.5367e-01, -1.3325e-01,\n",
       "           1.3773e-01, -1.4950e-01, -2.3418e-02, -9.7508e-02,  6.1760e-02,\n",
       "           1.4037e-02,  3.7396e-02,  1.9038e-01, -3.8300e-02,  3.8696e-02,\n",
       "           6.4354e-02, -1.0324e-01,  1.3277e-01, -5.0206e-02,  2.3116e-02,\n",
       "          -2.3093e-01, -6.3682e-02,  1.5918e-01,  3.4607e-02, -2.0302e-01,\n",
       "          -6.9659e-02,  1.5170e-01, -5.5256e-02,  3.2395e-02,  1.2660e-01,\n",
       "          -1.5917e-03,  2.2692e-01,  6.4513e-02,  8.0817e-02,  8.7244e-02,\n",
       "           7.2151e-02,  1.5747e-01, -1.2346e-01, -4.0435e-02, -9.8149e-02,\n",
       "          -7.3107e-02,  2.1308e-01, -5.9193e-02,  5.9529e-02,  3.0365e-03,\n",
       "          -8.9808e-02, -9.7464e-02,  3.1892e-03,  1.3669e-01, -2.6280e-01,\n",
       "          -1.3899e-02, -4.9243e-02,  8.3155e-02,  6.7136e-02, -2.1818e-01,\n",
       "          -1.2322e-01,  2.6834e-02, -8.3359e-02, -1.2199e-01,  1.9188e-02,\n",
       "          -8.0164e-02,  3.9502e-03,  1.9290e-01, -1.7055e-01,  3.4352e-01,\n",
       "           2.3982e-02, -8.6839e-02,  8.0277e-02, -5.2677e-02,  2.6666e-04,\n",
       "           2.7901e-02,  2.3013e-01, -2.1092e-01,  1.6005e-01, -2.9160e-01],\n",
       "         [-2.1666e-01,  2.8408e-01,  2.3676e-01,  2.8525e-03,  5.1110e-02,\n",
       "           1.6072e-01, -2.1831e-01,  1.0680e-01, -2.0876e-02, -4.3569e-02,\n",
       "           5.8048e-02, -4.1870e-02, -3.8542e-04,  2.8658e-02,  2.3632e-01,\n",
       "           8.0199e-02, -3.5266e-02, -9.7670e-02,  1.7770e-01,  1.4320e-01,\n",
       "           1.1626e-01, -5.5685e-02, -1.6702e-01, -1.3696e-02,  1.1103e-01,\n",
       "           2.1993e-01, -7.7105e-02,  1.2739e-01, -2.0779e-01, -1.3109e-01,\n",
       "           1.4786e-01, -1.1231e-01,  1.8754e-03, -1.5194e-01,  5.5979e-02,\n",
       "           2.5480e-02, -2.2982e-02,  1.2520e-01,  1.1590e-02, -4.0738e-02,\n",
       "           6.4322e-03, -1.2537e-01,  1.9361e-01, -1.0367e-01, -8.1035e-02,\n",
       "          -1.5487e-01,  1.9519e-02,  7.5983e-02,  1.8183e-01, -2.6472e-01,\n",
       "          -1.0184e-01,  1.2118e-01, -1.0829e-01,  4.7816e-02,  8.3477e-02,\n",
       "          -6.2993e-02,  2.0673e-01,  5.4127e-02,  8.5500e-02,  4.0550e-02,\n",
       "           4.1887e-02,  1.9630e-01, -1.2131e-01, -5.9669e-02, -4.1511e-02,\n",
       "          -7.7679e-02,  2.2646e-01, -1.1684e-01,  1.2310e-01,  4.4841e-03,\n",
       "          -1.3759e-01, -1.2990e-01,  1.7903e-02,  1.8167e-01, -2.5708e-01,\n",
       "          -1.1985e-03, -6.4338e-03,  6.0377e-02,  1.5710e-01, -1.9368e-01,\n",
       "          -1.4291e-01, -1.4850e-02, -1.7321e-01, -1.0226e-01, -3.4423e-02,\n",
       "          -3.1221e-02,  2.6280e-02,  2.1774e-01, -1.7523e-01,  3.2694e-01,\n",
       "          -5.1193e-02, -8.8760e-02, -7.2200e-03, -3.0463e-02, -5.6053e-03,\n",
       "           1.7407e-02,  1.9098e-01, -2.2657e-01,  1.0421e-01, -3.1794e-01]],\n",
       "        grad_fn=<IndexSelectBackward>),\n",
       " 'command_encoder_outputs': tensor([[[-0.1536,  0.1988,  0.2890,  ..., -0.2266, -0.0332, -0.1663],\n",
       "          [-0.1425,  0.1946,  0.2786,  ..., -0.2412, -0.0296, -0.1688]],\n",
       " \n",
       "         [[-0.0238,  0.0049,  0.2779,  ..., -0.2147, -0.0971, -0.1576],\n",
       "          [-0.0073, -0.0026,  0.2577,  ..., -0.2324, -0.0866, -0.1627]],\n",
       " \n",
       "         [[ 0.0060,  0.1173,  0.3007,  ..., -0.1309, -0.0636,  0.1305],\n",
       "          [ 0.0185,  0.1011,  0.2551,  ..., -0.1599, -0.0408,  0.1183]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.1535, -0.0896,  0.3423,  ...,  0.0312, -0.0013,  0.0324],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.3156,  0.2054,  0.2021,  ..., -0.0035,  0.1899,  0.0672],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.1949,  0.2446,  0.0878,  ..., -0.0916,  0.1739, -0.1916],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        grad_fn=<IndexSelectBackward>),\n",
       " 'command_sequence_lengths': [10, 7]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['return_lstm_output']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['return_lstm_output']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try some basic interventions.\n",
    "g.compute_node(\"decode_input_preparation\", all_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
