{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from antra.antra import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../../data-files/ReaSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-20 01:09 Formulating the dataset from the passed in json file...\n",
      "2021-07-20 01:09 Loading vocabularies...\n",
      "2021-07-20 01:09 Done loading vocabularies.\n"
     ]
    }
   ],
   "source": [
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_lstm_step_fxn(step_module, i):\n",
    "    \"\"\" \n",
    "    Generate a function for a layer in lstm.\n",
    "    \"\"\"\n",
    "\n",
    "    def _lstm_step_fxn(hidden_states):\n",
    "        (output, hidden, context_situation, attention_weights_commands,\n",
    "         attention_weights_situations) = step_module(\n",
    "            hidden_states[\"input_token\"], \n",
    "            hidden_states[\"hidden\"], \n",
    "            hidden_states[\"projected_keys_textual\"], \n",
    "            hidden_states[\"commands_lengths\"], \n",
    "            hidden_states[\"projected_keys_visual\"],\n",
    "        )\n",
    "        hidden_states[\"hidden\"] = hidden\n",
    "        hidden_states[\"return_lstm_output\"] += [output.unsqueeze(0)]\n",
    "        hidden_states[\"all_attention_weights\"] += [attention_weights_situations.unsqueeze(0)]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "    return _lstm_step_fxn\n",
    "\n",
    "def generate_compute_graph(model):\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input preparation.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Command Inputs.\n",
    "    \"\"\"\n",
    "    command_world_inputs = [\"commands_input\", \"commands_lengths\"]\n",
    "    command_world_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in command_world_inputs\n",
    "    ]\n",
    "    @GraphNode(*command_world_input_leaves, cache_results=False)\n",
    "    def command_input_preparation(\n",
    "        commands_input, commands_lengths, situations_input,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"commands_input\": commands_input,\n",
    "            \"commands_lengths\": commands_lengths,\n",
    "        }\n",
    "        # We may not need the following fields. But we leave it here in case we need these\n",
    "        # to generate other inputs.\n",
    "        batch_size = input_dict[\"commands_input\"].shape[0]\n",
    "        device = input_dict[\"commands_input\"].device\n",
    "        return input_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation Inputs.\n",
    "    \"\"\"\n",
    "    situation_inputs = [\"situations_input\"]\n",
    "    situation_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in situation_inputs\n",
    "    ]\n",
    "    @GraphNode(*situation_input_leaves, cache_results=False)\n",
    "    def situation_input_preparation(\n",
    "        situations_input,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"situations_input\": situations_input,\n",
    "        }\n",
    "        return input_dict\n",
    "        \n",
    "    \"\"\"\n",
    "    Target Inputs\n",
    "    \"\"\"\n",
    "    target_sequence_inputs = [\"target_batch\", \"target_lengths\"]\n",
    "    target_sequence_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in target_sequence_inputs\n",
    "    ]\n",
    "    @GraphNode(*target_sequence_input_leaves, cache_results=False)\n",
    "    def target_sequence_input_preparation(\n",
    "        target_batch, target_lengths\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"target_batch\": target_batch,\n",
    "            \"target_lengths\": target_lengths,\n",
    "        }\n",
    "        return input_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input encoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Situation Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(situation_input_preparation)\n",
    "    def situation_encode(input_dict):\n",
    "        encoded_image = model.situation_encoder(\n",
    "            input_images=input_dict[\"situations_input\"]\n",
    "        )\n",
    "        return encoded_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Language Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_preparation)\n",
    "    def command_input_encode(input_dict):\n",
    "        hidden, encoder_outputs = model.encoder(\n",
    "            input_batch=commands_input, \n",
    "            input_lengths=commands_lengths,\n",
    "            return_as_dict=False, # we return tensors.\n",
    "        )\n",
    "        output_dict = {\n",
    "            \"command_hidden\" : hidden,\n",
    "            \"command_encoder_outputs\" : encoder_outputs[\"encoder_outputs\"],\n",
    "            \"command_sequence_lengths\" : encoder_outputs[\"sequence_lengths\"],\n",
    "        }\n",
    "        return output_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Decoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Preparation of Decoding Data structure.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_encode, situation_encode, target_sequence_input_preparation)\n",
    "    def decode_input_preparation(s_encode, c_encode, target_sequence):\n",
    "        \"\"\"\n",
    "        The decoding step can be represented as:\n",
    "        h_T = f(h_T-1, C)\n",
    "        where h_i is the recurring hidden states, and C\n",
    "        is the static state representations.\n",
    "        \n",
    "        In this function, we want to abstract the C.\n",
    "        \"\"\"\n",
    "        \n",
    "        initial_hidden = model.attention_decoder.initialize_hidden(\n",
    "            model.tanh(model.enc_hidden_to_dec_hidden(c_encode[\"command_hidden\"])))\n",
    "        \n",
    "        \"\"\"\n",
    "        Renaming.\n",
    "        \"\"\"\n",
    "        input_tokens, input_lengths = target_sequence[\"target_batch\"], target_sequence[\"target_lengths\"]\n",
    "        init_hidden = initial_hidden\n",
    "        encoded_commands = c_encode[\"command_encoder_outputs\"]\n",
    "        commands_lengths = c_encode[\"command_sequence_lengths\"]\n",
    "        encoded_situations = s_encode\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping as well as getting the context-guided attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, max_time = input_tokens.size()\n",
    "        # Sort the sequences by length in descending order\n",
    "        input_lengths = torch.tensor(input_lengths, dtype=torch.long, device=device)\n",
    "        input_lengths, perm_idx = torch.sort(input_lengths, descending=True)\n",
    "        input_tokens_sorted = input_tokens.index_select(dim=0, index=perm_idx)\n",
    "        initial_h, initial_c = init_hidden\n",
    "        hidden = (initial_h.index_select(dim=1, index=perm_idx),\n",
    "                  initial_c.index_select(dim=1, index=perm_idx))\n",
    "        encoded_commands = encoded_commands.index_select(dim=1, index=perm_idx)\n",
    "        commands_lengths = torch.tensor(commands_lengths, device=device)\n",
    "        commands_lengths = commands_lengths.index_select(dim=0, index=perm_idx)\n",
    "        encoded_situations = encoded_situations.index_select(dim=0, index=perm_idx)\n",
    "\n",
    "        # For efficiency\n",
    "        projected_keys_visual = self.visual_attention.key_layer(\n",
    "            encoded_situations)  # [batch_size, situation_length, dec_hidden_dim]\n",
    "        projected_keys_textual = self.textual_attention.key_layer(\n",
    "            encoded_commands)  # [max_input_length, batch_size, dec_hidden_dim]\n",
    "        \n",
    "        return {\n",
    "            \"return_lstm_output\":[],\n",
    "            \"return_attention_weights\":[],\n",
    "            \"hidden\":hidden,\n",
    "            \"input_token\":input_token,\n",
    "            \"projected_keys_textual\":projected_keys_textual,\n",
    "            \"commands_lengths\":commands_lengths,\n",
    "            \"projected_keys_visual\":projected_keys_visual,\n",
    "            \"perm_idx\":perm_idx,\n",
    "            \"seq_lengths\":input_lengths,\n",
    "        }\n",
    "    hidden_layer = decode_input_preparation\n",
    "    \"\"\"\n",
    "    Here, we set to a static bound of decoding steps.\n",
    "    \"\"\"\n",
    "    max_time = 30\n",
    "    for i in range(max_time):\n",
    "        f = _generate_lstm_step_fxn(model.attention_decoder.forward_step, i)\n",
    "        hidden_layer = GraphNode(hidden_layer,\n",
    "                                 name=f\"lstm_step_{i}\",\n",
    "                                 forward=f)\n",
    "    \"\"\"\n",
    "    Formulating outputs.\n",
    "    \"\"\"\n",
    "    @GraphNode(hidden_layer)\n",
    "    def output_preparation(hidden_states):\n",
    "        hidden_states[\"return_lstm_output\"] = torch.cat(\n",
    "            hidden_states[\"return_lstm_output\"], dim=0)\n",
    "        hidden_states[\"return_attention_weights\"] = torch.cat(\n",
    "            hidden_states[\"return_attention_weights\"], dim=0)\n",
    "        \n",
    "        _, unperm_idx = hidden_states[\"perm_idx\"].sort(0)\n",
    "        hidden_states[\"return_lstm_output\"] = hidden_states[\"return_lstm_output\"].index_select(dim=1, index=unperm_idx)  # [max_time, batch_size, output_size]\n",
    "        hidden_states[\"seq_lengths\"] = hidden_states[\"seq_lengths\"][unperm_idx].tolist()\n",
    "        hidden_states[\"return_attention_weights\"] = hidden_states[\"return_attention_weights\"].index_select(dim=1, index=unperm_idx)\n",
    "        \n",
    "        decoder_output_batched = hidden_states[\"return_lstm_output\"]\n",
    "        context_situation = hidden_states[\"return_attention_weights\"]\n",
    "        decoder_output_batched = F.log_softmax(decoder_output_batched, dim=-1)\n",
    "        \n",
    "        if model.auxiliary_task:\n",
    "            pass # Not implemented yet.\n",
    "        else:\n",
    "            target_position_scores = torch.zeros(1), torch.zeros(1)\n",
    "            # We are not returning this as well, since it is not used...\n",
    "        \n",
    "        return decoder_output_batched.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "    \n",
    "    root = output_preparation # TODO: removing this and continue.\n",
    "    \n",
    "    return root\n",
    "    \n",
    "class ReaSCANMultiModalLSTMCompGraph(ComputationGraph):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        root = generate_compute_graph(model)\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/ReaSCAN-Simple/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReaSCANMultiModalLSTMCompGraph at 0x7f1e68944410>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReaSCANMultiModalLSTMCompGraph(\n",
    "     model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
