{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import json\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from antra.antra import *\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# setting up the seeds.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ReaSCAN dataset to load config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 12:36 Formulating the dataset from the passed in json file...\n",
      "2021-07-23 12:36 Loading vocabularies...\n",
      "2021-07-23 12:36 Done loading vocabularies.\n",
      "2021-07-23 12:36 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"../../../data-files/ReaSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\"\n",
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")\n",
    "# Loading a couple of example from ReaSCAN.\n",
    "dataset.read_dataset(\n",
    "    max_examples=10,\n",
    "    simple_situation_representation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define computatinal graph of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_lstm_step_fxn(step_module, i):\n",
    "    \"\"\" \n",
    "    Generate a function for a layer in lstm.\n",
    "    \"\"\"\n",
    "\n",
    "    def _lstm_step_fxn(hidden_states):\n",
    "        (output, hidden, context_situation, attention_weights_commands,\n",
    "         attention_weights_situations) = step_module(\n",
    "            hidden_states[\"input_tokens_sorted\"][:, i], \n",
    "            hidden_states[\"hidden\"], \n",
    "            hidden_states[\"projected_keys_textual\"], \n",
    "            hidden_states[\"commands_lengths\"], \n",
    "            hidden_states[\"projected_keys_visual\"],\n",
    "        )\n",
    "        hidden_states[\"hidden\"] = hidden\n",
    "        hidden_states[\"return_lstm_output\"] += [output.unsqueeze(0)]\n",
    "        hidden_states[\"return_attention_weights\"] += [attention_weights_situations.unsqueeze(0)]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "    return _lstm_step_fxn\n",
    "\n",
    "def generate_compute_graph(model):\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input preparation.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Command Inputs.\n",
    "    \"\"\"\n",
    "    command_world_inputs = [\"commands_input\", \"commands_lengths\"]\n",
    "    command_world_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in command_world_inputs\n",
    "    ]\n",
    "    @GraphNode(*command_world_input_leaves, cache_results=False)\n",
    "    def command_input_preparation(\n",
    "        commands_input, commands_lengths,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"commands_input\": commands_input,\n",
    "            \"commands_lengths\": commands_lengths,\n",
    "        }\n",
    "        # We may not need the following fields. But we leave it here in case we need these\n",
    "        # to generate other inputs.\n",
    "        batch_size = input_dict[\"commands_input\"].shape[0]\n",
    "        device = input_dict[\"commands_input\"].device\n",
    "        return input_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation Inputs.\n",
    "    \"\"\"\n",
    "    situation_inputs = [\"situations_input\"]\n",
    "    situation_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in situation_inputs\n",
    "    ]\n",
    "    @GraphNode(*situation_input_leaves, cache_results=False)\n",
    "    def situation_input_preparation(\n",
    "        situations_input,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"situations_input\": situations_input,\n",
    "        }\n",
    "        return input_dict\n",
    "        \n",
    "    \"\"\"\n",
    "    Target Inputs\n",
    "    \"\"\"\n",
    "    target_sequence_inputs = [\"target_batch\", \"target_lengths\"]\n",
    "    target_sequence_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in target_sequence_inputs\n",
    "    ]\n",
    "    @GraphNode(*target_sequence_input_leaves, cache_results=False)\n",
    "    def target_sequence_input_preparation(\n",
    "        target_batch, target_lengths\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"target_batch\": target_batch,\n",
    "            \"target_lengths\": target_lengths,\n",
    "        }\n",
    "        return input_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input encoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Situation Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(situation_input_preparation)\n",
    "    def situation_encode(input_dict):\n",
    "        encoded_image = model.situation_encoder(\n",
    "            input_images=input_dict[\"situations_input\"]\n",
    "        )\n",
    "        return encoded_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Language Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_preparation)\n",
    "    def command_input_encode(input_dict):\n",
    "        hidden, encoder_outputs = model.encoder(\n",
    "            input_batch=input_dict[\"commands_input\"], \n",
    "            input_lengths=input_dict[\"commands_lengths\"],\n",
    "        )\n",
    "        output_dict = {\n",
    "            \"command_hidden\" : hidden,\n",
    "            \"command_encoder_outputs\" : encoder_outputs[\"encoder_outputs\"],\n",
    "            \"command_sequence_lengths\" : encoder_outputs[\"sequence_lengths\"],\n",
    "        }\n",
    "        return output_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Decoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Preparation of Decoding Data structure.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_encode, situation_encode, target_sequence_input_preparation)\n",
    "    def decode_input_preparation(c_encode, s_encode, target_sequence):\n",
    "        \"\"\"\n",
    "        The decoding step can be represented as:\n",
    "        h_T = f(h_T-1, C)\n",
    "        where h_i is the recurring hidden states, and C\n",
    "        is the static state representations.\n",
    "        \n",
    "        In this function, we want to abstract the C.\n",
    "        \"\"\"\n",
    "        \n",
    "        initial_hidden = model.attention_decoder.initialize_hidden(\n",
    "            model.tanh(model.enc_hidden_to_dec_hidden(c_encode[\"command_hidden\"])))\n",
    "        \n",
    "        \"\"\"\n",
    "        Renaming.\n",
    "        \"\"\"\n",
    "        input_tokens, input_lengths = target_sequence[\"target_batch\"], target_sequence[\"target_lengths\"]\n",
    "        init_hidden = initial_hidden\n",
    "        encoded_commands = c_encode[\"command_encoder_outputs\"]\n",
    "        commands_lengths = c_encode[\"command_sequence_lengths\"]\n",
    "        encoded_situations = s_encode\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping as well as getting the context-guided attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, max_time = input_tokens.size()\n",
    "        # Sort the sequences by length in descending order\n",
    "        input_lengths = torch.tensor(input_lengths, dtype=torch.long, device=device)\n",
    "        input_lengths, perm_idx = torch.sort(input_lengths, descending=True)\n",
    "        input_tokens_sorted = input_tokens.index_select(dim=0, index=perm_idx)\n",
    "        initial_h, initial_c = init_hidden\n",
    "        hidden = (initial_h.index_select(dim=1, index=perm_idx),\n",
    "                  initial_c.index_select(dim=1, index=perm_idx))\n",
    "        encoded_commands = encoded_commands.index_select(dim=1, index=perm_idx)\n",
    "        commands_lengths = torch.tensor(commands_lengths, device=device)\n",
    "        commands_lengths = commands_lengths.index_select(dim=0, index=perm_idx)\n",
    "        encoded_situations = encoded_situations.index_select(dim=0, index=perm_idx)\n",
    "\n",
    "        # For efficiency\n",
    "        projected_keys_visual = model.visual_attention.key_layer(\n",
    "            encoded_situations)  # [batch_size, situation_length, dec_hidden_dim]\n",
    "        projected_keys_textual = model.textual_attention.key_layer(\n",
    "            encoded_commands)  # [max_input_length, batch_size, dec_hidden_dim]\n",
    "        \n",
    "        return {\n",
    "            \"return_lstm_output\":[],\n",
    "            \"return_attention_weights\":[],\n",
    "            \"hidden\":hidden,\n",
    "            \"input_tokens_sorted\":input_tokens_sorted,\n",
    "            \"projected_keys_textual\":projected_keys_textual,\n",
    "            \"commands_lengths\":commands_lengths,\n",
    "            \"projected_keys_visual\":projected_keys_visual,\n",
    "            \"perm_idx\":perm_idx,\n",
    "            \"seq_lengths\":input_lengths,\n",
    "        }\n",
    "\n",
    "    hidden_layer = decode_input_preparation\n",
    "    \"\"\"\n",
    "    Here, we set to a static bound of decoding steps.\n",
    "    \"\"\"\n",
    "    max_time = 4\n",
    "    for i in range(max_time):\n",
    "        f = _generate_lstm_step_fxn(model.attention_decoder.forward_step, i)\n",
    "        hidden_layer = GraphNode(hidden_layer,\n",
    "                                 name=f\"lstm_step_{i}\",\n",
    "                                 forward=f)\n",
    "    \"\"\"\n",
    "    Formulating outputs.\n",
    "    \"\"\"\n",
    "    @GraphNode(hidden_layer)\n",
    "    def output_preparation(hidden_states):\n",
    "        hidden_states[\"return_lstm_output\"] = torch.cat(\n",
    "            hidden_states[\"return_lstm_output\"], dim=0)\n",
    "        hidden_states[\"return_attention_weights\"] = torch.cat(\n",
    "            hidden_states[\"return_attention_weights\"], dim=0)\n",
    "        \n",
    "        _, unperm_idx = hidden_states[\"perm_idx\"].sort(0)\n",
    "        hidden_states[\"return_lstm_output\"] = hidden_states[\"return_lstm_output\"].index_select(dim=1, index=unperm_idx)  # [max_time, batch_size, output_size]\n",
    "        hidden_states[\"seq_lengths\"] = hidden_states[\"seq_lengths\"][unperm_idx].tolist()\n",
    "        hidden_states[\"return_attention_weights\"] = hidden_states[\"return_attention_weights\"].index_select(dim=1, index=unperm_idx)\n",
    "        \n",
    "        decoder_output_batched = hidden_states[\"return_lstm_output\"]\n",
    "        context_situation = hidden_states[\"return_attention_weights\"]\n",
    "        decoder_output_batched = F.log_softmax(decoder_output_batched, dim=-1)\n",
    "        \n",
    "        if model.auxiliary_task:\n",
    "            pass # Not implemented yet.\n",
    "        else:\n",
    "            target_position_scores = torch.zeros(1), torch.zeros(1)\n",
    "            # We are not returning this as well, since it is not used...\n",
    "        \n",
    "        return decoder_output_batched.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "    \n",
    "    root = output_preparation # TODO: removing this and continue.\n",
    "    \n",
    "    return root\n",
    "    \n",
    "class ReaSCANMultiModalLSTMCompGraph(ComputationGraph):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        root = generate_compute_graph(model)\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model to the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/ReaSCAN-Simple/\"\n",
    ")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "g = ReaSCANMultiModalLSTMCompGraph(\n",
    "     model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some examples to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (input_batch, input_lengths, _, situation_batch, _, target_batch,\n",
    "     target_lengths, agent_positions, target_positions) in dataset.get_data_iterator(batch_size=2):\n",
    "#     target_scores, target_position_scores = model(\n",
    "#         commands_input=input_batch, commands_lengths=input_lengths,\n",
    "#         situations_input=situation_batch, target_batch=target_batch,\n",
    "#         target_lengths=target_lengths\n",
    "#     )\n",
    "    # print(target_scores)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3,  4,  5,  6,  7,  8,  9, 10,  2],\n",
       "        [ 1,  3,  4,  5, 11, 12,  2,  0,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"commands_input\": input_batch, \n",
    "    \"commands_lengths\": input_lengths,\n",
    "    \"situations_input\": situation_batch,\n",
    "    \"target_batch\": target_batch,\n",
    "    \"target_lengths\": target_lengths,\n",
    "}\n",
    "all_in = GraphInput(input_dict, batched=True, batch_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command_hidden': tensor([[-2.1942e-01,  3.5031e-01,  2.8302e-01,  6.3546e-02, -3.9658e-02,\n",
       "           8.2039e-02, -2.6031e-01,  1.1407e-01,  3.5423e-02, -7.3205e-02,\n",
       "           2.2751e-02,  6.2365e-02, -5.6642e-02,  5.4872e-02,  1.8291e-01,\n",
       "           4.5804e-02, -1.5299e-01, -8.8536e-02,  1.8954e-01,  3.9032e-02,\n",
       "          -8.3567e-03,  1.2233e-02, -1.3982e-01, -1.5779e-03,  6.6031e-02,\n",
       "           1.9188e-01, -8.6329e-02,  1.6921e-01, -2.5367e-01, -1.3325e-01,\n",
       "           1.3773e-01, -1.4950e-01, -2.3418e-02, -9.7508e-02,  6.1760e-02,\n",
       "           1.4037e-02,  3.7396e-02,  1.9038e-01, -3.8300e-02,  3.8696e-02,\n",
       "           6.4354e-02, -1.0324e-01,  1.3277e-01, -5.0206e-02,  2.3116e-02,\n",
       "          -2.3093e-01, -6.3682e-02,  1.5918e-01,  3.4607e-02, -2.0302e-01,\n",
       "          -6.9659e-02,  1.5170e-01, -5.5256e-02,  3.2395e-02,  1.2660e-01,\n",
       "          -1.5917e-03,  2.2692e-01,  6.4513e-02,  8.0817e-02,  8.7244e-02,\n",
       "           7.2151e-02,  1.5747e-01, -1.2346e-01, -4.0435e-02, -9.8149e-02,\n",
       "          -7.3107e-02,  2.1308e-01, -5.9193e-02,  5.9529e-02,  3.0365e-03,\n",
       "          -8.9808e-02, -9.7464e-02,  3.1892e-03,  1.3669e-01, -2.6280e-01,\n",
       "          -1.3899e-02, -4.9243e-02,  8.3155e-02,  6.7136e-02, -2.1818e-01,\n",
       "          -1.2322e-01,  2.6834e-02, -8.3359e-02, -1.2199e-01,  1.9188e-02,\n",
       "          -8.0164e-02,  3.9502e-03,  1.9290e-01, -1.7055e-01,  3.4352e-01,\n",
       "           2.3982e-02, -8.6839e-02,  8.0277e-02, -5.2677e-02,  2.6666e-04,\n",
       "           2.7901e-02,  2.3013e-01, -2.1092e-01,  1.6005e-01, -2.9160e-01],\n",
       "         [-2.1666e-01,  2.8408e-01,  2.3676e-01,  2.8525e-03,  5.1110e-02,\n",
       "           1.6072e-01, -2.1831e-01,  1.0680e-01, -2.0876e-02, -4.3569e-02,\n",
       "           5.8048e-02, -4.1870e-02, -3.8542e-04,  2.8658e-02,  2.3632e-01,\n",
       "           8.0199e-02, -3.5266e-02, -9.7670e-02,  1.7770e-01,  1.4320e-01,\n",
       "           1.1626e-01, -5.5685e-02, -1.6702e-01, -1.3696e-02,  1.1103e-01,\n",
       "           2.1993e-01, -7.7105e-02,  1.2739e-01, -2.0779e-01, -1.3109e-01,\n",
       "           1.4786e-01, -1.1231e-01,  1.8754e-03, -1.5194e-01,  5.5979e-02,\n",
       "           2.5480e-02, -2.2982e-02,  1.2520e-01,  1.1590e-02, -4.0738e-02,\n",
       "           6.4322e-03, -1.2537e-01,  1.9361e-01, -1.0367e-01, -8.1035e-02,\n",
       "          -1.5487e-01,  1.9519e-02,  7.5983e-02,  1.8183e-01, -2.6472e-01,\n",
       "          -1.0184e-01,  1.2118e-01, -1.0829e-01,  4.7816e-02,  8.3477e-02,\n",
       "          -6.2993e-02,  2.0673e-01,  5.4127e-02,  8.5500e-02,  4.0550e-02,\n",
       "           4.1887e-02,  1.9630e-01, -1.2131e-01, -5.9669e-02, -4.1511e-02,\n",
       "          -7.7679e-02,  2.2646e-01, -1.1684e-01,  1.2310e-01,  4.4841e-03,\n",
       "          -1.3759e-01, -1.2990e-01,  1.7903e-02,  1.8167e-01, -2.5708e-01,\n",
       "          -1.1985e-03, -6.4338e-03,  6.0377e-02,  1.5710e-01, -1.9368e-01,\n",
       "          -1.4291e-01, -1.4850e-02, -1.7321e-01, -1.0226e-01, -3.4423e-02,\n",
       "          -3.1221e-02,  2.6280e-02,  2.1774e-01, -1.7523e-01,  3.2694e-01,\n",
       "          -5.1193e-02, -8.8760e-02, -7.2200e-03, -3.0463e-02, -5.6053e-03,\n",
       "           1.7407e-02,  1.9098e-01, -2.2657e-01,  1.0421e-01, -3.1794e-01]],\n",
       "        grad_fn=<CatBackward>),\n",
       " 'command_encoder_outputs': tensor([[[-0.1536,  0.1988,  0.2890,  0.0570,  0.1060,  0.0217, -0.1083,\n",
       "           -0.0173,  0.0609,  0.0480,  0.1385, -0.0908, -0.0266,  0.0456,\n",
       "            0.1931, -0.0689,  0.0565, -0.0190,  0.1395,  0.0075,  0.1049,\n",
       "           -0.0371, -0.0543,  0.1172,  0.2593,  0.1764, -0.1141, -0.0206,\n",
       "           -0.2410, -0.2235,  0.1692, -0.1114, -0.0913, -0.0874,  0.0413,\n",
       "           -0.0308,  0.0705,  0.1980,  0.0154, -0.0658,  0.0518, -0.1405,\n",
       "            0.0985, -0.1382, -0.0867, -0.0833, -0.0757, -0.0098,  0.0993,\n",
       "           -0.0294, -0.0601,  0.0706, -0.1416, -0.0262,  0.0998,  0.0788,\n",
       "            0.0679,  0.0596,  0.0197,  0.0730, -0.0419,  0.2292, -0.0682,\n",
       "           -0.0533, -0.0873, -0.0618,  0.1654, -0.0673,  0.1079, -0.0233,\n",
       "           -0.1364, -0.2070,  0.0020, -0.0007, -0.1820, -0.0340, -0.0955,\n",
       "           -0.0496,  0.1254, -0.2485, -0.1088,  0.0341,  0.0350, -0.1082,\n",
       "           -0.0161, -0.1799,  0.0532,  0.1786, -0.0839,  0.1770,  0.0616,\n",
       "           -0.2994, -0.0199,  0.0458,  0.0938,  0.1987,  0.3312, -0.2266,\n",
       "           -0.0332, -0.1663],\n",
       "          [-0.1425,  0.1946,  0.2786,  0.0540,  0.1299,  0.0254, -0.1004,\n",
       "           -0.0134,  0.0600,  0.0391,  0.1278, -0.0871,  0.0064,  0.0478,\n",
       "            0.1994, -0.0635,  0.0544, -0.0137,  0.1357,  0.0036,  0.0998,\n",
       "           -0.0379, -0.0379,  0.1109,  0.2521,  0.1761, -0.1122, -0.0358,\n",
       "           -0.2283, -0.2241,  0.1695, -0.1115, -0.0957, -0.0738,  0.0535,\n",
       "           -0.0427,  0.0709,  0.2041,  0.0196, -0.0643,  0.0418, -0.1376,\n",
       "            0.1074, -0.1392, -0.0938, -0.0881, -0.0643, -0.0158,  0.1043,\n",
       "           -0.0288, -0.0478,  0.0726, -0.1388, -0.0472,  0.0914,  0.0775,\n",
       "            0.0637,  0.0602,  0.0223,  0.0697, -0.0451,  0.2319, -0.0485,\n",
       "           -0.0319, -0.0752, -0.0436,  0.1569, -0.0682,  0.1044, -0.0101,\n",
       "           -0.1549, -0.2103, -0.0005,  0.0083, -0.1671, -0.0396, -0.0989,\n",
       "           -0.0340,  0.1425, -0.2530, -0.1089,  0.0388,  0.0393, -0.1011,\n",
       "           -0.0201, -0.1833,  0.0509,  0.1860, -0.0843,  0.1834,  0.0709,\n",
       "           -0.3046, -0.0161,  0.0559,  0.0995,  0.1954,  0.3313, -0.2412,\n",
       "           -0.0296, -0.1688]],\n",
       " \n",
       "         [[-0.0238,  0.0049,  0.2779, -0.0148,  0.1340,  0.0739, -0.2180,\n",
       "           -0.0187,  0.0755,  0.0462,  0.1535, -0.0124, -0.0156, -0.0647,\n",
       "            0.2163,  0.0732,  0.1503,  0.1072,  0.1460, -0.0342,  0.1804,\n",
       "            0.0562,  0.0602, -0.0468,  0.2282,  0.0977, -0.0882, -0.1079,\n",
       "           -0.1233,  0.0663,  0.1275,  0.0703,  0.0208, -0.0466,  0.0360,\n",
       "            0.1833, -0.0233,  0.1852,  0.1534, -0.1220, -0.1977, -0.1703,\n",
       "           -0.0167, -0.1990,  0.0166,  0.1343, -0.1515, -0.2478, -0.0537,\n",
       "           -0.0330, -0.0058,  0.1272, -0.0538, -0.3251,  0.0657,  0.0554,\n",
       "            0.2397,  0.1344,  0.0510,  0.1426, -0.1109,  0.2504, -0.0895,\n",
       "           -0.0104,  0.0961, -0.2205,  0.2350,  0.1347,  0.0247,  0.2047,\n",
       "           -0.1065, -0.1802, -0.0445, -0.0143, -0.0666, -0.2266, -0.2278,\n",
       "           -0.0815,  0.2300, -0.1720, -0.0389,  0.1163,  0.1478,  0.0540,\n",
       "            0.0593, -0.0596,  0.0403,  0.1712, -0.0710,  0.2018, -0.1483,\n",
       "           -0.0167, -0.0216, -0.0530,  0.1510,  0.2816,  0.1993, -0.2147,\n",
       "           -0.0971, -0.1576],\n",
       "          [-0.0073, -0.0026,  0.2577, -0.0164,  0.1693,  0.0778, -0.2004,\n",
       "           -0.0102,  0.0785,  0.0220,  0.1406, -0.0061,  0.0348, -0.0659,\n",
       "            0.2266,  0.0837,  0.1470,  0.1155,  0.1384, -0.0411,  0.1710,\n",
       "            0.0519,  0.0881, -0.0524,  0.2159,  0.0996, -0.0818, -0.1335,\n",
       "           -0.1169,  0.0595,  0.1270,  0.0657,  0.0161, -0.0249,  0.0488,\n",
       "            0.1668, -0.0237,  0.1975,  0.1644, -0.1179, -0.2187, -0.1576,\n",
       "           -0.0017, -0.2043,  0.0057,  0.1293, -0.1375, -0.2581, -0.0481,\n",
       "           -0.0326,  0.0109,  0.1341, -0.0483, -0.3655,  0.0497,  0.0509,\n",
       "            0.2341,  0.1316,  0.0534,  0.1358, -0.1165,  0.2553, -0.0627,\n",
       "            0.0195,  0.1188, -0.1990,  0.2223,  0.1307,  0.0204,  0.2207,\n",
       "           -0.1398, -0.1889, -0.0462, -0.0028, -0.0416, -0.2318, -0.2326,\n",
       "           -0.0600,  0.2581, -0.1771, -0.0350,  0.1311,  0.1571,  0.0600,\n",
       "            0.0503, -0.0591,  0.0344,  0.1791, -0.0726,  0.2125, -0.1323,\n",
       "           -0.0213, -0.0197, -0.0369,  0.1675,  0.2711,  0.1969, -0.2324,\n",
       "           -0.0866, -0.1627]]], grad_fn=<CatBackward>),\n",
       " 'command_sequence_lengths': [10, 7]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.compute_node(\"command_input_encode\", all_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
