{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import json\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from antra.antra import *\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# setting up the seeds.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ReaSCAN dataset to load config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 11:07 Formulating the dataset from the passed in json file...\n",
      "2021-07-21 11:07 Loading vocabularies...\n",
      "2021-07-21 11:07 Done loading vocabularies.\n",
      "2021-07-21 11:07 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"../../../data-files/ReaSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\"\n",
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")\n",
    "# Loading a couple of example from ReaSCAN.\n",
    "dataset.read_dataset(\n",
    "    max_examples=10,\n",
    "    simple_situation_representation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define computatinal graph of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_lstm_step_fxn(step_module, i):\n",
    "    \"\"\" \n",
    "    Generate a function for a layer in lstm.\n",
    "    \"\"\"\n",
    "\n",
    "    def _lstm_step_fxn(hidden_states):\n",
    "        (output, hidden, context_situation, attention_weights_commands,\n",
    "         attention_weights_situations) = step_module(\n",
    "            hidden_states[\"input_tokens_sorted\"][:, i], \n",
    "            hidden_states[\"hidden\"], \n",
    "            hidden_states[\"projected_keys_textual\"], \n",
    "            hidden_states[\"commands_lengths\"], \n",
    "            hidden_states[\"projected_keys_visual\"],\n",
    "        )\n",
    "        hidden_states[\"hidden\"] = hidden\n",
    "        hidden_states[\"return_lstm_output\"] += [output.unsqueeze(0)]\n",
    "        hidden_states[\"return_attention_weights\"] += [attention_weights_situations.unsqueeze(0)]\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "    return _lstm_step_fxn\n",
    "\n",
    "def generate_compute_graph(model):\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input preparation.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Command Inputs.\n",
    "    \"\"\"\n",
    "    command_world_inputs = [\"commands_input\", \"commands_lengths\"]\n",
    "    command_world_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in command_world_inputs\n",
    "    ]\n",
    "    @GraphNode(*command_world_input_leaves, cache_results=False)\n",
    "    def command_input_preparation(\n",
    "        commands_input, commands_lengths,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"commands_input\": commands_input,\n",
    "            \"commands_lengths\": commands_lengths,\n",
    "        }\n",
    "        # We may not need the following fields. But we leave it here in case we need these\n",
    "        # to generate other inputs.\n",
    "        batch_size = input_dict[\"commands_input\"].shape[0]\n",
    "        device = input_dict[\"commands_input\"].device\n",
    "        return input_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation Inputs.\n",
    "    \"\"\"\n",
    "    situation_inputs = [\"situations_input\"]\n",
    "    situation_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in situation_inputs\n",
    "    ]\n",
    "    @GraphNode(*situation_input_leaves, cache_results=False)\n",
    "    def situation_input_preparation(\n",
    "        situations_input,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"situations_input\": situations_input,\n",
    "        }\n",
    "        return input_dict\n",
    "        \n",
    "    \"\"\"\n",
    "    Target Inputs\n",
    "    \"\"\"\n",
    "    target_sequence_inputs = [\"target_batch\", \"target_lengths\"]\n",
    "    target_sequence_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in target_sequence_inputs\n",
    "    ]\n",
    "    @GraphNode(*target_sequence_input_leaves, cache_results=False)\n",
    "    def target_sequence_input_preparation(\n",
    "        target_batch, target_lengths\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"target_batch\": target_batch,\n",
    "            \"target_lengths\": target_lengths,\n",
    "        }\n",
    "        return input_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input encoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Situation Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(situation_input_preparation)\n",
    "    def situation_encode(input_dict):\n",
    "        encoded_image = model.situation_encoder(\n",
    "            input_images=input_dict[\"situations_input\"]\n",
    "        )\n",
    "        return encoded_image\n",
    "    \n",
    "    \"\"\"\n",
    "    Language Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_preparation)\n",
    "    def command_input_encode(input_dict):\n",
    "        hidden, encoder_outputs = model.encoder(\n",
    "            input_batch=input_dict[\"commands_input\"], \n",
    "            input_lengths=input_dict[\"commands_lengths\"],\n",
    "        )\n",
    "        output_dict = {\n",
    "            \"command_hidden\" : hidden,\n",
    "            \"command_encoder_outputs\" : encoder_outputs[\"encoder_outputs\"],\n",
    "            \"command_sequence_lengths\" : encoder_outputs[\"sequence_lengths\"],\n",
    "        }\n",
    "        return output_dict\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Decoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Preparation of Decoding Data structure.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_encode, situation_encode, target_sequence_input_preparation)\n",
    "    def decode_input_preparation(c_encode, s_encode, target_sequence):\n",
    "        \"\"\"\n",
    "        The decoding step can be represented as:\n",
    "        h_T = f(h_T-1, C)\n",
    "        where h_i is the recurring hidden states, and C\n",
    "        is the static state representations.\n",
    "        \n",
    "        In this function, we want to abstract the C.\n",
    "        \"\"\"\n",
    "        \n",
    "        initial_hidden = model.attention_decoder.initialize_hidden(\n",
    "            model.tanh(model.enc_hidden_to_dec_hidden(c_encode[\"command_hidden\"])))\n",
    "        \n",
    "        \"\"\"\n",
    "        Renaming.\n",
    "        \"\"\"\n",
    "        input_tokens, input_lengths = target_sequence[\"target_batch\"], target_sequence[\"target_lengths\"]\n",
    "        init_hidden = initial_hidden\n",
    "        encoded_commands = c_encode[\"command_encoder_outputs\"]\n",
    "        commands_lengths = c_encode[\"command_sequence_lengths\"]\n",
    "        encoded_situations = s_encode\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping as well as getting the context-guided attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, max_time = input_tokens.size()\n",
    "        # Sort the sequences by length in descending order\n",
    "        input_lengths = torch.tensor(input_lengths, dtype=torch.long, device=device)\n",
    "        input_lengths, perm_idx = torch.sort(input_lengths, descending=True)\n",
    "        input_tokens_sorted = input_tokens.index_select(dim=0, index=perm_idx)\n",
    "        initial_h, initial_c = init_hidden\n",
    "        hidden = (initial_h.index_select(dim=1, index=perm_idx),\n",
    "                  initial_c.index_select(dim=1, index=perm_idx))\n",
    "        encoded_commands = encoded_commands.index_select(dim=1, index=perm_idx)\n",
    "        commands_lengths = torch.tensor(commands_lengths, device=device)\n",
    "        commands_lengths = commands_lengths.index_select(dim=0, index=perm_idx)\n",
    "        encoded_situations = encoded_situations.index_select(dim=0, index=perm_idx)\n",
    "\n",
    "        # For efficiency\n",
    "        projected_keys_visual = model.visual_attention.key_layer(\n",
    "            encoded_situations)  # [batch_size, situation_length, dec_hidden_dim]\n",
    "        projected_keys_textual = model.textual_attention.key_layer(\n",
    "            encoded_commands)  # [max_input_length, batch_size, dec_hidden_dim]\n",
    "        \n",
    "        return {\n",
    "            \"return_lstm_output\":[],\n",
    "            \"return_attention_weights\":[],\n",
    "            \"hidden\":hidden,\n",
    "            \"input_tokens_sorted\":input_tokens_sorted,\n",
    "            \"projected_keys_textual\":projected_keys_textual,\n",
    "            \"commands_lengths\":commands_lengths,\n",
    "            \"projected_keys_visual\":projected_keys_visual,\n",
    "            \"perm_idx\":perm_idx,\n",
    "            \"seq_lengths\":input_lengths,\n",
    "        }\n",
    "    hidden_layer = decode_input_preparation\n",
    "    \"\"\"\n",
    "    Here, we set to a static bound of decoding steps.\n",
    "    \"\"\"\n",
    "    max_time = 4\n",
    "    for i in range(max_time):\n",
    "        f = _generate_lstm_step_fxn(model.attention_decoder.forward_step, i)\n",
    "        hidden_layer = GraphNode(hidden_layer,\n",
    "                                 name=f\"lstm_step_{i}\",\n",
    "                                 forward=f)\n",
    "    \"\"\"\n",
    "    Formulating outputs.\n",
    "    \"\"\"\n",
    "    @GraphNode(hidden_layer)\n",
    "    def output_preparation(hidden_states):\n",
    "        hidden_states[\"return_lstm_output\"] = torch.cat(\n",
    "            hidden_states[\"return_lstm_output\"], dim=0)\n",
    "        hidden_states[\"return_attention_weights\"] = torch.cat(\n",
    "            hidden_states[\"return_attention_weights\"], dim=0)\n",
    "        \n",
    "        _, unperm_idx = hidden_states[\"perm_idx\"].sort(0)\n",
    "        hidden_states[\"return_lstm_output\"] = hidden_states[\"return_lstm_output\"].index_select(dim=1, index=unperm_idx)  # [max_time, batch_size, output_size]\n",
    "        hidden_states[\"seq_lengths\"] = hidden_states[\"seq_lengths\"][unperm_idx].tolist()\n",
    "        hidden_states[\"return_attention_weights\"] = hidden_states[\"return_attention_weights\"].index_select(dim=1, index=unperm_idx)\n",
    "        \n",
    "        decoder_output_batched = hidden_states[\"return_lstm_output\"]\n",
    "        context_situation = hidden_states[\"return_attention_weights\"]\n",
    "        decoder_output_batched = F.log_softmax(decoder_output_batched, dim=-1)\n",
    "        \n",
    "        if model.auxiliary_task:\n",
    "            pass # Not implemented yet.\n",
    "        else:\n",
    "            target_position_scores = torch.zeros(1), torch.zeros(1)\n",
    "            # We are not returning this as well, since it is not used...\n",
    "        \n",
    "        return (decoder_output_batched.transpose(0, 1), \n",
    "                target_position_scores) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "    \n",
    "    root = output_preparation # TODO: removing this and continue.\n",
    "    \n",
    "    return root\n",
    "    \n",
    "class ReaSCANMultiModalLSTMCompGraph(ComputationGraph):\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        self.model = model\n",
    "        root = generate_compute_graph(model)\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model to the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/ReaSCAN-Simple/\"\n",
    ")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "g = ReaSCANMultiModalLSTMCompGraph(\n",
    "     model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some examples to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1911, -1.9745, -2.0447, -1.8779, -1.9720, -1.7679, -1.8505],\n",
      "         [-1.8861, -1.9734, -1.8465, -2.0182, -2.0061, -1.8882, -2.0189],\n",
      "         [-1.8752, -1.9630, -1.8398, -2.0234, -2.0173, -1.9010, -2.0192],\n",
      "         [-1.8743, -1.9563, -1.8381, -2.0251, -2.0205, -1.9070, -2.0178],\n",
      "         [-1.8760, -1.9521, -1.8376, -2.0255, -2.0214, -1.9095, -2.0168],\n",
      "         [-1.9016, -2.0647, -1.8947, -2.1291, -1.9441, -1.8685, -1.8512],\n",
      "         [-1.8975, -1.9529, -1.8267, -2.0353, -2.0107, -1.9009, -2.0152],\n",
      "         [-1.8850, -1.9511, -1.8303, -2.0352, -2.0170, -1.9037, -2.0176],\n",
      "         [-1.8816, -1.9495, -1.8333, -2.0326, -2.0192, -1.9060, -2.0176],\n",
      "         [-1.8812, -1.9481, -1.8350, -2.0303, -2.0201, -1.9076, -2.0170],\n",
      "         [-1.9035, -2.0623, -1.8934, -2.1322, -1.9451, -1.8665, -1.8514],\n",
      "         [-1.8994, -1.9508, -1.8257, -2.0376, -2.0103, -1.8992, -2.0168],\n",
      "         [-1.8863, -1.9499, -1.8298, -2.0365, -2.0167, -1.9025, -2.0185],\n",
      "         [-1.8825, -1.9487, -1.8330, -2.0332, -2.0191, -1.9052, -2.0181],\n",
      "         [-1.8818, -1.9476, -1.8348, -2.0306, -2.0201, -1.9070, -2.0174],\n",
      "         [-2.0230, -2.1150, -1.8582, -1.9061, -1.8566, -1.6666, -2.3288],\n",
      "         [-1.9292, -2.0629, -1.8839, -2.1293, -1.9420, -1.8597, -1.8477],\n",
      "         [-1.9072, -1.9578, -1.8203, -2.0322, -2.0092, -1.8943, -2.0189],\n",
      "         [-1.8886, -1.9532, -1.8277, -2.0339, -2.0173, -1.8994, -2.0203],\n",
      "         [-1.8833, -1.9501, -1.8322, -2.0319, -2.0200, -1.9031, -2.0193],\n",
      "         [-1.8823, -1.9481, -1.8345, -2.0297, -2.0210, -1.9055, -2.0182],\n",
      "         [-1.9045, -2.0621, -1.8930, -2.1317, -1.9463, -1.8650, -1.8518],\n",
      "         [-1.8998, -1.9506, -1.8255, -2.0369, -2.0115, -1.8980, -2.0174],\n",
      "         [-1.8867, -1.9496, -1.8298, -2.0359, -2.0176, -1.9017, -2.0189],\n",
      "         [-1.8829, -1.9484, -1.8330, -2.0328, -2.0199, -1.9046, -2.0184],\n",
      "         [-1.8822, -1.9473, -1.8348, -2.0302, -2.0208, -1.9066, -2.0176],\n",
      "         [-1.9040, -2.0618, -1.8933, -2.1321, -1.9458, -1.8659, -1.8514],\n",
      "         [-2.0968, -1.9375, -1.9571, -1.8274, -1.6807, -2.2847, -1.9453]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for (input_batch, input_lengths, _, situation_batch, _, target_batch,\n",
    "     target_lengths, agent_positions, target_positions) in dataset.get_data_iterator(batch_size=1):\n",
    "    target_scores, target_position_scores = model(\n",
    "        commands_input=input_batch, commands_lengths=input_lengths,\n",
    "        situations_input=situation_batch, target_batch=target_batch,\n",
    "        target_lengths=target_lengths\n",
    "    )\n",
    "    print(target_scores)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"commands_input\": input_batch, \n",
    "    \"commands_lengths\": input_lengths,\n",
    "    \"situations_input\": situation_batch,\n",
    "    \"target_batch\": target_batch,\n",
    "    \"target_lengths\": target_lengths,\n",
    "}\n",
    "all_in = GraphInput(input_dict, batched=True, batch_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.1911, -1.9745, -2.0447, -1.8779, -1.9720, -1.7679, -1.8505],\n",
      "         [-1.8861, -1.9734, -1.8465, -2.0182, -2.0061, -1.8882, -2.0189],\n",
      "         [-1.8752, -1.9630, -1.8398, -2.0234, -2.0173, -1.9010, -2.0192],\n",
      "         [-1.8743, -1.9563, -1.8381, -2.0251, -2.0205, -1.9070, -2.0178]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target_scores, target_position_scores = g.compute(all_in)\n",
    "print(target_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
