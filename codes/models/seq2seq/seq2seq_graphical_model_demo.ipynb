{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script works on transform the seq2seq model to a graphical model using antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import json\n",
    "from model import *\n",
    "from ReaSCAN_dataset import *\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from antra.antra import *\n",
    "from decode_graphical_models import *\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# setting up the seeds.\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ReaSCAN dataset to load config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 02:10 Formulating the dataset from the passed in json file...\n",
      "2021-08-12 02:10 Loading vocabularies...\n",
      "2021-08-12 02:10 Done loading vocabularies.\n",
      "2021-08-12 02:10 Converting dataset to tensors...\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"../../../data-files/gSCAN-Simple/\"\n",
    "data_file = \"data-compositional-splits.txt\"\n",
    "input_vocab_file = \"input_vocabulary.txt\"\n",
    "target_vocab_file = \"target_vocabulary.txt\"\n",
    "dataset = ReaSCANDataset(\n",
    "    json.load(open(os.path.join(data_directory, data_file), \"r\")), \n",
    "    data_directory, split=\"train\",\n",
    "    input_vocabulary_file=input_vocab_file,\n",
    "    target_vocabulary_file=target_vocab_file,\n",
    "    generate_vocabulary=False,\n",
    "    k=0,\n",
    ")\n",
    "# Loading a couple of example from ReaSCAN.\n",
    "dataset.read_dataset(\n",
    "    max_examples=100,\n",
    "    simple_situation_representation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_vocabularies(\n",
    "#     input_vocabulary_file=\"input_vocabulary.txt\", \n",
    "#     target_vocabulary_file=\"target_vocabulary.txt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n",
    "def _generate_lstm_step_fxn(step_module, i, max_decode_step, \n",
    "                            image_size=6, hidden_dim=100, \n",
    "                            vocab_size=6):\n",
    "    \"\"\" \n",
    "    Generate a function for a layer in lstm.\n",
    "    \"\"\"\n",
    "\n",
    "    def _lstm_step_fxn(hidden_states):\n",
    "        if isnotebook():\n",
    "            \n",
    "            last_states = hidden_states\n",
    "            batch_size = last_states.size(0)\n",
    "\n",
    "            last_hidden = last_states[:,:hidden_dim].unsqueeze(dim=1).contiguous()\n",
    "            last_cell = last_states[:,hidden_dim:hidden_dim*2].unsqueeze(dim=1).contiguous()\n",
    "            input_tokens_sorted = last_states[:,hidden_dim*2:hidden_dim*2+max_decode_step].long().contiguous()\n",
    "            \n",
    "            commands_lengths = last_states[\n",
    "                :,hidden_dim*2+max_decode_step:hidden_dim*2+max_decode_step+1\n",
    "            ].long().contiguous()\n",
    "            \n",
    "            projected_keys_visual = last_states[\n",
    "                :,hidden_dim*2+max_decode_step+1:hidden_dim*2+max_decode_step+1+image_size*image_size*hidden_dim\n",
    "            ].reshape(\n",
    "                batch_size, image_size*image_size, hidden_dim\n",
    "            ).contiguous()\n",
    "            \n",
    "            _output = last_states[\n",
    "                :,hidden_dim*2+max_decode_step+1+image_size*image_size*hidden_dim:hidden_dim*2+max_decode_step+1+image_size*image_size*hidden_dim+vocab_size\n",
    "            ].contiguous()\n",
    "            \n",
    "            projected_keys_textual = last_states[\n",
    "                :,hidden_dim*2+max_decode_step+1+image_size*image_size*hidden_dim+vocab_size:\n",
    "            ].reshape(\n",
    "                batch_size, -1, hidden_dim\n",
    "            ).contiguous()\n",
    "            \n",
    "            (output, hidden) = step_module.forward(\n",
    "                lstm_input_tokens_sorted=input_tokens_sorted[:, i], \n",
    "                lstm_hidden=(last_hidden, last_cell), \n",
    "                lstm_projected_keys_textual=projected_keys_textual, \n",
    "                lstm_commands_lengths=commands_lengths, \n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "        else:\n",
    "            (output, hidden) = step_module(\n",
    "                lstm_input_tokens_sorted=hidden_states[\"input_tokens_sorted\"][:, i], \n",
    "                lstm_hidden=hidden_states[\"hidden\"], \n",
    "                lstm_projected_keys_textual=hidden_states[\"projected_keys_textual\"], \n",
    "                lstm_commands_lengths=hidden_states[\"commands_lengths\"], \n",
    "                lstm_projected_keys_visual=hidden_states[\"projected_keys_visual\"],\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "        \n",
    "        last_states = torch.cat(\n",
    "                [\n",
    "                    hidden[0].squeeze(dim=1),\n",
    "                    hidden[1].squeeze(dim=1),\n",
    "                    input_tokens_sorted,\n",
    "                    commands_lengths,\n",
    "                    projected_keys_visual.reshape(batch_size, -1),\n",
    "                    output,\n",
    "                    projected_keys_textual.reshape(batch_size, -1),\n",
    "                ], dim=-1\n",
    "        )\n",
    "        return last_states\n",
    "\n",
    "    return _lstm_step_fxn\n",
    "\n",
    "def generate_compute_graph(\n",
    "    model, \n",
    "    max_decode_step,\n",
    "    cache_results=False, \n",
    "    vocab_size=6,\n",
    "    image_size=6, \n",
    "    hidden_dim=100\n",
    "):\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input preparation.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Command Inputs.\n",
    "    \"\"\"\n",
    "    command_world_inputs = [\"commands_input\", \"commands_lengths\"]\n",
    "    command_world_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, default_value=None) \n",
    "        for name in command_world_inputs\n",
    "    ]\n",
    "    @GraphNode(*command_world_input_leaves, cache_results=False)\n",
    "    def command_input_preparation(\n",
    "        commands_input, commands_lengths,\n",
    "    ):\n",
    "        input_dict = {\n",
    "            \"commands_input\": commands_input,\n",
    "            \"commands_lengths\": commands_lengths,\n",
    "        }\n",
    "        # We may not need the following fields. But we leave it here in case we need these\n",
    "        # to generate other inputs.\n",
    "        batch_size = input_dict[\"commands_input\"].shape[0]\n",
    "        device = input_dict[\"commands_input\"].device\n",
    "        return input_dict\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation Inputs.\n",
    "    \"\"\"\n",
    "    situation_inputs = [\"situations_input\"]\n",
    "    situation_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, \n",
    "                       default_value=None) \n",
    "        for name in situation_inputs\n",
    "    ]\n",
    "    @GraphNode(*situation_input_leaves, cache_results=cache_results)\n",
    "    def situation_input_preparation(\n",
    "        situations_input,\n",
    "    ):\n",
    "        return {\n",
    "            \"situations_input\": situations_input,\n",
    "        }\n",
    "        \n",
    "    \"\"\"\n",
    "    Target Inputs\n",
    "    \"\"\"\n",
    "    target_sequence_inputs = [\"target_batch\", \"target_lengths\"]\n",
    "    target_sequence_input_leaves = [\n",
    "        GraphNode.leaf(name=name, use_default=True, \n",
    "                       default_value=None) \n",
    "        for name in target_sequence_inputs\n",
    "    ]\n",
    "    @GraphNode(*target_sequence_input_leaves, \n",
    "               cache_results=cache_results)\n",
    "    def target_sequence_input_preparation(\n",
    "        target_batch, target_lengths\n",
    "    ):\n",
    "        return {\n",
    "            \"target_batch\": target_batch,\n",
    "            \"target_lengths\": target_lengths,\n",
    "        }\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Input encoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Situation Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(situation_input_preparation, \n",
    "               cache_results=cache_results)\n",
    "    def situation_encode(input_dict):\n",
    "        if isnotebook():\n",
    "            return model.forward(\n",
    "                situations_input=input_dict[\"situations_input\"],\n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "        else:\n",
    "            return model(\n",
    "                situations_input=input_dict[\"situations_input\"],\n",
    "                tag=\"situation_encode\"\n",
    "            )\n",
    "    \n",
    "    \"\"\"\n",
    "    Language Encoding.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_preparation, \n",
    "               cache_results=cache_results)\n",
    "    def command_input_encode(input_dict):\n",
    "        if isnotebook():\n",
    "            return model.forward(\n",
    "                commands_input=input_dict[\"commands_input\"], \n",
    "                commands_lengths=input_dict[\"commands_lengths\"],\n",
    "                tag=\"command_input_encode\"\n",
    "            )\n",
    "        else:\n",
    "            return model(\n",
    "                commands_input=input_dict[\"commands_input\"], \n",
    "                commands_lengths=input_dict[\"commands_lengths\"],\n",
    "                tag=\"command_input_encode\"\n",
    "            )\n",
    "    \n",
    "    ####################\n",
    "    #\n",
    "    # Decoding.\n",
    "    #\n",
    "    ####################\n",
    "    \"\"\"\n",
    "    Preparation of Decoding Data structure.\n",
    "    \"\"\"\n",
    "    @GraphNode(command_input_encode, situation_encode, \n",
    "               target_sequence_input_preparation, \n",
    "               cache_results=cache_results)\n",
    "    def decode_input_preparation(c_encode, s_encode, target_sequence):\n",
    "        if isnotebook():\n",
    "            hidden_states = model.forward(\n",
    "                target_batch=target_sequence[\"target_batch\"],\n",
    "                target_lengths=target_sequence[\"target_lengths\"],\n",
    "                command_hidden=c_encode[\"command_hidden\"],\n",
    "                command_encoder_outputs=c_encode[\"command_encoder_outputs\"],\n",
    "                command_sequence_lengths=c_encode[\"command_sequence_lengths\"],\n",
    "                encoded_situations=s_encode,\n",
    "                tag=\"decode_input_preparation\"\n",
    "            )\n",
    "        else:\n",
    "            hidden_states = model(\n",
    "                target_batch=target_sequence[\"target_batch\"],\n",
    "                target_lengths=target_sequence[\"target_lengths\"],\n",
    "                command_hidden=c_encode[\"command_hidden\"],\n",
    "                command_encoder_outputs=c_encode[\"command_encoder_outputs\"],\n",
    "                command_sequence_lengths=c_encode[\"command_sequence_lengths\"],\n",
    "                encoded_situations=s_encode,\n",
    "                tag=\"decode_input_preparation\"\n",
    "            )\n",
    "        # dummy output tensor for the first time.\n",
    "        batch_size = hidden_states[\"input_tokens_sorted\"].size(0)\n",
    "        return torch.cat(\n",
    "                [\n",
    "                    hidden_states[\"hidden\"][0].squeeze(dim=1),\n",
    "                    hidden_states[\"hidden\"][1].squeeze(dim=1),\n",
    "                    hidden_states[\"input_tokens_sorted\"],\n",
    "                    hidden_states[\"commands_lengths\"],\n",
    "                    hidden_states[\"projected_keys_visual\"].reshape(batch_size, -1),\n",
    "                    torch.zeros(batch_size, vocab_size),\n",
    "                    # we need the textual key to be at last since the dimension is not interpretable.\n",
    "                    hidden_states[\"projected_keys_textual\"].reshape(batch_size, -1)\n",
    "                ], dim=-1\n",
    "            )\n",
    "        \n",
    "\n",
    "    hidden_layer = decode_input_preparation\n",
    "    \"\"\"\n",
    "    Here, we set to a static bound of decoding steps.\n",
    "    \"\"\"\n",
    "    for i in range(max_decode_step):\n",
    "        f = _generate_lstm_step_fxn(\n",
    "            model, i, max_decode_step,\n",
    "            vocab_size=vocab_size,\n",
    "            image_size=image_size, \n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "        hidden_layer = GraphNode(hidden_layer,\n",
    "                                 name=f\"lstm_step_{i}\",\n",
    "                                 forward=f, cache_results=cache_results)\n",
    "        \n",
    "    # Do we really need this?\n",
    "    # \"\"\"\n",
    "    # Formulating outputs.\n",
    "    # \"\"\"\n",
    "    # @GraphNode(hidden_layer, cache_results=cache_results)\n",
    "    # def output_preparation(hidden_states):\n",
    "    #     hidden_states[\"return_lstm_output\"] = torch.cat(\n",
    "    #         hidden_states[\"return_lstm_output\"], dim=0)\n",
    "    #     hidden_states[\"return_attention_weights\"] = torch.cat(\n",
    "    #         hidden_states[\"return_attention_weights\"], dim=0)\n",
    "    #     \n",
    "    #     decoder_output_batched = hidden_states[\"return_lstm_output\"]\n",
    "    #     context_situation = hidden_states[\"return_attention_weights\"]\n",
    "    #     decoder_output_batched = F.log_softmax(decoder_output_batched, dim=-1)\n",
    "    #     \n",
    "    #     # if model.module.auxiliary_task:\n",
    "    #     if False:\n",
    "    #         pass # Not implemented yet.\n",
    "    #     else:\n",
    "    #         target_position_scores = torch.zeros(1), torch.zeros(1)\n",
    "    #        # We are not returning this as well, since it is not used...\n",
    "    #    print(decoder_output_batched.shape)\n",
    "    #    return decoder_output_batched.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "    # root = hidden_layer # TODO: removing this and continue.\n",
    "    \n",
    "    return hidden_layer\n",
    "    \n",
    "class ReaSCANMultiModalLSTMCompGraph(ComputationGraph):\n",
    "    def __init__(self, model,\n",
    "                 max_decode_step,\n",
    "                 cache_results=False):\n",
    "        self.model = model\n",
    "        root = generate_compute_graph(\n",
    "            model,\n",
    "            max_decode_step,\n",
    "        )\n",
    "\n",
    "        super().__init__(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model to the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    input_vocabulary_size=dataset.input_vocabulary_size,\n",
    "    target_vocabulary_size=dataset.target_vocabulary_size,\n",
    "    num_cnn_channels=dataset.image_channels,\n",
    "    input_padding_idx=dataset.input_vocabulary.pad_idx,\n",
    "    target_pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "    target_eos_idx=dataset.target_vocabulary.eos_idx,\n",
    "    # language encoder config\n",
    "    embedding_dimension=25,\n",
    "    encoder_hidden_size=100,\n",
    "    num_encoder_layers=1,\n",
    "    encoder_dropout_p=0.3,\n",
    "    encoder_bidirectional=True,\n",
    "    # world encoder config\n",
    "    simple_situation_representation=True,\n",
    "    cnn_hidden_num_channels=50,\n",
    "    cnn_kernel_size=7,\n",
    "    cnn_dropout_p=0.1,\n",
    "    auxiliary_task=False,\n",
    "    # decoder config\n",
    "    num_decoder_layers=1,\n",
    "    attention_type=\"bahdanau\",\n",
    "    decoder_dropout_p=0.3,\n",
    "    decoder_hidden_size=100,\n",
    "    conditional_attention=True,\n",
    "    output_directory=\"../../../saved_models/gSCAN-Simple/\"\n",
    ")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "G = ReaSCANMultiModalLSTMCompGraph(\n",
    "    model=model,\n",
    "    max_decode_step=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = dataset.get_dual_dataset()\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some examples to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # just using this loop to get a pair of examples\n",
    "    input_batch, target_batch, situation_batch, \\\n",
    "        agent_positions_batch, target_positions_batch, \\\n",
    "        input_lengths_batch, target_lengths_batch, \\\n",
    "        dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "        dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "        dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "low1 = {\n",
    "    \"commands_input\": input_batch, \n",
    "    \"commands_lengths\": input_lengths_batch,\n",
    "    \"situations_input\": situation_batch,\n",
    "    \"target_batch\": target_batch,\n",
    "    \"target_lengths\": target_lengths_batch,\n",
    "}\n",
    "low1 = GraphInput(low1, batched=True, batch_dim=0, cache_results=False)\n",
    "\n",
    "low2 = {\n",
    "    \"commands_input\": dual_input_batch, \n",
    "    \"commands_lengths\": dual_input_lengths_batch,\n",
    "    \"situations_input\": dual_situation_batch,\n",
    "    \"target_batch\": dual_target_batch,\n",
    "    \"target_lengths\": dual_target_lengths_batch,\n",
    "}\n",
    "low2 = GraphInput(low2, batched=True, batch_dim=0, cache_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dual_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "low2_hidden = G.compute_node('lstm_step_2', low2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0345, -0.2683,  0.0763,  ...,  0.0531, -0.1266, -0.0160],\n",
       "        [-0.0148, -0.2442, -0.0280,  ...,  0.0531, -0.1266, -0.0160]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low2_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "low2_hidden_select = {\n",
    "    'hidden': low2_hidden['hidden']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from antra.antra.utils import serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [serialize(x) for x in low2_hidden[\"projected_keys_textual\"]]\n",
    "low_interv_input = GraphInput.batched(\n",
    "    values={\"lstm_step_1\": low2_hidden_select},\n",
    "    batch_dim=0,\n",
    "    keys=keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_interv = Intervention.batched(\n",
    "    low1, low_interv_input, \n",
    "    cache_results=False,\n",
    "    cache_base_results=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'return_lstm_output': [tensor([[[ 0.1085,  0.3292, -0.3566,  0.1918,  0.0502,  0.2859, -0.1443,\n",
       "              0.1870]]], grad_fn=<UnsqueezeBackward0>),\n",
       "   tensor([[[ 0.4515, -0.1367, -0.0346,  0.1133, -0.0544, -0.4565, -0.1353,\n",
       "             -0.0419]]], grad_fn=<UnsqueezeBackward0>)],\n",
       "  'return_attention_weights': [tensor([[[0.0273, 0.0286, 0.0273, 0.0274, 0.0290, 0.0280, 0.0275, 0.0278,\n",
       "             0.0283, 0.0273, 0.0280, 0.0276, 0.0273, 0.0274, 0.0280, 0.0288,\n",
       "             0.0275, 0.0291, 0.0274, 0.0283, 0.0275, 0.0282, 0.0269, 0.0274,\n",
       "             0.0269, 0.0284, 0.0276, 0.0277, 0.0281, 0.0274, 0.0275, 0.0294,\n",
       "             0.0275, 0.0273, 0.0274, 0.0268]]], grad_fn=<UnsqueezeBackward0>),\n",
       "   tensor([[[0.0273, 0.0286, 0.0273, 0.0274, 0.0290, 0.0280, 0.0275, 0.0278,\n",
       "             0.0283, 0.0273, 0.0280, 0.0276, 0.0273, 0.0274, 0.0280, 0.0288,\n",
       "             0.0275, 0.0291, 0.0274, 0.0283, 0.0275, 0.0282, 0.0269, 0.0274,\n",
       "             0.0269, 0.0284, 0.0276, 0.0277, 0.0281, 0.0274, 0.0275, 0.0294,\n",
       "             0.0275, 0.0273, 0.0274, 0.0268]]], grad_fn=<UnsqueezeBackward0>)],\n",
       "  'hidden': (tensor([[[-0.2941,  0.0210,  0.1611,  0.1123, -0.0443, -0.0085, -0.0109,\n",
       "             -0.2795, -0.1061, -0.1821, -0.1906,  0.4063, -0.2103, -0.0786,\n",
       "              0.0607,  0.0716,  0.0083, -0.2088,  0.1770, -0.0096, -0.0532,\n",
       "             -0.0768, -0.0402,  0.1037, -0.1295,  0.2640, -0.0923, -0.0755,\n",
       "             -0.2759, -0.0068,  0.0416,  0.1832,  0.1545, -0.0816,  0.1754,\n",
       "              0.0478, -0.0019, -0.0351, -0.0860,  0.1058, -0.1117,  0.1267,\n",
       "             -0.0415,  0.1165,  0.1278,  0.0479, -0.2348,  0.0461,  0.0599,\n",
       "              0.0220,  0.0294, -0.2257,  0.0733,  0.0144, -0.1154,  0.0529,\n",
       "             -0.1442,  0.0672,  0.0155, -0.1509,  0.0092,  0.1691,  0.0336,\n",
       "              0.2901, -0.0581,  0.0164, -0.1927, -0.1589, -0.0773, -0.1761,\n",
       "              0.1626, -0.1013, -0.1475,  0.0845, -0.0093, -0.1235,  0.0105,\n",
       "              0.1052,  0.0316,  0.0179,  0.1157, -0.0401,  0.2079,  0.2140,\n",
       "             -0.0277, -0.0232,  0.0414,  0.2259,  0.1889, -0.0243, -0.0605,\n",
       "              0.2362, -0.0554,  0.0422,  0.0573, -0.1195, -0.1436, -0.1049,\n",
       "             -0.1196,  0.0074]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[-0.5133,  0.0355,  0.3359,  0.2600, -0.0787, -0.0126, -0.0186,\n",
       "             -0.6003, -0.2028, -0.4345, -0.2673,  0.7137, -0.4544, -0.1383,\n",
       "              0.0970,  0.1857,  0.0133, -0.3856,  0.2427, -0.0149, -0.1468,\n",
       "             -0.1804, -0.1448,  0.2874, -0.2363,  0.3701, -0.1783, -0.1171,\n",
       "             -0.3948, -0.0123,  0.0842,  0.3740,  0.3328, -0.1558,  0.3042,\n",
       "              0.1040, -0.0067, -0.1083, -0.2048,  0.1491, -0.1877,  0.3822,\n",
       "             -0.0582,  0.3209,  0.3891,  0.0817, -0.3500,  0.1558,  0.1295,\n",
       "              0.0555,  0.0695, -0.4228,  0.0928,  0.0219, -0.1753,  0.1056,\n",
       "             -0.3281,  0.1829,  0.0258, -0.3066,  0.0172,  0.3021,  0.0747,\n",
       "              0.5888, -0.1342,  0.0327, -0.6701, -0.2508, -0.1453, -0.3074,\n",
       "              0.2790, -0.1364, -0.2814,  0.2469, -0.0194, -0.2533,  0.0137,\n",
       "              0.1587,  0.1158,  0.0427,  0.3680, -0.1040,  0.4196,  0.4281,\n",
       "             -0.0940, -0.0529,  0.2011,  0.4451,  0.2775, -0.0593, -0.1009,\n",
       "              0.3788, -0.1310,  0.0877,  0.0906, -0.2485, -0.3024, -0.2051,\n",
       "             -0.2624,  0.0134]]], grad_fn=<TransposeBackward0>)),\n",
       "  'input_tokens_sorted': tensor([[1, 4, 3, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0]]),\n",
       "  'projected_keys_textual': tensor([[[ 4.7460e-02,  8.5277e-02, -9.4986e-02,  7.0504e-02, -6.4376e-02,\n",
       "            -4.8510e-02, -2.5552e-02,  3.8378e-02, -1.3505e-01, -9.4378e-05,\n",
       "            -1.1899e-02, -4.3824e-02,  8.8509e-02, -1.2764e-02, -7.9371e-02,\n",
       "            -5.6533e-04,  1.2283e-01, -4.4914e-02, -5.5439e-02,  1.1354e-01,\n",
       "             6.0451e-02, -1.1641e-01, -5.3944e-02, -6.2068e-03,  1.5395e-01,\n",
       "             1.4703e-02,  6.7290e-02, -4.1493e-03, -1.0215e-02,  8.2744e-03,\n",
       "             8.0950e-02,  1.6959e-02,  1.8093e-02, -5.4427e-02, -5.6345e-03,\n",
       "            -1.3236e-01,  3.4416e-02,  1.4945e-02, -5.6635e-03, -1.0648e-02,\n",
       "            -8.0301e-02, -8.0361e-02, -1.0489e-02, -3.9586e-03, -3.6036e-03,\n",
       "             2.9003e-02, -8.8163e-02, -1.7494e-02, -1.2132e-01, -1.1659e-02,\n",
       "             1.3971e-01, -5.0124e-02, -4.8936e-02,  8.6705e-02, -4.0693e-03,\n",
       "            -3.3868e-02, -1.5533e-02,  3.6177e-02,  2.6552e-02, -1.0255e-01,\n",
       "             2.7717e-02, -1.5652e-01, -6.1686e-02,  5.9268e-02, -8.0197e-03,\n",
       "            -2.7756e-02, -7.1741e-02, -3.5789e-02, -4.9152e-02, -7.3845e-02,\n",
       "            -2.2124e-03,  9.7916e-02,  3.9815e-02, -4.5376e-02, -5.8784e-02,\n",
       "            -1.7536e-02, -3.3865e-02,  1.3114e-01, -1.0281e-01, -2.5154e-04,\n",
       "             2.4449e-03,  3.0693e-03, -6.7002e-02, -1.5610e-02, -2.6793e-02,\n",
       "            -4.3472e-02,  1.4288e-02,  3.9555e-02,  5.6393e-02, -7.2569e-02,\n",
       "            -1.0969e-02,  8.5925e-02,  3.0651e-02,  4.2885e-02, -8.7499e-03,\n",
       "            -2.9760e-03,  8.6029e-02,  4.9900e-02, -5.6183e-02, -5.3163e-02],\n",
       "           [-5.1131e-02,  7.2843e-02,  1.1782e-01,  1.0282e-01, -2.5361e-02,\n",
       "            -8.5709e-02, -4.7266e-02,  1.0061e-01, -5.8152e-02,  2.6752e-03,\n",
       "            -1.3149e-01, -1.1998e-02,  1.4750e-01,  2.7714e-02,  1.4732e-02,\n",
       "            -7.4834e-02,  4.7676e-02, -8.6581e-03, -1.8700e-01,  7.9342e-02,\n",
       "            -2.9818e-02,  9.7108e-03,  5.7466e-03, -5.2767e-02,  5.2330e-02,\n",
       "            -1.2637e-02,  2.7650e-02,  3.2225e-02,  1.5347e-02, -2.9910e-02,\n",
       "             9.5774e-02,  1.0035e-01,  4.6744e-02, -8.3731e-02,  3.3850e-02,\n",
       "            -9.5089e-02,  4.1095e-02, -1.9426e-02,  3.2930e-02, -5.5443e-02,\n",
       "            -2.2201e-02, -9.9154e-02, -7.2614e-02,  1.4891e-03,  5.3565e-03,\n",
       "            -3.6563e-02, -9.0047e-02,  5.0900e-02,  2.4651e-02, -8.0326e-04,\n",
       "             1.3208e-01, -7.8697e-02, -3.6843e-02,  1.9311e-01, -1.1443e-01,\n",
       "             1.6508e-02, -4.6118e-02, -3.4801e-02,  9.9569e-02,  1.2777e-01,\n",
       "             3.0874e-02, -1.9130e-02, -4.1792e-02,  6.3593e-02,  5.5375e-02,\n",
       "             4.3933e-02, -6.5124e-02, -8.2066e-02, -5.4708e-02, -4.5416e-02,\n",
       "            -1.2438e-01,  1.2113e-01,  1.1149e-01, -6.6882e-02, -1.2581e-01,\n",
       "            -3.3420e-03, -1.3929e-01,  3.0838e-01, -4.4129e-02, -8.1191e-02,\n",
       "             3.6756e-02, -9.8851e-02, -3.3986e-02, -1.1701e-02,  1.7578e-02,\n",
       "            -9.0434e-02,  7.8400e-02,  7.0256e-02, -3.5653e-02, -2.1804e-02,\n",
       "            -1.0460e-01,  1.3548e-01,  1.0524e-02,  5.4955e-02, -2.0313e-02,\n",
       "            -9.9010e-02,  1.4775e-01,  5.5330e-02, -5.1441e-02, -1.2535e-01],\n",
       "           [-7.1749e-02,  2.9261e-02,  3.0692e-02,  1.0662e-01,  3.2708e-02,\n",
       "            -5.8314e-02,  1.0063e-02,  8.6071e-02, -9.8172e-02,  3.4661e-02,\n",
       "            -5.5773e-02, -1.2194e-02,  1.0236e-01,  1.2372e-03,  7.5321e-02,\n",
       "            -1.1914e-01, -5.2117e-02, -4.1283e-02, -1.2615e-01,  6.3999e-02,\n",
       "            -5.4736e-02,  1.6684e-02, -1.7370e-03, -2.4536e-02, -4.8959e-03,\n",
       "             6.3408e-02,  1.4311e-02, -2.5597e-03, -1.7924e-02, -1.3607e-02,\n",
       "             7.0014e-02,  9.4271e-02, -3.4306e-02, -8.2708e-02,  5.4088e-02,\n",
       "            -6.1782e-02,  5.1903e-02, -3.5434e-03, -3.9151e-02, -5.0081e-02,\n",
       "            -4.2355e-02, -9.7917e-02, -6.6543e-02, -3.8217e-02,  2.8807e-02,\n",
       "             1.0014e-01, -4.1692e-02, -2.5457e-03,  5.5867e-05, -7.7712e-02,\n",
       "            -1.4461e-04, -4.8585e-02,  4.1122e-03,  1.0507e-01, -4.5657e-02,\n",
       "             8.6304e-03, -3.9861e-02,  1.1403e-02,  9.0227e-02,  3.2678e-02,\n",
       "             5.8541e-03, -8.2354e-02, -7.4843e-02,  7.8196e-02,  7.0121e-03,\n",
       "            -2.3159e-02, -7.7607e-02, -5.6743e-02, -3.2915e-02, -5.4917e-02,\n",
       "            -1.2637e-01,  8.3718e-02, -3.1844e-02,  1.5213e-02, -1.5456e-01,\n",
       "             1.0402e-01, -4.1944e-02,  2.9933e-01, -1.0339e-01, -7.5630e-02,\n",
       "             9.2017e-02, -1.0202e-01,  2.1392e-02, -3.5003e-02, -4.4907e-02,\n",
       "            -7.5103e-02,  3.8052e-02,  8.9114e-02, -3.7847e-02, -4.3347e-02,\n",
       "            -3.8889e-02,  1.7948e-01,  2.5412e-02, -8.2306e-02, -2.3889e-02,\n",
       "            -6.1512e-02, -4.8974e-03,  5.2350e-02, -6.7753e-02, -1.6746e-01],\n",
       "           [ 5.5556e-02, -1.0426e-04,  1.5071e-02,  7.3226e-02, -6.6108e-02,\n",
       "            -8.1450e-02,  7.0043e-04,  1.3731e-02, -1.9850e-02,  4.9307e-02,\n",
       "            -4.8018e-02, -9.1174e-02,  3.7333e-02,  4.0853e-02,  1.0849e-02,\n",
       "            -8.8703e-02, -2.8565e-02, -5.9615e-02,  4.9483e-03,  6.0324e-02,\n",
       "             4.9696e-02,  8.2904e-02,  1.6528e-02, -2.7537e-02, -1.9562e-02,\n",
       "             9.7623e-03, -9.4718e-02, -3.1269e-02,  3.7276e-02, -6.7321e-03,\n",
       "            -1.8151e-02,  1.1711e-01, -6.1154e-02,  5.9132e-04, -6.4353e-02,\n",
       "             4.2530e-02,  7.2309e-02, -6.2162e-02, -2.5994e-02, -4.8582e-03,\n",
       "            -2.3492e-02,  1.5727e-02, -7.6271e-02, -5.4142e-02,  5.9609e-02,\n",
       "             9.5869e-02,  5.9863e-03,  4.9868e-03,  1.2799e-03, -8.0024e-02,\n",
       "             1.3794e-02,  1.8801e-02,  3.5053e-02,  6.4527e-02,  2.1154e-02,\n",
       "            -8.4928e-02, -1.6599e-02,  2.3871e-02,  7.3555e-02,  2.8226e-02,\n",
       "             6.4560e-02, -1.4741e-01, -1.3619e-01, -5.0165e-02,  4.4166e-02,\n",
       "             6.2052e-02, -4.3540e-02, -1.3824e-02, -5.7266e-02, -3.3444e-02,\n",
       "             6.9866e-03,  1.1727e-01, -2.0544e-02,  1.8752e-02, -5.9377e-03,\n",
       "             5.8086e-02, -9.7940e-02,  1.7221e-01, -5.1340e-02, -1.3418e-01,\n",
       "             4.9832e-02, -5.0734e-02, -5.0721e-03, -2.3101e-02, -1.3115e-01,\n",
       "             2.4663e-02,  1.1737e-01,  3.7756e-02, -9.6785e-03, -3.1473e-02,\n",
       "            -2.0900e-02, -3.8436e-02, -2.6599e-02, -1.1369e-01, -6.1035e-02,\n",
       "            -4.8196e-02,  3.5287e-03,  3.1111e-02, -5.8675e-02, -4.4188e-02],\n",
       "           [ 3.3498e-02, -9.6755e-02,  2.6076e-02,  3.5064e-02,  1.0572e-02,\n",
       "            -5.9544e-02, -7.1324e-02,  1.2474e-02, -2.8835e-03,  6.2324e-02,\n",
       "            -6.8777e-02, -1.2542e-02,  2.9977e-02,  8.6180e-02,  7.2821e-04,\n",
       "            -8.9666e-02,  1.4404e-01, -2.0707e-02,  2.6103e-02,  1.6059e-01,\n",
       "             9.3123e-02,  4.6286e-02,  4.6858e-02, -6.6827e-02,  1.6079e-02,\n",
       "            -5.7256e-02, -1.2681e-02, -3.9985e-02,  3.4257e-02,  4.7575e-02,\n",
       "             7.0290e-02,  2.7944e-03, -3.3482e-02,  5.6329e-02, -3.1650e-02,\n",
       "             3.7197e-02,  7.1486e-02, -7.3542e-02,  6.5911e-02, -3.7691e-02,\n",
       "            -8.4256e-04,  4.8624e-02, -1.6341e-01,  2.0188e-03,  1.7334e-02,\n",
       "             4.6636e-02, -5.9097e-02,  2.6386e-02, -4.3805e-03,  3.7176e-02,\n",
       "             4.4744e-02, -4.7913e-02, -5.9563e-02,  4.0939e-02,  1.4899e-04,\n",
       "            -3.2696e-02,  2.7511e-02, -6.6452e-02,  3.8092e-02,  1.0750e-02,\n",
       "             4.3177e-02, -8.0740e-02,  4.0241e-02,  2.8931e-03,  6.5522e-02,\n",
       "             9.1411e-03, -3.5308e-02,  1.5216e-02,  7.9893e-02, -7.1470e-02,\n",
       "            -8.3247e-03,  1.0458e-01,  7.1280e-02, -5.7346e-02, -3.2411e-02,\n",
       "            -6.6533e-02, -1.3565e-01,  1.3009e-01, -9.9928e-03, -5.4137e-02,\n",
       "            -2.7496e-02,  3.6660e-02,  8.2580e-03,  3.6921e-02, -5.7435e-02,\n",
       "             1.8154e-02,  7.3972e-02,  3.4874e-03,  1.0328e-01, -6.2285e-02,\n",
       "            -4.8282e-02, -5.9297e-02, -3.8582e-02, -3.5548e-02, -1.3294e-02,\n",
       "            -1.3683e-03,  5.4642e-02,  6.0412e-02, -3.4229e-02,  7.7969e-02],\n",
       "           [ 3.2697e-02, -4.0040e-02,  5.0802e-02,  1.4147e-02, -3.5988e-02,\n",
       "            -1.3374e-01, -4.8695e-02,  3.5449e-02, -1.0199e-01, -7.6902e-02,\n",
       "            -7.4898e-02, -6.6948e-02,  1.3258e-02,  6.7126e-02,  1.9089e-02,\n",
       "            -1.3704e-01,  7.0914e-02, -2.5406e-02, -3.7496e-02,  2.1336e-02,\n",
       "            -9.9669e-03, -7.3446e-02, -2.2905e-02, -1.0869e-01,  1.0291e-01,\n",
       "            -7.0882e-02,  2.0242e-02, -4.0210e-03, -2.3607e-02,  5.4725e-03,\n",
       "             1.7806e-01, -3.7153e-02, -4.0216e-02, -8.2188e-02,  2.5298e-02,\n",
       "             3.9624e-02, -8.6150e-03, -4.3215e-03,  7.8683e-02, -1.0192e-01,\n",
       "             2.1541e-02, -4.6118e-02, -1.4443e-01,  4.2716e-02, -7.9815e-02,\n",
       "            -1.4342e-02, -9.9636e-02, -8.5373e-02,  5.7513e-02,  3.2214e-02,\n",
       "             8.9994e-02, -1.2140e-01, -8.3592e-02,  8.5672e-02,  1.1418e-02,\n",
       "            -4.8292e-02,  9.4594e-02, -3.0605e-02,  4.0850e-02,  4.0365e-03,\n",
       "             1.3107e-02, -9.2535e-02,  7.4307e-03,  8.7999e-03,  1.5539e-01,\n",
       "            -5.4044e-02, -3.8017e-02, -1.9521e-02,  8.4911e-02, -1.2702e-01,\n",
       "            -2.5060e-02,  8.3484e-02,  7.8085e-02, -7.8983e-02, -1.3750e-01,\n",
       "            -9.3435e-02, -8.7978e-02,  2.3252e-01, -3.1771e-02, -1.5689e-01,\n",
       "             3.4060e-02,  4.9066e-02,  7.8876e-02,  8.6235e-02,  1.0550e-02,\n",
       "            -2.9121e-02,  1.1229e-01, -4.2790e-02,  1.5288e-01, -9.6772e-03,\n",
       "            -8.9245e-02,  6.5734e-02,  1.9387e-02, -4.5078e-02, -5.3174e-02,\n",
       "            -1.6077e-02,  7.2190e-02,  8.1317e-02, -2.4726e-02,  1.1641e-02],\n",
       "           [ 1.7292e-02,  1.7065e-02,  2.1621e-02,  8.5497e-03, -1.0553e-01,\n",
       "            -5.4947e-02,  2.1310e-02, -7.0492e-02, -3.3451e-02,  7.5866e-02,\n",
       "             5.1485e-02,  3.2510e-02,  4.6512e-02, -4.9105e-02, -2.4563e-02,\n",
       "            -8.7928e-02,  1.2218e-01, -5.7102e-02, -5.7950e-02,  4.1794e-02,\n",
       "             6.3900e-02, -5.2032e-02, -2.5242e-02, -6.4056e-02,  9.7671e-02,\n",
       "            -8.2224e-02, -1.1510e-02, -1.1201e-01, -7.9528e-02, -1.1501e-03,\n",
       "             1.3363e-01, -7.0310e-02,  7.9003e-02, -9.0152e-02,  1.1732e-01,\n",
       "             6.0044e-02, -1.9849e-02,  1.4336e-01,  1.3351e-02, -2.7847e-03,\n",
       "             8.4602e-02, -1.0403e-01,  6.9431e-02,  5.3466e-02, -7.5220e-02,\n",
       "            -2.3215e-02, -1.1241e-02,  1.5727e-02, -6.5190e-03, -4.2409e-02,\n",
       "             1.0818e-02, -1.1996e-01, -3.8482e-03,  2.4036e-02,  1.2332e-01,\n",
       "             1.5053e-02,  8.7920e-02,  6.9108e-02,  8.7300e-03,  7.0794e-02,\n",
       "            -2.6061e-02, -1.7689e-01, -2.9684e-02,  5.1999e-04, -3.2634e-02,\n",
       "             4.0304e-02,  3.3504e-03, -7.3749e-02, -8.2116e-02, -1.1550e-01,\n",
       "             3.7945e-03, -3.9883e-02,  5.2411e-02,  8.8799e-03, -6.9665e-02,\n",
       "            -5.4063e-02, -1.4635e-01,  2.9470e-03, -1.0738e-01, -5.9514e-02,\n",
       "             9.5726e-02, -4.3365e-03,  1.0184e-02,  3.0215e-02,  5.1501e-02,\n",
       "             1.0558e-02,  5.4284e-02,  1.5251e-02,  1.3695e-01, -1.1235e-01,\n",
       "             2.8968e-02,  1.2124e-01, -9.4781e-02,  5.7386e-03, -2.3276e-02,\n",
       "            -9.3913e-02,  3.2728e-02, -2.1533e-02, -5.7571e-02, -3.5437e-02],\n",
       "           [ 7.4561e-02, -4.4901e-02, -6.6015e-02, -5.1989e-02,  4.5485e-03,\n",
       "            -6.9075e-02, -5.9513e-03, -2.4185e-02, -7.0253e-02,  8.9664e-02,\n",
       "            -5.4352e-02,  6.4839e-02,  6.3328e-02, -6.4327e-02,  4.8071e-02,\n",
       "             2.1124e-02,  2.6108e-01, -2.0269e-02, -5.4297e-02,  1.1750e-01,\n",
       "             5.8916e-02, -9.0439e-02,  1.5722e-02, -8.7547e-02,  2.6459e-02,\n",
       "            -5.5826e-02,  7.8250e-02,  8.0679e-03, -2.7534e-02,  2.5248e-02,\n",
       "             7.9535e-02,  5.4188e-02,  1.2401e-01, -1.1074e-01,  2.2923e-02,\n",
       "            -3.4922e-02, -2.1747e-02,  8.3767e-02,  1.8716e-02,  4.3680e-02,\n",
       "            -5.6174e-02, -5.4217e-02, -6.0129e-02,  8.7492e-02,  6.3932e-02,\n",
       "            -7.4017e-02, -1.8554e-02, -5.2167e-04,  7.1854e-02,  2.4283e-02,\n",
       "             3.1050e-02, -1.0550e-01, -1.8205e-02,  2.7875e-02,  6.3543e-02,\n",
       "            -4.9103e-02,  1.9647e-01, -1.1508e-01, -1.2559e-02,  2.0901e-02,\n",
       "             4.9238e-03, -5.3676e-02,  5.2760e-02, -1.1990e-02, -7.3388e-02,\n",
       "             2.5511e-02, -1.3494e-01,  1.5463e-02, -1.0055e-03, -2.3154e-02,\n",
       "            -9.5410e-02, -1.5427e-02,  1.3179e-02,  3.1416e-02, -8.9625e-02,\n",
       "            -1.3794e-01, -1.2001e-01,  7.0478e-02,  5.8667e-03, -1.9261e-02,\n",
       "             2.1607e-03,  6.5107e-02, -7.3400e-02,  6.0349e-02,  3.0165e-02,\n",
       "             2.7520e-02,  5.7916e-02, -4.7784e-02,  6.2071e-02, -1.5819e-01,\n",
       "             6.6928e-03, -1.2019e-02, -6.0393e-02, -2.2019e-02,  1.3541e-01,\n",
       "            -6.1032e-02,  2.0823e-02, -6.7708e-02, -8.2053e-02,  3.7787e-02]]],\n",
       "         grad_fn=<UnsafeViewBackward>),\n",
       "  'commands_lengths': tensor([[8]]),\n",
       "  'projected_keys_visual': tensor([[[-0.0048,  0.0596,  0.0038,  ...,  0.0327,  0.0746, -0.0445],\n",
       "           [-0.0476,  0.0287,  0.0239,  ...,  0.0526, -0.0035,  0.0468],\n",
       "           [-0.0754,  0.0297,  0.0057,  ..., -0.0021,  0.0650, -0.0226],\n",
       "           ...,\n",
       "           [-0.0582,  0.0180, -0.0266,  ..., -0.0459,  0.0657, -0.0842],\n",
       "           [-0.0612,  0.0104,  0.0162,  ..., -0.0075,  0.0212, -0.0442],\n",
       "           [-0.0544,  0.0225, -0.0732,  ..., -0.0475,  0.0272, -0.0456]]],\n",
       "         grad_fn=<UnsafeViewBackward>),\n",
       "  'seq_lengths': tensor([[7]])},\n",
       " {'hidden': (tensor([[[-0.2962,  0.0179,  0.1611,  0.1112, -0.0442, -0.0063, -0.0096,\n",
       "             -0.2785, -0.1047, -0.1820, -0.1892,  0.4060, -0.2105, -0.0801,\n",
       "              0.0608,  0.0713,  0.0100, -0.2094,  0.1748, -0.0079, -0.0533,\n",
       "             -0.0752, -0.0411,  0.1037, -0.1286,  0.2651, -0.0921, -0.0771,\n",
       "             -0.2773, -0.0085,  0.0422,  0.1823,  0.1543, -0.0818,  0.1756,\n",
       "              0.0497, -0.0006, -0.0355, -0.0874,  0.1066, -0.1118,  0.1275,\n",
       "             -0.0376,  0.1158,  0.1282,  0.0487, -0.2319,  0.0459,  0.0612,\n",
       "              0.0220,  0.0286, -0.2256,  0.0725,  0.0144, -0.1163,  0.0553,\n",
       "             -0.1440,  0.0676,  0.0157, -0.1500,  0.0083,  0.1691,  0.0354,\n",
       "              0.2895, -0.0582,  0.0167, -0.1927, -0.1593, -0.0764, -0.1769,\n",
       "              0.1613, -0.1040, -0.1459,  0.0860, -0.0101, -0.1241,  0.0081,\n",
       "              0.1057,  0.0315,  0.0180,  0.1154, -0.0386,  0.2069,  0.2130,\n",
       "             -0.0277, -0.0245,  0.0416,  0.2254,  0.1865, -0.0243, -0.0635,\n",
       "              0.2376, -0.0554,  0.0435,  0.0565, -0.1213, -0.1449, -0.1046,\n",
       "             -0.1206,  0.0106]]], grad_fn=<TransposeBackward0>),\n",
       "   tensor([[[-0.5174,  0.0302,  0.3361,  0.2586, -0.0786, -0.0094, -0.0164,\n",
       "             -0.5994, -0.2001, -0.4352, -0.2656,  0.7142, -0.4547, -0.1414,\n",
       "              0.0973,  0.1840,  0.0160, -0.3866,  0.2390, -0.0124, -0.1469,\n",
       "             -0.1758, -0.1481,  0.2871, -0.2345,  0.3727, -0.1782, -0.1194,\n",
       "             -0.3965, -0.0155,  0.0852,  0.3718,  0.3328, -0.1566,  0.3042,\n",
       "              0.1078, -0.0020, -0.1091, -0.2083,  0.1503, -0.1876,  0.3842,\n",
       "             -0.0527,  0.3191,  0.3885,  0.0830, -0.3455,  0.1560,  0.1322,\n",
       "              0.0557,  0.0674, -0.4220,  0.0917,  0.0219, -0.1771,  0.1102,\n",
       "             -0.3277,  0.1833,  0.0260, -0.3045,  0.0154,  0.3008,  0.0787,\n",
       "              0.5882, -0.1349,  0.0334, -0.6687, -0.2509, -0.1436, -0.3096,\n",
       "              0.2770, -0.1402, -0.2789,  0.2514, -0.0212, -0.2548,  0.0105,\n",
       "              0.1597,  0.1161,  0.0430,  0.3671, -0.1002,  0.4186,  0.4275,\n",
       "             -0.0937, -0.0557,  0.2027,  0.4432,  0.2742, -0.0591, -0.1057,\n",
       "              0.3821, -0.1307,  0.0906,  0.0894, -0.2521, -0.3046, -0.2052,\n",
       "             -0.2639,  0.0190]]], grad_fn=<TransposeBackward0>))})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.intervene_node(\"lstm_step_1\", low_interv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-b7a3b9e77343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"lstm_step_2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      GraphInput({\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m\"lstm_step_1\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mlow2_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m      })          \n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph.py\u001b[0m in \u001b[0;36mcompute_node\u001b[0;34m(self, node_name, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Invalid key {node_name}. Valid keys are {self.nodes.keys()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mcompute_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintervention\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mintervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mcompute_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintervention\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mintervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mcompute_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintervention\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mintervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mcompute_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintervention\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mintervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mcompute_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mintervention\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mintervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/antra/antra/graph_node.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, inputs, res_dict)\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mchild_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0mchildren_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mchildren_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;31m# Do intervention on the results from forward(), if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/ReaSCAN-Causal/codes/models/seq2seq/decode_graphical_models.py\u001b[0m in \u001b[0;36mcommand_input_preparation\u001b[0;34m(commands_input, commands_lengths)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# We may not need the following fields. But we leave it here in case we need these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# to generate other inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"commands_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"commands_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "G.compute_node(\n",
    "    \"lstm_step_2\", \n",
    "     GraphInput({\n",
    "        \"lstm_step_1\":  low2_hidden\n",
    "     })          \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return_lstm_output': [tensor([[[ 0.1083,  0.3289, -0.3572,  0.1932,  0.0509,  0.2854, -0.1446,\n",
       "             0.1894]]], grad_fn=<UnsqueezeBackward0>),\n",
       "  tensor([[[ 0.4512, -0.1370, -0.0352,  0.1147, -0.0538, -0.4568, -0.1355,\n",
       "            -0.0396]]], grad_fn=<UnsqueezeBackward0>)],\n",
       " 'return_attention_weights': [tensor([[[0.0280, 0.0275, 0.0274, 0.0281, 0.0285, 0.0277, 0.0273, 0.0281,\n",
       "            0.0282, 0.0272, 0.0273, 0.0276, 0.0273, 0.0278, 0.0277, 0.0290,\n",
       "            0.0284, 0.0279, 0.0279, 0.0277, 0.0290, 0.0275, 0.0267, 0.0270,\n",
       "            0.0278, 0.0282, 0.0272, 0.0273, 0.0274, 0.0289, 0.0282, 0.0273,\n",
       "            0.0283, 0.0284, 0.0266, 0.0280]]], grad_fn=<UnsqueezeBackward0>),\n",
       "  tensor([[[0.0280, 0.0275, 0.0274, 0.0281, 0.0285, 0.0277, 0.0273, 0.0281,\n",
       "            0.0282, 0.0272, 0.0273, 0.0276, 0.0273, 0.0278, 0.0277, 0.0290,\n",
       "            0.0284, 0.0279, 0.0279, 0.0277, 0.0290, 0.0275, 0.0267, 0.0270,\n",
       "            0.0278, 0.0282, 0.0272, 0.0273, 0.0274, 0.0289, 0.0282, 0.0273,\n",
       "            0.0283, 0.0284, 0.0266, 0.0280]]], grad_fn=<UnsqueezeBackward0>)],\n",
       " 'hidden': (tensor([[[-0.2962,  0.0179,  0.1611,  0.1112, -0.0442, -0.0063, -0.0096,\n",
       "            -0.2785, -0.1047, -0.1820, -0.1892,  0.4060, -0.2105, -0.0801,\n",
       "             0.0608,  0.0713,  0.0100, -0.2094,  0.1748, -0.0079, -0.0533,\n",
       "            -0.0752, -0.0411,  0.1037, -0.1286,  0.2651, -0.0921, -0.0771,\n",
       "            -0.2773, -0.0085,  0.0422,  0.1823,  0.1543, -0.0818,  0.1756,\n",
       "             0.0497, -0.0006, -0.0355, -0.0874,  0.1066, -0.1118,  0.1275,\n",
       "            -0.0376,  0.1158,  0.1282,  0.0487, -0.2319,  0.0459,  0.0612,\n",
       "             0.0220,  0.0286, -0.2256,  0.0725,  0.0144, -0.1163,  0.0553,\n",
       "            -0.1440,  0.0676,  0.0157, -0.1500,  0.0083,  0.1691,  0.0354,\n",
       "             0.2895, -0.0582,  0.0167, -0.1927, -0.1593, -0.0764, -0.1769,\n",
       "             0.1613, -0.1040, -0.1459,  0.0860, -0.0101, -0.1241,  0.0081,\n",
       "             0.1057,  0.0315,  0.0180,  0.1154, -0.0386,  0.2069,  0.2130,\n",
       "            -0.0277, -0.0245,  0.0416,  0.2254,  0.1865, -0.0243, -0.0635,\n",
       "             0.2376, -0.0554,  0.0435,  0.0565, -0.1213, -0.1449, -0.1046,\n",
       "            -0.1206,  0.0106]]], grad_fn=<TransposeBackward0>),\n",
       "  tensor([[[-0.5174,  0.0302,  0.3361,  0.2586, -0.0786, -0.0094, -0.0164,\n",
       "            -0.5994, -0.2001, -0.4352, -0.2656,  0.7142, -0.4547, -0.1414,\n",
       "             0.0973,  0.1840,  0.0160, -0.3866,  0.2390, -0.0124, -0.1469,\n",
       "            -0.1758, -0.1481,  0.2871, -0.2345,  0.3727, -0.1782, -0.1194,\n",
       "            -0.3965, -0.0155,  0.0852,  0.3718,  0.3328, -0.1566,  0.3042,\n",
       "             0.1078, -0.0020, -0.1091, -0.2083,  0.1503, -0.1876,  0.3842,\n",
       "            -0.0527,  0.3191,  0.3885,  0.0830, -0.3455,  0.1560,  0.1322,\n",
       "             0.0557,  0.0674, -0.4220,  0.0917,  0.0219, -0.1771,  0.1102,\n",
       "            -0.3277,  0.1833,  0.0260, -0.3045,  0.0154,  0.3008,  0.0787,\n",
       "             0.5882, -0.1349,  0.0334, -0.6687, -0.2509, -0.1436, -0.3096,\n",
       "             0.2770, -0.1402, -0.2789,  0.2514, -0.0212, -0.2548,  0.0105,\n",
       "             0.1597,  0.1161,  0.0430,  0.3671, -0.1002,  0.4186,  0.4275,\n",
       "            -0.0937, -0.0557,  0.2027,  0.4432,  0.2742, -0.0591, -0.1057,\n",
       "             0.3821, -0.1307,  0.0906,  0.0894, -0.2521, -0.3046, -0.2052,\n",
       "            -0.2639,  0.0190]]], grad_fn=<TransposeBackward0>)),\n",
       " 'input_tokens_sorted': tensor([[1, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 2]]),\n",
       " 'projected_keys_textual': tensor([[[ 4.7460e-02,  8.5277e-02, -9.4986e-02,  7.0504e-02, -6.4376e-02,\n",
       "           -4.8510e-02, -2.5552e-02,  3.8378e-02, -1.3505e-01, -9.4378e-05,\n",
       "           -1.1899e-02, -4.3824e-02,  8.8509e-02, -1.2764e-02, -7.9371e-02,\n",
       "           -5.6533e-04,  1.2283e-01, -4.4914e-02, -5.5439e-02,  1.1354e-01,\n",
       "            6.0451e-02, -1.1641e-01, -5.3944e-02, -6.2068e-03,  1.5395e-01,\n",
       "            1.4703e-02,  6.7290e-02, -4.1493e-03, -1.0215e-02,  8.2744e-03,\n",
       "            8.0950e-02,  1.6959e-02,  1.8093e-02, -5.4427e-02, -5.6345e-03,\n",
       "           -1.3236e-01,  3.4416e-02,  1.4945e-02, -5.6635e-03, -1.0648e-02,\n",
       "           -8.0301e-02, -8.0361e-02, -1.0489e-02, -3.9586e-03, -3.6036e-03,\n",
       "            2.9003e-02, -8.8163e-02, -1.7494e-02, -1.2132e-01, -1.1659e-02,\n",
       "            1.3971e-01, -5.0124e-02, -4.8936e-02,  8.6705e-02, -4.0693e-03,\n",
       "           -3.3868e-02, -1.5533e-02,  3.6177e-02,  2.6552e-02, -1.0255e-01,\n",
       "            2.7717e-02, -1.5652e-01, -6.1686e-02,  5.9268e-02, -8.0197e-03,\n",
       "           -2.7756e-02, -7.1741e-02, -3.5789e-02, -4.9152e-02, -7.3845e-02,\n",
       "           -2.2124e-03,  9.7916e-02,  3.9815e-02, -4.5376e-02, -5.8784e-02,\n",
       "           -1.7536e-02, -3.3865e-02,  1.3114e-01, -1.0281e-01, -2.5154e-04,\n",
       "            2.4449e-03,  3.0693e-03, -6.7002e-02, -1.5610e-02, -2.6793e-02,\n",
       "           -4.3472e-02,  1.4288e-02,  3.9555e-02,  5.6393e-02, -7.2569e-02,\n",
       "           -1.0969e-02,  8.5925e-02,  3.0651e-02,  4.2885e-02, -8.7499e-03,\n",
       "           -2.9760e-03,  8.6029e-02,  4.9900e-02, -5.6183e-02, -5.3163e-02],\n",
       "          [-5.1131e-02,  7.2843e-02,  1.1782e-01,  1.0282e-01, -2.5361e-02,\n",
       "           -8.5709e-02, -4.7266e-02,  1.0061e-01, -5.8152e-02,  2.6752e-03,\n",
       "           -1.3149e-01, -1.1998e-02,  1.4750e-01,  2.7714e-02,  1.4732e-02,\n",
       "           -7.4834e-02,  4.7676e-02, -8.6581e-03, -1.8700e-01,  7.9342e-02,\n",
       "           -2.9818e-02,  9.7108e-03,  5.7466e-03, -5.2767e-02,  5.2330e-02,\n",
       "           -1.2637e-02,  2.7650e-02,  3.2225e-02,  1.5347e-02, -2.9910e-02,\n",
       "            9.5774e-02,  1.0035e-01,  4.6744e-02, -8.3731e-02,  3.3850e-02,\n",
       "           -9.5089e-02,  4.1095e-02, -1.9426e-02,  3.2930e-02, -5.5443e-02,\n",
       "           -2.2201e-02, -9.9154e-02, -7.2614e-02,  1.4891e-03,  5.3565e-03,\n",
       "           -3.6563e-02, -9.0047e-02,  5.0900e-02,  2.4651e-02, -8.0326e-04,\n",
       "            1.3208e-01, -7.8697e-02, -3.6843e-02,  1.9311e-01, -1.1443e-01,\n",
       "            1.6508e-02, -4.6118e-02, -3.4801e-02,  9.9569e-02,  1.2777e-01,\n",
       "            3.0874e-02, -1.9130e-02, -4.1792e-02,  6.3593e-02,  5.5375e-02,\n",
       "            4.3933e-02, -6.5124e-02, -8.2066e-02, -5.4708e-02, -4.5416e-02,\n",
       "           -1.2438e-01,  1.2113e-01,  1.1149e-01, -6.6882e-02, -1.2581e-01,\n",
       "           -3.3420e-03, -1.3929e-01,  3.0838e-01, -4.4129e-02, -8.1191e-02,\n",
       "            3.6756e-02, -9.8851e-02, -3.3986e-02, -1.1701e-02,  1.7578e-02,\n",
       "           -9.0434e-02,  7.8400e-02,  7.0256e-02, -3.5653e-02, -2.1804e-02,\n",
       "           -1.0460e-01,  1.3548e-01,  1.0524e-02,  5.4955e-02, -2.0313e-02,\n",
       "           -9.9010e-02,  1.4775e-01,  5.5330e-02, -5.1441e-02, -1.2535e-01],\n",
       "          [-7.1749e-02,  2.9261e-02,  3.0692e-02,  1.0662e-01,  3.2708e-02,\n",
       "           -5.8314e-02,  1.0063e-02,  8.6071e-02, -9.8172e-02,  3.4661e-02,\n",
       "           -5.5773e-02, -1.2194e-02,  1.0236e-01,  1.2372e-03,  7.5321e-02,\n",
       "           -1.1914e-01, -5.2117e-02, -4.1283e-02, -1.2615e-01,  6.3999e-02,\n",
       "           -5.4736e-02,  1.6684e-02, -1.7370e-03, -2.4536e-02, -4.8959e-03,\n",
       "            6.3408e-02,  1.4311e-02, -2.5597e-03, -1.7924e-02, -1.3607e-02,\n",
       "            7.0014e-02,  9.4271e-02, -3.4306e-02, -8.2708e-02,  5.4088e-02,\n",
       "           -6.1782e-02,  5.1903e-02, -3.5434e-03, -3.9151e-02, -5.0081e-02,\n",
       "           -4.2355e-02, -9.7917e-02, -6.6543e-02, -3.8217e-02,  2.8807e-02,\n",
       "            1.0014e-01, -4.1692e-02, -2.5457e-03,  5.5867e-05, -7.7712e-02,\n",
       "           -1.4461e-04, -4.8585e-02,  4.1122e-03,  1.0507e-01, -4.5657e-02,\n",
       "            8.6304e-03, -3.9861e-02,  1.1403e-02,  9.0227e-02,  3.2678e-02,\n",
       "            5.8541e-03, -8.2354e-02, -7.4843e-02,  7.8196e-02,  7.0121e-03,\n",
       "           -2.3159e-02, -7.7607e-02, -5.6743e-02, -3.2915e-02, -5.4917e-02,\n",
       "           -1.2637e-01,  8.3718e-02, -3.1844e-02,  1.5213e-02, -1.5456e-01,\n",
       "            1.0402e-01, -4.1944e-02,  2.9933e-01, -1.0339e-01, -7.5630e-02,\n",
       "            9.2017e-02, -1.0202e-01,  2.1392e-02, -3.5003e-02, -4.4907e-02,\n",
       "           -7.5103e-02,  3.8052e-02,  8.9114e-02, -3.7847e-02, -4.3347e-02,\n",
       "           -3.8889e-02,  1.7948e-01,  2.5412e-02, -8.2306e-02, -2.3889e-02,\n",
       "           -6.1512e-02, -4.8974e-03,  5.2350e-02, -6.7753e-02, -1.6746e-01],\n",
       "          [ 5.5556e-02, -1.0426e-04,  1.5071e-02,  7.3226e-02, -6.6108e-02,\n",
       "           -8.1450e-02,  7.0043e-04,  1.3731e-02, -1.9850e-02,  4.9307e-02,\n",
       "           -4.8018e-02, -9.1174e-02,  3.7333e-02,  4.0853e-02,  1.0849e-02,\n",
       "           -8.8703e-02, -2.8565e-02, -5.9615e-02,  4.9483e-03,  6.0324e-02,\n",
       "            4.9696e-02,  8.2904e-02,  1.6528e-02, -2.7537e-02, -1.9562e-02,\n",
       "            9.7623e-03, -9.4718e-02, -3.1269e-02,  3.7276e-02, -6.7321e-03,\n",
       "           -1.8151e-02,  1.1711e-01, -6.1154e-02,  5.9132e-04, -6.4353e-02,\n",
       "            4.2530e-02,  7.2309e-02, -6.2162e-02, -2.5994e-02, -4.8582e-03,\n",
       "           -2.3492e-02,  1.5727e-02, -7.6271e-02, -5.4142e-02,  5.9609e-02,\n",
       "            9.5869e-02,  5.9863e-03,  4.9868e-03,  1.2799e-03, -8.0024e-02,\n",
       "            1.3794e-02,  1.8801e-02,  3.5053e-02,  6.4527e-02,  2.1154e-02,\n",
       "           -8.4928e-02, -1.6599e-02,  2.3871e-02,  7.3555e-02,  2.8226e-02,\n",
       "            6.4560e-02, -1.4741e-01, -1.3619e-01, -5.0165e-02,  4.4166e-02,\n",
       "            6.2052e-02, -4.3540e-02, -1.3824e-02, -5.7266e-02, -3.3444e-02,\n",
       "            6.9866e-03,  1.1727e-01, -2.0544e-02,  1.8752e-02, -5.9377e-03,\n",
       "            5.8086e-02, -9.7940e-02,  1.7221e-01, -5.1340e-02, -1.3418e-01,\n",
       "            4.9832e-02, -5.0734e-02, -5.0721e-03, -2.3101e-02, -1.3115e-01,\n",
       "            2.4663e-02,  1.1737e-01,  3.7756e-02, -9.6785e-03, -3.1473e-02,\n",
       "           -2.0900e-02, -3.8436e-02, -2.6599e-02, -1.1369e-01, -6.1035e-02,\n",
       "           -4.8196e-02,  3.5287e-03,  3.1111e-02, -5.8675e-02, -4.4188e-02],\n",
       "          [ 3.3498e-02, -9.6755e-02,  2.6076e-02,  3.5064e-02,  1.0572e-02,\n",
       "           -5.9544e-02, -7.1324e-02,  1.2474e-02, -2.8835e-03,  6.2324e-02,\n",
       "           -6.8777e-02, -1.2542e-02,  2.9977e-02,  8.6180e-02,  7.2821e-04,\n",
       "           -8.9666e-02,  1.4404e-01, -2.0707e-02,  2.6103e-02,  1.6059e-01,\n",
       "            9.3123e-02,  4.6286e-02,  4.6858e-02, -6.6827e-02,  1.6079e-02,\n",
       "           -5.7256e-02, -1.2681e-02, -3.9985e-02,  3.4257e-02,  4.7575e-02,\n",
       "            7.0290e-02,  2.7944e-03, -3.3482e-02,  5.6329e-02, -3.1650e-02,\n",
       "            3.7197e-02,  7.1486e-02, -7.3542e-02,  6.5911e-02, -3.7691e-02,\n",
       "           -8.4256e-04,  4.8624e-02, -1.6341e-01,  2.0188e-03,  1.7334e-02,\n",
       "            4.6636e-02, -5.9097e-02,  2.6386e-02, -4.3805e-03,  3.7176e-02,\n",
       "            4.4744e-02, -4.7913e-02, -5.9563e-02,  4.0939e-02,  1.4899e-04,\n",
       "           -3.2696e-02,  2.7511e-02, -6.6452e-02,  3.8092e-02,  1.0750e-02,\n",
       "            4.3177e-02, -8.0740e-02,  4.0241e-02,  2.8931e-03,  6.5522e-02,\n",
       "            9.1411e-03, -3.5308e-02,  1.5216e-02,  7.9893e-02, -7.1470e-02,\n",
       "           -8.3247e-03,  1.0458e-01,  7.1280e-02, -5.7346e-02, -3.2411e-02,\n",
       "           -6.6533e-02, -1.3565e-01,  1.3009e-01, -9.9928e-03, -5.4137e-02,\n",
       "           -2.7496e-02,  3.6660e-02,  8.2580e-03,  3.6921e-02, -5.7435e-02,\n",
       "            1.8154e-02,  7.3972e-02,  3.4874e-03,  1.0328e-01, -6.2285e-02,\n",
       "           -4.8282e-02, -5.9297e-02, -3.8582e-02, -3.5548e-02, -1.3294e-02,\n",
       "           -1.3683e-03,  5.4642e-02,  6.0412e-02, -3.4229e-02,  7.7969e-02],\n",
       "          [ 3.2697e-02, -4.0040e-02,  5.0802e-02,  1.4147e-02, -3.5988e-02,\n",
       "           -1.3374e-01, -4.8695e-02,  3.5449e-02, -1.0199e-01, -7.6902e-02,\n",
       "           -7.4898e-02, -6.6948e-02,  1.3258e-02,  6.7126e-02,  1.9089e-02,\n",
       "           -1.3704e-01,  7.0914e-02, -2.5406e-02, -3.7496e-02,  2.1336e-02,\n",
       "           -9.9669e-03, -7.3446e-02, -2.2905e-02, -1.0869e-01,  1.0291e-01,\n",
       "           -7.0882e-02,  2.0242e-02, -4.0210e-03, -2.3607e-02,  5.4725e-03,\n",
       "            1.7806e-01, -3.7153e-02, -4.0216e-02, -8.2188e-02,  2.5298e-02,\n",
       "            3.9624e-02, -8.6150e-03, -4.3215e-03,  7.8683e-02, -1.0192e-01,\n",
       "            2.1541e-02, -4.6118e-02, -1.4443e-01,  4.2716e-02, -7.9815e-02,\n",
       "           -1.4342e-02, -9.9636e-02, -8.5373e-02,  5.7513e-02,  3.2214e-02,\n",
       "            8.9994e-02, -1.2140e-01, -8.3592e-02,  8.5672e-02,  1.1418e-02,\n",
       "           -4.8292e-02,  9.4594e-02, -3.0605e-02,  4.0850e-02,  4.0365e-03,\n",
       "            1.3107e-02, -9.2535e-02,  7.4307e-03,  8.7999e-03,  1.5539e-01,\n",
       "           -5.4044e-02, -3.8017e-02, -1.9521e-02,  8.4911e-02, -1.2702e-01,\n",
       "           -2.5060e-02,  8.3484e-02,  7.8085e-02, -7.8983e-02, -1.3750e-01,\n",
       "           -9.3435e-02, -8.7978e-02,  2.3252e-01, -3.1771e-02, -1.5689e-01,\n",
       "            3.4060e-02,  4.9066e-02,  7.8876e-02,  8.6235e-02,  1.0550e-02,\n",
       "           -2.9121e-02,  1.1229e-01, -4.2790e-02,  1.5288e-01, -9.6772e-03,\n",
       "           -8.9245e-02,  6.5734e-02,  1.9387e-02, -4.5078e-02, -5.3174e-02,\n",
       "           -1.6077e-02,  7.2190e-02,  8.1317e-02, -2.4726e-02,  1.1641e-02],\n",
       "          [ 1.7292e-02,  1.7065e-02,  2.1621e-02,  8.5497e-03, -1.0553e-01,\n",
       "           -5.4947e-02,  2.1310e-02, -7.0492e-02, -3.3451e-02,  7.5866e-02,\n",
       "            5.1485e-02,  3.2510e-02,  4.6512e-02, -4.9105e-02, -2.4563e-02,\n",
       "           -8.7928e-02,  1.2218e-01, -5.7102e-02, -5.7950e-02,  4.1794e-02,\n",
       "            6.3900e-02, -5.2032e-02, -2.5242e-02, -6.4056e-02,  9.7671e-02,\n",
       "           -8.2224e-02, -1.1510e-02, -1.1201e-01, -7.9528e-02, -1.1501e-03,\n",
       "            1.3363e-01, -7.0310e-02,  7.9003e-02, -9.0152e-02,  1.1732e-01,\n",
       "            6.0044e-02, -1.9849e-02,  1.4336e-01,  1.3351e-02, -2.7847e-03,\n",
       "            8.4602e-02, -1.0403e-01,  6.9431e-02,  5.3466e-02, -7.5220e-02,\n",
       "           -2.3215e-02, -1.1241e-02,  1.5727e-02, -6.5190e-03, -4.2409e-02,\n",
       "            1.0818e-02, -1.1996e-01, -3.8482e-03,  2.4036e-02,  1.2332e-01,\n",
       "            1.5053e-02,  8.7920e-02,  6.9108e-02,  8.7300e-03,  7.0794e-02,\n",
       "           -2.6061e-02, -1.7689e-01, -2.9684e-02,  5.1999e-04, -3.2634e-02,\n",
       "            4.0304e-02,  3.3504e-03, -7.3749e-02, -8.2116e-02, -1.1550e-01,\n",
       "            3.7945e-03, -3.9883e-02,  5.2411e-02,  8.8799e-03, -6.9665e-02,\n",
       "           -5.4063e-02, -1.4635e-01,  2.9470e-03, -1.0738e-01, -5.9514e-02,\n",
       "            9.5726e-02, -4.3365e-03,  1.0184e-02,  3.0215e-02,  5.1501e-02,\n",
       "            1.0558e-02,  5.4284e-02,  1.5251e-02,  1.3695e-01, -1.1235e-01,\n",
       "            2.8968e-02,  1.2124e-01, -9.4781e-02,  5.7386e-03, -2.3276e-02,\n",
       "           -9.3913e-02,  3.2728e-02, -2.1533e-02, -5.7571e-02, -3.5437e-02],\n",
       "          [ 7.4561e-02, -4.4901e-02, -6.6015e-02, -5.1989e-02,  4.5485e-03,\n",
       "           -6.9075e-02, -5.9513e-03, -2.4185e-02, -7.0253e-02,  8.9664e-02,\n",
       "           -5.4352e-02,  6.4839e-02,  6.3328e-02, -6.4327e-02,  4.8071e-02,\n",
       "            2.1124e-02,  2.6108e-01, -2.0269e-02, -5.4297e-02,  1.1750e-01,\n",
       "            5.8916e-02, -9.0439e-02,  1.5722e-02, -8.7547e-02,  2.6459e-02,\n",
       "           -5.5826e-02,  7.8250e-02,  8.0679e-03, -2.7534e-02,  2.5248e-02,\n",
       "            7.9535e-02,  5.4188e-02,  1.2401e-01, -1.1074e-01,  2.2923e-02,\n",
       "           -3.4922e-02, -2.1747e-02,  8.3767e-02,  1.8716e-02,  4.3680e-02,\n",
       "           -5.6174e-02, -5.4217e-02, -6.0129e-02,  8.7492e-02,  6.3932e-02,\n",
       "           -7.4017e-02, -1.8554e-02, -5.2167e-04,  7.1854e-02,  2.4283e-02,\n",
       "            3.1050e-02, -1.0550e-01, -1.8205e-02,  2.7875e-02,  6.3543e-02,\n",
       "           -4.9103e-02,  1.9647e-01, -1.1508e-01, -1.2559e-02,  2.0901e-02,\n",
       "            4.9238e-03, -5.3676e-02,  5.2760e-02, -1.1990e-02, -7.3388e-02,\n",
       "            2.5511e-02, -1.3494e-01,  1.5463e-02, -1.0055e-03, -2.3154e-02,\n",
       "           -9.5410e-02, -1.5427e-02,  1.3179e-02,  3.1416e-02, -8.9625e-02,\n",
       "           -1.3794e-01, -1.2001e-01,  7.0478e-02,  5.8667e-03, -1.9261e-02,\n",
       "            2.1607e-03,  6.5107e-02, -7.3400e-02,  6.0349e-02,  3.0165e-02,\n",
       "            2.7520e-02,  5.7916e-02, -4.7784e-02,  6.2071e-02, -1.5819e-01,\n",
       "            6.6928e-03, -1.2019e-02, -6.0393e-02, -2.2019e-02,  1.3541e-01,\n",
       "           -6.1032e-02,  2.0823e-02, -6.7708e-02, -8.2053e-02,  3.7787e-02]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " 'commands_lengths': tensor([[8]]),\n",
       " 'projected_keys_visual': tensor([[[-0.0594,  0.0170, -0.0078,  ..., -0.0135,  0.0245, -0.0443],\n",
       "          [-0.0774,  0.0461,  0.0339,  ..., -0.0269,  0.0487, -0.0519],\n",
       "          [-0.0355,  0.0714,  0.0072,  ...,  0.0072,  0.1082, -0.0278],\n",
       "          ...,\n",
       "          [-0.0270,  0.0529,  0.0015,  ...,  0.0234,  0.0199, -0.0350],\n",
       "          [-0.0282,  0.0347, -0.0640,  ..., -0.0849,  0.1024, -0.0714],\n",
       "          [-0.0557,  0.0685, -0.0176,  ..., -0.0119, -0.0104, -0.1000]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " 'seq_lengths': tensor([[13]])}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low2_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"commands_input\": input_batch, \n",
    "    \"commands_lengths\": input_lengths,\n",
    "    \"situations_input\": situation_batch,\n",
    "    \"target_batch\": target_batch,\n",
    "    \"target_lengths\": target_lengths,\n",
    "}\n",
    "all_in = GraphInput(input_dict, batched=True, batch_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command_hidden': tensor([[-0.0716,  0.0218, -0.0727, -0.1464,  0.2586,  0.0150,  0.1276, -0.1273,\n",
       "           0.1017, -0.1471,  0.0721,  0.0877, -0.0730,  0.0089,  0.0756,  0.0308,\n",
       "           0.0855,  0.1926, -0.0424, -0.1425, -0.0747,  0.2126,  0.1662, -0.0599,\n",
       "          -0.1097, -0.0167, -0.0249,  0.1125,  0.1209, -0.0604,  0.1541, -0.1178,\n",
       "          -0.2140,  0.2301, -0.1050, -0.1421, -0.0394,  0.0096,  0.1389, -0.0672,\n",
       "           0.1375,  0.1194,  0.0805,  0.1265, -0.2354,  0.2100,  0.0325, -0.2087,\n",
       "          -0.1465, -0.0817,  0.1489,  0.1670, -0.2006, -0.1258,  0.1037, -0.1835,\n",
       "          -0.0210,  0.0387, -0.0457,  0.0012, -0.1044, -0.0470,  0.0509,  0.1522,\n",
       "           0.0645, -0.1165,  0.1050, -0.0331, -0.0771,  0.2220, -0.0576,  0.1506,\n",
       "           0.0650,  0.0381, -0.0998, -0.0811,  0.0228, -0.0296,  0.0171, -0.0422,\n",
       "           0.0539, -0.0739, -0.0856, -0.0870, -0.0091, -0.0355, -0.2654, -0.0285,\n",
       "          -0.1823,  0.0548,  0.1766, -0.0890,  0.3243, -0.0289, -0.1823,  0.1486,\n",
       "          -0.0326,  0.0592,  0.2273,  0.1531],\n",
       "         [-0.0716,  0.0218, -0.0727, -0.1464,  0.2586,  0.0150,  0.1276, -0.1273,\n",
       "           0.1017, -0.1471,  0.0721,  0.0877, -0.0730,  0.0089,  0.0756,  0.0308,\n",
       "           0.0855,  0.1926, -0.0424, -0.1425, -0.0747,  0.2126,  0.1662, -0.0599,\n",
       "          -0.1097, -0.0167, -0.0249,  0.1125,  0.1209, -0.0604,  0.1541, -0.1178,\n",
       "          -0.2140,  0.2301, -0.1050, -0.1421, -0.0394,  0.0096,  0.1389, -0.0672,\n",
       "           0.1375,  0.1194,  0.0805,  0.1265, -0.2354,  0.2100,  0.0325, -0.2087,\n",
       "          -0.1465, -0.0817,  0.1489,  0.1670, -0.2006, -0.1258,  0.1037, -0.1835,\n",
       "          -0.0210,  0.0387, -0.0457,  0.0012, -0.1044, -0.0470,  0.0509,  0.1522,\n",
       "           0.0645, -0.1165,  0.1050, -0.0331, -0.0771,  0.2220, -0.0576,  0.1506,\n",
       "           0.0650,  0.0381, -0.0998, -0.0811,  0.0228, -0.0296,  0.0171, -0.0422,\n",
       "           0.0539, -0.0739, -0.0856, -0.0870, -0.0091, -0.0355, -0.2654, -0.0285,\n",
       "          -0.1823,  0.0548,  0.1766, -0.0890,  0.3243, -0.0289, -0.1823,  0.1486,\n",
       "          -0.0326,  0.0592,  0.2273,  0.1531]], grad_fn=<SumBackward1>),\n",
       " 'command_encoder_outputs': tensor([[[ 0.0217, -0.1899, -0.0163,  ...,  0.1107,  0.2261,  0.2767],\n",
       "          [-0.0687, -0.1485,  0.1923,  ...,  0.1537,  0.2974,  0.1147],\n",
       "          [-0.0558, -0.1607,  0.1911,  ...,  0.0427,  0.2051,  0.0765],\n",
       "          ...,\n",
       "          [-0.0365, -0.0540, -0.0825,  ...,  0.0496,  0.2192,  0.0141],\n",
       "          [-0.1179, -0.0930, -0.0556,  ...,  0.1111,  0.2590, -0.1440],\n",
       "          [-0.0886,  0.2024, -0.0799,  ..., -0.0012,  0.1221, -0.0355]],\n",
       " \n",
       "         [[ 0.0217, -0.1899, -0.0163,  ...,  0.1107,  0.2261,  0.2767],\n",
       "          [-0.0687, -0.1485,  0.1923,  ...,  0.1537,  0.2974,  0.1147],\n",
       "          [-0.0558, -0.1607,  0.1911,  ...,  0.0427,  0.2051,  0.0765],\n",
       "          ...,\n",
       "          [-0.0365, -0.0540, -0.0825,  ...,  0.0496,  0.2192,  0.0141],\n",
       "          [-0.1179, -0.0930, -0.0556,  ...,  0.1111,  0.2590, -0.1440],\n",
       "          [-0.0886,  0.2024, -0.0799,  ..., -0.0012,  0.1221, -0.0355]]],\n",
       "        grad_fn=<SumBackward1>),\n",
       " 'command_sequence_lengths': array([8., 8.])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.compute_node(\n",
    "    \"command_input_encode\", \n",
    "    all_in\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up training loop for this model in antra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
