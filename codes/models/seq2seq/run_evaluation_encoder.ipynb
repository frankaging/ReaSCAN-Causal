{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(tqdm(data_iterator)):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths, \\\n",
    "            dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "            dual_agent_positions, dual_target_positions, \\\n",
    "            dual_input_lengths, dual_target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        \n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        commands_embedding = model(\n",
    "            commands_input=input_sequence, \n",
    "            tag=\"command_input_encode_embedding\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_embedding=commands_embedding, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict_with_embedding\"\n",
    "        )\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        attention_weights_commands = []\n",
    "        attention_weights_situations = []\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "\n",
    "        yield (\n",
    "            input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target,\n",
    "        )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_sentence(sentence_array, vocab):\n",
    "    return [vocab.itos[word_idx] for word_idx in sentence_array]\n",
    "\n",
    "def predict_and_save(\n",
    "    dataset: ReaSCANDataset, \n",
    "    model: nn.Module, \n",
    "    output_file_path: str, \n",
    "    max_decoding_steps: int,\n",
    "    device,\n",
    "    max_testing_examples=None, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict all data in dataset with a model and write the predictions to output_file_path.\n",
    "    :param dataset: a dataset with test examples\n",
    "    :param model: a trained model from model.py\n",
    "    :param output_file_path: a path where a .json file with predictions will be saved.\n",
    "    :param max_decoding_steps: after how many steps to force quit decoding\n",
    "    :param max_testing_examples: after how many examples to stop predicting, if None all examples will be evaluated\n",
    "    \"\"\"\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    # read-in datasets\n",
    "    test_data, _ = dataset.get_dual_dataset()\n",
    "    data_iterator = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    eval_max_decoding_steps = max_decoding_steps\n",
    "    with open(output_file_path, mode='w') as outfile:\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #################\n",
    "            #\n",
    "            # Task-based\n",
    "            #\n",
    "            #################\n",
    "            exact_match_count = 0\n",
    "            example_count = 0\n",
    "            for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "                    data_iterator=data_iterator, model=model,\n",
    "                    max_decoding_steps=max_decoding_steps, \n",
    "                    pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                    sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                    eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                    max_examples_to_evaluate=max_testing_examples, \n",
    "                    device=device,\n",
    "            ):\n",
    "                example_count += 1\n",
    "                accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                if accuracy == 100:\n",
    "                    exact_match_count += 1\n",
    "                output.append({\"input\": input_str_sequence, \"prediction\": output_str_sequence,\n",
    "                               \"target\": target_str_sequence,\n",
    "                               \"accuracy\": accuracy,\n",
    "                               \"exact_match\": True if accuracy == 100 else False,\n",
    "                              })      \n",
    "            exact_match = (exact_match_count/example_count)*100.0\n",
    "            logger.info(\" Task Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "            logger.info(\"Wrote predictions for {} examples.\".format(example_count))\n",
    "            json.dump(output, outfile, indent=4)\n",
    "            return output_file_path\n",
    "        \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    random.seed(flags[\"seed\"])\n",
    "    torch.manual_seed(flags[\"seed\"])\n",
    "    np.random.seed(flags[\"seed\"])\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        assert False # we don't allow train in the evaluation script.\n",
    "    elif flags[\"mode\"] == \"test\":\n",
    "        logger.info(\"Loading all data into memory for evaluation...\")\n",
    "        logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "        data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "        assert os.path.exists(os.path.join(flags[\"data_directory\"], flags[\"input_vocab_path\"])) and os.path.exists(\n",
    "            os.path.join(flags[\"data_directory\"], flags[\"target_vocab_path\"])), \\\n",
    "            \"No vocabs found at {} and {}\".format(flags[\"input_vocab_path\"], flags[\"target_vocab_path\"])\n",
    "        splits = flags[\"splits\"].split(\",\")\n",
    "        for split in splits:\n",
    "            logger.info(\"Loading {} dataset split...\".format(split))\n",
    "            \n",
    "            test_set = ReaSCANDataset(\n",
    "                data_json, flags[\"data_directory\"], split=split,\n",
    "                input_vocabulary_file=flags[\"input_vocab_path\"],\n",
    "                target_vocabulary_file=flags[\"target_vocab_path\"],\n",
    "                generate_vocabulary=False, k=flags[\"k\"]\n",
    "            )\n",
    "            test_set.read_dataset(\n",
    "                max_examples=flags[\"max_testing_examples\"],\n",
    "                simple_situation_representation=flags[\"simple_situation_representation\"]\n",
    "            )\n",
    "            logger.info(\"Done Loading {} dataset split.\".format(flags[\"split\"]))\n",
    "            logger.info(\"  Loaded {} examples.\".format(test_set.num_examples))\n",
    "            logger.info(\"  Input vocabulary size: {}\".format(test_set.input_vocabulary_size))\n",
    "            logger.info(\"  Most common input words: {}\".format(test_set.input_vocabulary.most_common(5)))\n",
    "            logger.info(\"  Output vocabulary size: {}\".format(test_set.target_vocabulary_size))\n",
    "            logger.info(\"  Most common target words: {}\".format(test_set.target_vocabulary.most_common(5)))\n",
    "            \n",
    "            grid_size = test_set.grid_size\n",
    "            target_position_size = 2*grid_size - 1\n",
    "            \n",
    "            # create modell based on our dataset.\n",
    "            model = Model(input_vocabulary_size=test_set.input_vocabulary_size,\n",
    "                          target_vocabulary_size=test_set.target_vocabulary_size,\n",
    "                          num_cnn_channels=test_set.image_channels,\n",
    "                          input_padding_idx=test_set.input_vocabulary.pad_idx,\n",
    "                          target_pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                          target_eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                          target_position_size=target_position_size,\n",
    "                          **flags)\n",
    "\n",
    "            # gpu setups\n",
    "            use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            # logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            model.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            We have two low model so that our computation is much faster.\n",
    "            \"\"\"\n",
    "            eval_max_decoding_steps = flags[\"max_decoding_steps\"] # we need to use this extended step to measure for eval.\n",
    "            \n",
    "            # Load model and vocabularies if resuming.\n",
    "            evaluate_checkpoint = flags[\"evaluate_checkpoint\"]\n",
    "            if flags[\"evaluate_checkpoint\"] == \"\":\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], \"model_best.pth.tar\")\n",
    "            else:\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], f\"checkpoint-{evaluate_checkpoint}.pth.tar\")\n",
    "            assert os.path.isfile(model_path), \"No checkpoint found at {}\".format(model_path)\n",
    "            logger.info(\"Loading checkpoint from file at '{}'\".format(model_path))\n",
    "            model.load_model(device, model_path, strict=False)\n",
    "            start_iteration = model.trained_iterations\n",
    "            logger.info(\"Loaded checkpoint '{}' (iter {})\".format(model_path, start_iteration))\n",
    "            output_file_name = \"_\".join([split, flags[\"output_file_name\"]])\n",
    "            output_file_path = os.path.join(flags[\"resume_from_file\"], output_file_name)\n",
    "            logger.info(\"All results will be saved to '{}'\".format(output_file_path))\n",
    "            \n",
    "            output_file = predict_and_save(\n",
    "                dataset=test_set, \n",
    "                model=model,\n",
    "                output_file_path=output_file_path, \n",
    "                device=device,\n",
    "                **flags\n",
    "            )\n",
    "            logger.info(\"Saved predictions to {}\".format(output_file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
