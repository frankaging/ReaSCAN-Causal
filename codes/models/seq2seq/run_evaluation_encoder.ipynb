{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from seq2seq.model import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    hi_model,\n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device,\n",
    "    intervene_time,\n",
    "    intervene_dimension_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(tqdm(data_iterator)):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "        agent_positions, target_positions, \\\n",
    "        input_lengths, target_lengths, \\\n",
    "        dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "        dual_agent_positions, dual_target_positions, \\\n",
    "        dual_input_lengths, dual_target_lengths, \\\n",
    "        intervened_main_swap_index, intervened_dual_swap_index, \\\n",
    "        intervened_main_shape_index,intervened_dual_shape_index, \\\n",
    "        intervened_target_batch, intervened_swap_attr, \\\n",
    "        intervened_target_lengths_batch, size_class, color_class, shape_class = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        \n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        attention_weights_commands = []\n",
    "        attention_weights_situations = []\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            \n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "\n",
    "        yield (\n",
    "            input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target\n",
    "        )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterfactual_predict(\n",
    "    data_iterator, \n",
    "    model, hi_model,\n",
    "    max_decoding_steps, \n",
    "    eval_max_decoding_steps,\n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device,\n",
    "    intervene_attribute=-1,\n",
    "    intervene_time=-1,\n",
    "    intervene_dimension_size=25,\n",
    "    intervene_position=\"last_hidden\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    total_count = 0\n",
    "    bad_count = 0\n",
    "    \n",
    "    random_attr = False\n",
    "    random_time = False\n",
    "    if intervene_attribute == -1:\n",
    "        random_attr = True\n",
    "    if intervene_time == -1:\n",
    "        random_time = True\n",
    "    for step, batch in enumerate(tqdm(data_iterator)):\n",
    "        \n",
    "        total_count += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if total_count > max_examples_to_evaluate:\n",
    "                break\n",
    "\n",
    "        # main batch\n",
    "        input_batch, target_batch, situation_batch, \\\n",
    "            agent_positions_batch, target_positions_batch, \\\n",
    "            input_lengths_batch, target_lengths_batch, \\\n",
    "            dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "            dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "            dual_input_lengths_batch, dual_target_lengths_batch, \\\n",
    "            intervened_main_swap_index, intervened_dual_swap_index, \\\n",
    "            intervened_main_shape_index,intervened_dual_shape_index, \\\n",
    "            intervened_target_batch, intervened_swap_attr, \\\n",
    "            intervened_target_lengths_batch, size_class, color_class, shape_class = batch\n",
    "\n",
    "        \n",
    "        assert input_batch.size(0) == 1\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        \n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        situation_batch = situation_batch.to(device)\n",
    "        agent_positions_batch = agent_positions_batch.to(device)\n",
    "        target_positions_batch = target_positions_batch.to(device)\n",
    "        input_lengths_batch = input_lengths_batch.to(device)\n",
    "        target_lengths_batch = target_lengths_batch.to(device)\n",
    "\n",
    "        dual_input_max_seq_lens = max(dual_input_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        dual_input_batch = dual_input_batch.to(device)\n",
    "        dual_target_batch = dual_target_batch.to(device)\n",
    "        dual_situation_batch = dual_situation_batch.to(device)\n",
    "        dual_agent_positions_batch = dual_agent_positions_batch.to(device)\n",
    "        dual_target_positions_batch = dual_target_positions_batch.to(device)\n",
    "        dual_input_lengths_batch = dual_input_lengths_batch.to(device)\n",
    "        dual_target_lengths_batch = dual_target_lengths_batch.to(device)\n",
    "        # intervention data for novel attribute.\n",
    "        intervened_main_swap_index = intervened_main_swap_index.to(device)\n",
    "        intervened_dual_swap_index = intervened_dual_swap_index.to(device)\n",
    "        intervened_main_shape_index = intervened_dual_swap_index.to(device)\n",
    "        intervened_dual_shape_index = intervened_dual_shape_index.to(device)\n",
    "        intervened_target_batch = intervened_target_batch.to(device)\n",
    "        intervened_swap_attr = intervened_swap_attr.to(device)\n",
    "        intervened_target_lengths_batch = intervened_target_lengths_batch.to(device)\n",
    "        size_class = size_class.to(device)\n",
    "        color_class = color_class.to(device)\n",
    "        shape_class = shape_class.to(device)\n",
    "            \n",
    "        # Step 1: using a for loop to get the hidden states from the dual data\n",
    "        input_batch = input_batch[:,:input_max_seq_lens]\n",
    "        target_batch = target_batch[:,:target_max_seq_lens]\n",
    "        dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "        \n",
    "        # what to intervene\n",
    "        \"\"\"\n",
    "        For the sake of quick training, for a single batch,\n",
    "        we select the same attribute to intervenen on:\n",
    "        0: x\n",
    "        1: y\n",
    "        2: orientation\n",
    "        \"\"\"\n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        min_len = min(target_max_seq_lens, dual_target_max_seq_lens)\n",
    "        \n",
    "        batch_size = input_batch.size(0)\n",
    "        # calculate cf loss.\n",
    "        # just like other cf, we need to first filter out some examples.\n",
    "        idx_selected = []\n",
    "        for i in range(batch_size):\n",
    "            if intervened_main_swap_index[i] != -1:\n",
    "                idx_selected += [i]\n",
    "            else:\n",
    "                assert intervened_target_lengths_batch[i] == 0 # this is a validation.\n",
    "        if len(idx_selected) > 0:\n",
    "            situation_batch = situation_batch[idx_selected]\n",
    "            input_batch = input_batch[idx_selected]\n",
    "            input_lengths_batch = input_lengths_batch[idx_selected]\n",
    "            dual_situation_batch = dual_situation_batch[idx_selected]\n",
    "            dual_input_batch = dual_input_batch[idx_selected]\n",
    "            dual_input_lengths_batch = dual_input_lengths_batch[idx_selected]\n",
    "            dual_target_batch = dual_target_batch[idx_selected]\n",
    "            intervened_target_lengths_batch = intervened_target_lengths_batch[idx_selected]\n",
    "            intervened_target_batch = intervened_target_batch[idx_selected]\n",
    "            intervened_main_swap_index = intervened_main_swap_index[idx_selected]\n",
    "            intervened_dual_swap_index = intervened_dual_swap_index[idx_selected]\n",
    "            intervened_main_shape_index = intervened_main_shape_index[idx_selected]\n",
    "            intervened_dual_shape_index = intervened_dual_shape_index[idx_selected]\n",
    "            intervened_swap_attr = intervened_swap_attr[idx_selected]\n",
    "        \n",
    "            if intervene_position == \"embedding\":\n",
    "                assert False # not supporting it for this version.\n",
    "            elif intervene_position == \"hidden\":\n",
    "                assert False # not supporting it for this version.\n",
    "            elif intervene_position == \"last_hidden\":\n",
    "                commands_embedding = model(\n",
    "                    commands_input=input_batch, \n",
    "                    tag=\"command_input_encode_embedding\"\n",
    "                )\n",
    "                dual_commands_embedding = model(\n",
    "                    commands_input=dual_input_batch, \n",
    "                    tag=\"command_input_encode_embedding\"\n",
    "                )\n",
    "                hidden, encoder_outputs = model(\n",
    "                    commands_embedding=commands_embedding, \n",
    "                    commands_lengths=input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict_with_embedding\"\n",
    "                )\n",
    "                dual_hidden, dual_encoder_outputs = model(\n",
    "                    commands_embedding=dual_commands_embedding, \n",
    "                    commands_lengths=input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict_with_embedding\"\n",
    "                )\n",
    "                # intervene on lstm hidden.\n",
    "                intervened_encoder_outputs = encoder_outputs[\"encoder_outputs\"]\n",
    "                # intervene on init hidden as well.\n",
    "                intervened_hidden = hidden\n",
    "                for i in range(len(idx_selected)):\n",
    "                    assert intervened_main_swap_index[i] != -1\n",
    "                    start_idx = intervened_swap_attr[i]*intervene_dimension_size\n",
    "                    end_idx = (intervened_swap_attr[i]+1)*intervene_dimension_size\n",
    "                    intervened_encoder_outputs[\n",
    "                        i,intervened_main_shape_index[i]:intervened_main_shape_index[i]+1,start_idx:end_idx\n",
    "                    ] = dual_encoder_outputs[\"encoder_outputs\"][\n",
    "                        i,intervened_dual_shape_index[i]:intervened_dual_shape_index[i]+1,start_idx:end_idx\n",
    "                    ]\n",
    "                    intervened_hidden[i,start_idx:end_idx] = hidden[i,start_idx:end_idx]\n",
    "                hidden = model(\n",
    "                    command_hidden=intervened_hidden,\n",
    "                    tag=\"initialize_hidden\"\n",
    "                )\n",
    "                encoded_image = model(\n",
    "                    situations_input=situation_batch,\n",
    "                    tag=\"situation_encode\"\n",
    "                )\n",
    "                projected_keys_visual = model(\n",
    "                    encoded_situations=encoded_image,\n",
    "                    tag=\"projected_keys_visual\"\n",
    "                )\n",
    "                projected_keys_textual = model(\n",
    "                    command_encoder_outputs=intervened_encoder_outputs,\n",
    "                    tag=\"projected_keys_textual\"\n",
    "                )\n",
    "\n",
    "            # decoder which we do not touch at all!\n",
    "            output_sequence = []\n",
    "            cf_token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "            cf_hidden = hidden\n",
    "            for j in range(intervened_target_batch.shape[1]):\n",
    "                (cf_output, cf_hidden) = model(\n",
    "                    lstm_input_tokens_sorted=cf_token,\n",
    "                    lstm_hidden=cf_hidden,\n",
    "                    lstm_projected_keys_textual=projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "                # record the output for loss calculation.\n",
    "                cf_output = F.log_softmax(cf_output, dim=-1)\n",
    "                cf_token = cf_output.max(dim=-1)[1]\n",
    "                output_sequence.append(cf_token.data[0].item())\n",
    "                # we need to stop.\n",
    "                if cf_token == eos_idx:\n",
    "                    break\n",
    "\n",
    "            if output_sequence[-1] == eos_idx:\n",
    "                output_sequence.pop()\n",
    "            yield input_batch, dual_input_batch, \\\n",
    "                output_sequence, intervened_target_batch[:,:intervened_target_lengths_batch[0]], target_batch, dual_target_batch, 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_sentence(sentence_array, vocab):\n",
    "    return [vocab.itos[word_idx] for word_idx in sentence_array]\n",
    "\n",
    "def predict_and_save(\n",
    "    dataset: ReaSCANDataset, \n",
    "    model: nn.Module, \n",
    "    hi_model,\n",
    "    output_file_path: str, \n",
    "    max_decoding_steps: int,\n",
    "    device,\n",
    "    max_testing_examples=None, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict all data in dataset with a model and write the predictions to output_file_path.\n",
    "    :param dataset: a dataset with test examples\n",
    "    :param model: a trained model from model.py\n",
    "    :param output_file_path: a path where a .json file with predictions will be saved.\n",
    "    :param max_decoding_steps: after how many steps to force quit decoding\n",
    "    :param max_testing_examples: after how many examples to stop predicting, if None all examples will be evaluated\n",
    "    \"\"\"\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    # read-in datasets\n",
    "    if cfg[\"kwargs\"][\"restrict_sampling\"] == \"none\":\n",
    "        restrict_sampling = None\n",
    "    else:\n",
    "        restrict_sampling = cfg[\"kwargs\"][\"restrict_sampling\"]\n",
    "    test_data, _ = dataset.get_dual_dataset(novel_attribute=True, restrict_sampling=restrict_sampling)\n",
    "    data_iterator = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    eval_max_decoding_steps = max_decoding_steps\n",
    "    with open(output_file_path, mode='w') as outfile:\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if cfg[\"kwargs\"][\"task_evaluate\"]:\n",
    "                #################\n",
    "                #\n",
    "                # Task-based\n",
    "                #\n",
    "                #################\n",
    "                exact_match_count = 0\n",
    "                example_count = 0\n",
    "                for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "                        data_iterator=data_iterator, model=model, hi_model=hi_model, \n",
    "                        max_decoding_steps=max_decoding_steps, \n",
    "                        pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                        sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                        eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                        max_examples_to_evaluate=max_testing_examples, \n",
    "                        device=device,\n",
    "                        intervene_time=cfg[\"kwargs\"][\"intervene_time\"],\n",
    "                        intervene_dimension_size=cfg[\"kwargs\"][\"intervene_dimension_size\"],\n",
    "                ):\n",
    "                    example_count += 1\n",
    "                    accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                    input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                    input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                    target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                    target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                    output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                    if accuracy == 100:\n",
    "                        exact_match_count += 1\n",
    "                    output.append({\"input\": input_str_sequence, \"prediction\": output_str_sequence,\n",
    "                                   \"target\": target_str_sequence,\n",
    "                                   \"accuracy\": accuracy,\n",
    "                                   \"exact_match\": True if accuracy == 100 else False,\n",
    "                                  })      \n",
    "                exact_match = (exact_match_count/example_count)*100.0\n",
    "                logger.info(\" Task Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "                if not cfg[\"kwargs\"][\"counterfactual_evaluate\"]:\n",
    "                    logger.info(\"Wrote predictions for {} examples.\".format(example_count))\n",
    "                    json.dump(output, outfile, indent=4)\n",
    "                    return output_file_path\n",
    "            else:\n",
    "                logger.info(\"WARNING: Skipping regular task evaluation...\")\n",
    "            \n",
    "            if cfg[\"kwargs\"][\"counterfactual_evaluate\"]:\n",
    "                #################\n",
    "                #\n",
    "                # Counterfactual\n",
    "                #\n",
    "                #################\n",
    "                logger.info(\" Starting our counterfactual analysis ... \")\n",
    "                exact_match_count = 0\n",
    "                example_count = 0\n",
    "                for input_sequence, dual_input_sequence, \\\n",
    "                    output_sequence, target_sequence, original_target_sequence, original_dual_target_str_sequence, \\\n",
    "                    aux_acc_target in counterfactual_predict(\n",
    "                        data_iterator=data_iterator, model=model, \n",
    "                        hi_model=hi_model,\n",
    "                        max_decoding_steps=max_decoding_steps, \n",
    "                        eval_max_decoding_steps=eval_max_decoding_steps,\n",
    "                        pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                        sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                        eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                        max_examples_to_evaluate=max_testing_examples, \n",
    "                        device=device,\n",
    "                        intervene_attribute=cfg[\"kwargs\"][\"intervene_attribute\"],\n",
    "                        intervene_time=cfg[\"kwargs\"][\"intervene_time\"],\n",
    "                        intervene_dimension_size=cfg[\"kwargs\"][\"intervene_dimension_size\"],\n",
    "                ):\n",
    "                    example_count += 1\n",
    "                    accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                    input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                    input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                    dual_input_str_sequence = dataset.array_to_sentence(dual_input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                    dual_input_str_sequence = dual_input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "\n",
    "                    target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                    original_target_str_sequence = dataset.array_to_sentence(original_target_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                    original_dual_target_str_sequence = dataset.array_to_sentence(original_dual_target_str_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                    target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                    output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                    if accuracy == 100:\n",
    "                        exact_match_count += 1\n",
    "                    output.append({\"input\": input_str_sequence, \n",
    "                                   \"dual_input\": dual_input_str_sequence,\n",
    "                                   \"prediction\": output_str_sequence,\n",
    "                                   \"target\": target_str_sequence,\n",
    "                                   \"main_target\": original_target_str_sequence,\n",
    "                                   \"dual_target\": original_dual_target_str_sequence,\n",
    "                                   \"accuracy\": accuracy,\n",
    "                                   \"exact_match\": True if accuracy == 100 else False,\n",
    "                                   \"position_accuracy\": aux_acc_target})\n",
    "                logger.info(\"Wrote predictions for {} examples including counterfactual ones.\".format(example_count))\n",
    "                json.dump(output, outfile, indent=4)\n",
    "                exact_match = (exact_match_count/example_count)*100.0\n",
    "                logger.info(\" Counterfactual Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "        \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    random.seed(flags[\"seed\"])\n",
    "    torch.manual_seed(flags[\"seed\"])\n",
    "    np.random.seed(flags[\"seed\"])\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        assert False # we don't allow train in the evaluation script.\n",
    "    elif flags[\"mode\"] == \"test\":\n",
    "        logger.info(\"Loading all data into memory for evaluation...\")\n",
    "        logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "        data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "        assert os.path.exists(os.path.join(flags[\"data_directory\"], flags[\"input_vocab_path\"])) and os.path.exists(\n",
    "            os.path.join(flags[\"data_directory\"], flags[\"target_vocab_path\"])), \\\n",
    "            \"No vocabs found at {} and {}\".format(flags[\"input_vocab_path\"], flags[\"target_vocab_path\"])\n",
    "        splits = flags[\"splits\"].split(\",\")\n",
    "        for split in splits:\n",
    "            logger.info(\"Loading {} dataset split...\".format(split))\n",
    "            \n",
    "            test_set = ReaSCANDataset(\n",
    "                data_json, flags[\"data_directory\"], split=split,\n",
    "                input_vocabulary_file=flags[\"input_vocab_path\"],\n",
    "                target_vocabulary_file=flags[\"target_vocab_path\"],\n",
    "                generate_vocabulary=False, k=flags[\"k\"]\n",
    "            )\n",
    "            test_set.read_dataset(\n",
    "                max_examples=flags[\"max_testing_examples\"],\n",
    "                simple_situation_representation=flags[\"simple_situation_representation\"]\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Done Loading {} dataset split.\".format(flags[\"split\"]))\n",
    "            logger.info(\"  Loaded {} examples.\".format(test_set.num_examples))\n",
    "            logger.info(\"  Input vocabulary size: {}\".format(test_set.input_vocabulary_size))\n",
    "            logger.info(\"  Most common input words: {}\".format(test_set.input_vocabulary.most_common(5)))\n",
    "            logger.info(\"  Output vocabulary size: {}\".format(test_set.target_vocabulary_size))\n",
    "            logger.info(\"  Most common target words: {}\".format(test_set.target_vocabulary.most_common(5)))\n",
    "            \n",
    "            grid_size = test_set.grid_size\n",
    "            target_position_size = 2*grid_size - 1\n",
    "            \n",
    "            # create modell based on our dataset.\n",
    "            model = Model(input_vocabulary_size=test_set.input_vocabulary_size,\n",
    "                          target_vocabulary_size=test_set.target_vocabulary_size,\n",
    "                          num_cnn_channels=test_set.image_channels,\n",
    "                          input_padding_idx=test_set.input_vocabulary.pad_idx,\n",
    "                          target_pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                          target_eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                          target_position_size=target_position_size,\n",
    "                          **flags)\n",
    "\n",
    "            # gpu setups\n",
    "            use_cuda = True if torch.cuda.is_available() and not isnotebook() and not flags[\"no_cuda\"] else False\n",
    "            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            # logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            model.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            We have two low model so that our computation is much faster.\n",
    "            \"\"\"\n",
    "            eval_max_decoding_steps = flags[\"max_decoding_steps\"] # we need to use this extended step to measure for eval.\n",
    "\n",
    "            hi_model = HighLevelModel(\n",
    "                # None\n",
    "            )\n",
    "            hi_model.to(device)\n",
    "            # create high level model for counterfactual training.\n",
    "            # hi_model = get_counter_compgraph(\n",
    "            #     eval_max_decoding_steps,\n",
    "            #     cache_results=False\n",
    "            # )\n",
    "            # logger.info(\"Finish loading both low and high models..\")\n",
    "            \n",
    "            # optimizer\n",
    "            # log_parameters(model)\n",
    "            \n",
    "            # Load model and vocabularies if resuming.\n",
    "            evaluate_checkpoint = flags[\"evaluate_checkpoint\"]\n",
    "            if flags[\"evaluate_checkpoint\"] == \"\":\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], \"model_best.pth.tar\")\n",
    "            else:\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], f\"checkpoint-{evaluate_checkpoint}.pth.tar\")\n",
    "            assert os.path.isfile(model_path), \"No checkpoint found at {}\".format(model_path)\n",
    "            logger.info(\"Loading checkpoint from file at '{}'\".format(model_path))\n",
    "            model.load_model(device, model_path, strict=False)\n",
    "            start_iteration = model.trained_iterations\n",
    "            logger.info(\"Loaded checkpoint '{}' (iter {})\".format(model_path, start_iteration))\n",
    "            output_file_name = \"_\".join([split, flags[\"output_file_name\"]])\n",
    "            output_file_path = os.path.join(flags[\"resume_from_file\"], output_file_name)\n",
    "            logger.info(\"All results will be saved to '{}'\".format(output_file_path))\n",
    "            \n",
    "            output_file = predict_and_save(\n",
    "                dataset=test_set, \n",
    "                model=model, hi_model=hi_model,\n",
    "                output_file_path=output_file_path, \n",
    "                device=device,\n",
    "                **flags\n",
    "            )\n",
    "            logger.info(\"Saved predictions to {}\".format(output_file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
