{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from decode_graphical_models import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    hi_model,\n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device,\n",
    "    intervene_time,\n",
    "    intervene_dimension_size,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(tqdm(data_iterator)):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths, \\\n",
    "            dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "            dual_agent_positions, dual_target_positions, \\\n",
    "            dual_input_lengths, dual_target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        high_hidden_states = hi_model(\n",
    "            agent_positions_batch=agent_positions.unsqueeze(dim=-1), \n",
    "            target_positions_batch=target_positions.unsqueeze(dim=-1), \n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        high_actions = torch.zeros(\n",
    "            high_hidden_states.size(0), 1\n",
    "        ).long().to(device)\n",
    "\n",
    "        # get the intercepted dual hidden states.\n",
    "        for j in range(intervene_time):\n",
    "            high_hidden_states, _ = hi_model(\n",
    "                hmm_states=high_hidden_states, \n",
    "                hmm_actions=high_actions, \n",
    "                tag=\"_hmm_step_fxn\"\n",
    "            )\n",
    "        true_target_positions = high_hidden_states+5\n",
    "        \n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        attention_weights_commands = []\n",
    "        attention_weights_situations = []\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            \n",
    "            # pred target positions.\n",
    "            if decoding_iteration == intervene_time-1:\n",
    "                # we need to evaluate our position pred as well.\n",
    "                \n",
    "                x_s_idx = 0\n",
    "                y_s_idx = intervene_dimension_size\n",
    "                y_e_idx = 2*intervene_dimension_size\n",
    "                \n",
    "                cf_target_positions_x = model(\n",
    "                    position_hidden=hidden[0][:,:,x_s_idx:y_s_idx].squeeze(dim=1),\n",
    "                    cf_auxiliary_task_tag=\"x\",\n",
    "                    tag=\"cf_auxiliary_task\"\n",
    "                )\n",
    "                cf_target_positions_x = F.log_softmax(cf_target_positions_x, dim=-1)\n",
    "                cf_target_positions_y = model(\n",
    "                    position_hidden=hidden[0][:,:,y_s_idx:y_e_idx].squeeze(dim=1),\n",
    "                    cf_auxiliary_task_tag=\"x\",\n",
    "                    tag=\"cf_auxiliary_task\"\n",
    "                )\n",
    "                cf_target_positions_y = F.log_softmax(cf_target_positions_y, dim=-1)\n",
    "                \n",
    "                loss_position_x = model(\n",
    "                    loss_pred_target_positions=cf_target_positions_x,\n",
    "                    loss_true_target_positions=true_target_positions[:,0],\n",
    "                    tag=\"cf_auxiliary_task_loss\"\n",
    "                )\n",
    "                loss_position_y = model(\n",
    "                    loss_pred_target_positions=cf_target_positions_y,\n",
    "                    loss_true_target_positions=true_target_positions[:,1],\n",
    "                    tag=\"cf_auxiliary_task_loss\"\n",
    "                )\n",
    "                # some metrics\n",
    "                metrics_position_x = model(\n",
    "                    loss_pred_target_positions=cf_target_positions_x,\n",
    "                    loss_true_target_positions=true_target_positions[:,0],\n",
    "                    tag=\"cf_auxiliary_task_metrics\"\n",
    "                )\n",
    "                metrics_position_y = model(\n",
    "                    loss_pred_target_positions=cf_target_positions_y,\n",
    "                    loss_true_target_positions=true_target_positions[:,1],\n",
    "                    tag=\"cf_auxiliary_task_metrics\"\n",
    "                )\n",
    "                predicted_target_positions_x = cf_target_positions_x.max(dim=-1)[1]\n",
    "                predicted_target_positions_y = cf_target_positions_y.max(dim=-1)[1]\n",
    "            \n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "\n",
    "        yield (\n",
    "            input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target, \n",
    "            metrics_position_x, metrics_position_y, \n",
    "            predicted_target_positions_x, predicted_target_positions_y,\n",
    "            true_target_positions[:,0], true_target_positions[:,1],\n",
    "        )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterfactual_predict(\n",
    "    data_iterator, \n",
    "    model, hi_model,\n",
    "    max_decoding_steps, \n",
    "    eval_max_decoding_steps,\n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device,\n",
    "    intervene_attribute=-1,\n",
    "    intervene_time=-1,\n",
    "    intervene_dimension_size=25,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    total_count = 0\n",
    "    bad_count = 0\n",
    "    \n",
    "    random_attr = False\n",
    "    random_time = False\n",
    "    if intervene_attribute == -1:\n",
    "        random_attr = True\n",
    "    if intervene_time == -1:\n",
    "        random_time = True\n",
    "    for step, batch in enumerate(tqdm(data_iterator)):\n",
    "        \n",
    "        total_count += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if total_count > max_examples_to_evaluate:\n",
    "                break\n",
    "\n",
    "        # main batch\n",
    "        input_batch, target_batch, situation_batch, \\\n",
    "            agent_positions_batch, target_positions_batch, \\\n",
    "            input_lengths_batch, target_lengths_batch, \\\n",
    "            dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "            dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "            dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "        \n",
    "        assert input_batch.size(0) == 1\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        \n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        situation_batch = situation_batch.to(device)\n",
    "        agent_positions_batch = agent_positions_batch.to(device)\n",
    "        target_positions_batch = target_positions_batch.to(device)\n",
    "        input_lengths_batch = input_lengths_batch.to(device)\n",
    "        target_lengths_batch = target_lengths_batch.to(device)\n",
    "\n",
    "        dual_input_max_seq_lens = max(dual_input_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        dual_input_batch = dual_input_batch.to(device)\n",
    "        dual_target_batch = dual_target_batch.to(device)\n",
    "        dual_situation_batch = dual_situation_batch.to(device)\n",
    "        dual_agent_positions_batch = dual_agent_positions_batch.to(device)\n",
    "        dual_target_positions_batch = dual_target_positions_batch.to(device)\n",
    "        dual_input_lengths_batch = dual_input_lengths_batch.to(device)\n",
    "        dual_target_lengths_batch = dual_target_lengths_batch.to(device)\n",
    "\n",
    "        # Step 1: using a for loop to get the hidden states from the dual data\n",
    "        input_batch = input_batch[:,:input_max_seq_lens]\n",
    "        target_batch = target_batch[:,:target_max_seq_lens]\n",
    "        dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "        \n",
    "        # what to intervene\n",
    "        \"\"\"\n",
    "        For the sake of quick training, for a single batch,\n",
    "        we select the same attribute to intervenen on:\n",
    "        0: x\n",
    "        1: y\n",
    "        2: orientation\n",
    "        \"\"\"\n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        min_len = min(target_max_seq_lens, dual_target_max_seq_lens)\n",
    "           \n",
    "        if random_attr:\n",
    "            intervene_attribute = random.choice([0,1,2])\n",
    "        if random_time:\n",
    "            intervene_time = random.randint(\n",
    "                1, max(target_lengths_batch)[0]-2\n",
    "            ) # we get rid of SOS and EOS tokens\n",
    "            # we also need to get the to-intervened time\n",
    "            intervene_with_time = random.randint(\n",
    "                1, max(dual_target_lengths_batch)[0]-2\n",
    "            ) # we get rid of SOS and EOS tokens\n",
    "        else:\n",
    "            intervene_with_time = intervene_time\n",
    "        # print(intervene_time, intervene_with_time, intervene_attribute)\n",
    "        #####################\n",
    "        #\n",
    "        # high data start\n",
    "        #\n",
    "        #####################\n",
    "\n",
    "        # to have high quality high data, we need to iterate through each example.\n",
    "        batch_size = agent_positions_batch.size(0)\n",
    "\n",
    "        high_hidden_states = hi_model(\n",
    "            agent_positions_batch=agent_positions_batch.unsqueeze(dim=-1), \n",
    "            target_positions_batch=target_positions_batch.unsqueeze(dim=-1), \n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        high_actions = torch.zeros(\n",
    "            high_hidden_states.size(0), 1\n",
    "        ).long().to(device)\n",
    "\n",
    "        dual_high_hidden_states = hi_model(\n",
    "            agent_positions_batch=dual_agent_positions_batch.unsqueeze(dim=-1), \n",
    "            target_positions_batch=dual_target_positions_batch.unsqueeze(dim=-1), \n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        dual_high_actions = torch.zeros(\n",
    "            dual_high_hidden_states.size(0), 1\n",
    "        ).long().to(device)\n",
    "        \n",
    "            \n",
    "        # get the intercepted dual hidden states.\n",
    "        for j in range(intervene_with_time):\n",
    "            dual_high_hidden_states, dual_high_actions = hi_model(\n",
    "                hmm_states=dual_high_hidden_states, \n",
    "                hmm_actions=dual_high_actions, \n",
    "                tag=\"_hmm_step_fxn\"\n",
    "            )\n",
    "\n",
    "        # main intervene for loop.\n",
    "        cf_high_hidden_states = high_hidden_states\n",
    "        cf_high_actions = high_actions\n",
    "        intervened_target_batch = [torch.ones(high_hidden_states.size(0), 1).long().to(device)] # SOS tokens\n",
    "        intervened_target_lengths_batch = torch.zeros(high_hidden_states.size(0), 1).long().to(device)\n",
    "        # we need to take of the SOS and EOS tokens.\n",
    "        for j in range(eval_max_decoding_steps-1):\n",
    "            # intercept like antra!\n",
    "            if j == intervene_time-1:\n",
    "                # we need idle once by getting the states but not continue the HMM!\n",
    "                cf_high_hidden_states, _ = hi_model(\n",
    "                    hmm_states=cf_high_hidden_states, \n",
    "                    hmm_actions=cf_high_actions, \n",
    "                    tag=\"_hmm_step_fxn\"\n",
    "                )\n",
    "                # only swap out this part.\n",
    "                cf_high_hidden_states[:,intervene_attribute] = dual_high_hidden_states[:,intervene_attribute]\n",
    "                # comment out two lines below if it is not for testing.\n",
    "                # cf_high_hidden_states = dual_high_hidden_states\n",
    "                cf_high_actions = torch.zeros(\n",
    "                    high_hidden_states.size(0), 1\n",
    "                ).long().to(device)\n",
    "            cf_high_hidden_states, cf_high_actions = hi_model(\n",
    "                hmm_states=cf_high_hidden_states, \n",
    "                hmm_actions=cf_high_actions, \n",
    "                tag=\"_hmm_step_fxn\"\n",
    "            )\n",
    "            # record the output for loss calculation.\n",
    "            intervened_target_batch += [cf_high_actions]\n",
    "            intervened_target_lengths_batch += (cf_high_actions!=0).long()\n",
    "            # let us early stop!\n",
    "            if cf_high_actions[0,0] == 0:\n",
    "                break\n",
    "        intervened_target_lengths_batch += 2\n",
    "        intervened_target_batch = torch.cat(intervened_target_batch, dim=-1)\n",
    "        for i in range(high_hidden_states.size(0)):\n",
    "            if intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] == 0:\n",
    "                intervened_target_batch[i,intervened_target_lengths_batch[i,0]-1] = 2\n",
    "\n",
    "        # we need to make sure the interven`ed action list is NOT\n",
    "        # exactly the same as the original target list!\n",
    "        # otherwise, we are overestimating the performance a lot.\n",
    "        match_target_intervened = intervened_target_batch[:,:intervened_target_lengths_batch[0,0]]\n",
    "        match_target_main = target_batch\n",
    "        # is_bad_intervened = torch.equal(match_target_intervened, match_target_main)\n",
    "        is_bad_intervened = (intervene_time>(target_lengths_batch[i,0]-2)) or \\\n",
    "                    (intervene_with_time>(dual_target_lengths_batch[i,0]-2))\n",
    "        if is_bad_intervened:\n",
    "            continue # we need to skip these.\n",
    "            \n",
    "        # print(\"->\", target_batch)\n",
    "        # print(\"->\", dual_target_batch)\n",
    "        # print(\"--\", intervene_time)\n",
    "        # print(\"<-\", intervened_target_batch[:,:intervened_target_lengths_batch[0,0]])\n",
    "            \n",
    "        #####################\n",
    "        #\n",
    "        # high data end\n",
    "        #\n",
    "        #####################\n",
    "        \n",
    "        #####################\n",
    "        #\n",
    "        # low data start\n",
    "        #\n",
    "        #####################\n",
    "        ## main\n",
    "        \"\"\"\n",
    "        Low level data requires GPU forwarding.\n",
    "        \"\"\"\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation_batch,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_batch, \n",
    "            commands_lengths=input_lengths_batch,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        main_hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "\n",
    "        dual_input_batch = dual_input_batch[:,:dual_input_max_seq_lens]\n",
    "        dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        dual_encoded_image = model(\n",
    "            situations_input=dual_situation_batch,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        dual_hidden, dual_encoder_outputs = model(\n",
    "            commands_input=dual_input_batch, \n",
    "            commands_lengths=dual_input_lengths_batch,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        dual_hidden = model(\n",
    "            command_hidden=dual_hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        dual_projected_keys_visual = model(\n",
    "            encoded_situations=dual_encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        dual_projected_keys_textual = model(\n",
    "            command_encoder_outputs=dual_encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        for j in range(intervene_with_time):\n",
    "            (dual_ouput, dual_hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=dual_hidden,\n",
    "                lstm_projected_keys_textual=dual_projected_keys_textual,\n",
    "                lstm_commands_lengths=dual_input_lengths_batch,\n",
    "                lstm_projected_keys_visual=dual_projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            dual_ouput = F.log_softmax(dual_ouput, dim=-1)\n",
    "            token = dual_ouput.max(dim=-1)[1]\n",
    "\n",
    "        # Step 2: using a for loop to decode the main data \n",
    "        # as well as inject the hidden states from the dual data\n",
    "\n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        cf_token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        \n",
    "        cf_hidden = main_hidden\n",
    "        # while cf_token != eos_idx and decoding_iteration < max_decoding_steps:\n",
    "        for decoding_iteration in range(intervened_target_batch.shape[1]):\n",
    "            # this is for testing.\n",
    "            # cf_token=intervened_target_batch[:,decoding_iteration]\n",
    "            if decoding_iteration == intervene_time-1:\n",
    "                # we need idle once by getting the states but not continue the HMM!\n",
    "                (_, cf_hidden) = model(\n",
    "                    lstm_input_tokens_sorted=cf_token,\n",
    "                    lstm_hidden=cf_hidden,\n",
    "                    lstm_projected_keys_textual=projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "                s_idx = intervene_attribute*intervene_dimension_size\n",
    "                e_idx = (intervene_attribute+1)*intervene_dimension_size\n",
    "                cf_hidden[0][:,:,s_idx:e_idx] = dual_hidden[0][:,:,s_idx:e_idx] # only swap out this part.\n",
    "                # WARNING: this is a special way to bypassing the state\n",
    "                # updates during intervention!\n",
    "                cf_token = None\n",
    "            (cf_output, cf_hidden) = model(\n",
    "                lstm_input_tokens_sorted=cf_token,\n",
    "                lstm_hidden=cf_hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths_batch,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            cf_output = F.log_softmax(cf_output, dim=-1)\n",
    "            cf_token = cf_output.max(dim=-1)[1]\n",
    "            output_sequence.append(cf_token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "            # we need to stop.\n",
    "            if cf_token == eos_idx:\n",
    "                break\n",
    "        \n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "\n",
    "        # print(\"<-\", output_sequence)\n",
    "        #####################\n",
    "        #\n",
    "        # low data end\n",
    "        #\n",
    "        #####################\n",
    "        \n",
    "        yield input_batch, dual_input_batch, intervene_attribute, intervene_time, \\\n",
    "            output_sequence, intervened_target_batch[:,:intervened_target_lengths_batch[0,0]], target_batch, \\\n",
    "            dual_target_batch, 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_sentence(sentence_array, vocab):\n",
    "    return [vocab.itos[word_idx] for word_idx in sentence_array]\n",
    "\n",
    "def predict_and_save(\n",
    "    dataset: ReaSCANDataset, \n",
    "    model: nn.Module, \n",
    "    hi_model,\n",
    "    output_file_path: str, \n",
    "    max_decoding_steps: int,\n",
    "    device,\n",
    "    max_testing_examples=None, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict all data in dataset with a model and write the predictions to output_file_path.\n",
    "    :param dataset: a dataset with test examples\n",
    "    :param model: a trained model from model.py\n",
    "    :param output_file_path: a path where a .json file with predictions will be saved.\n",
    "    :param max_decoding_steps: after how many steps to force quit decoding\n",
    "    :param max_testing_examples: after how many examples to stop predicting, if None all examples will be evaluated\n",
    "    \"\"\"\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    # read-in datasets\n",
    "    test_data, _ = dataset.get_dual_dataset()\n",
    "    data_iterator = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    eval_max_decoding_steps = max_decoding_steps\n",
    "    with open(output_file_path, mode='w') as outfile:\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #################\n",
    "            #\n",
    "            # Task-based\n",
    "            #\n",
    "            #################\n",
    "            exact_match_count = 0\n",
    "            example_count = 0\n",
    "            for input_sequence, output_sequence, target_sequence, aux_acc_target, pred_pos_x_acc, \\\n",
    "                pred_pos_y_acc, pred_pos_x, pred_pos_y, true_pos_x, true_pos_y in predict(\n",
    "                    data_iterator=data_iterator, model=model, hi_model=hi_model, \n",
    "                    max_decoding_steps=max_decoding_steps, \n",
    "                    pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                    sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                    eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                    max_examples_to_evaluate=max_testing_examples, \n",
    "                    device=device,\n",
    "                    intervene_time=cfg[\"kwargs\"][\"intervene_time\"],\n",
    "                    intervene_dimension_size=cfg[\"kwargs\"][\"intervene_dimension_size\"],\n",
    "            ):\n",
    "                example_count += 1\n",
    "                accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                if accuracy == 100:\n",
    "                    exact_match_count += 1\n",
    "                output.append({\"input\": input_str_sequence, \"prediction\": output_str_sequence,\n",
    "                               \"target\": target_str_sequence,\n",
    "                               \"accuracy\": accuracy,\n",
    "                               \"exact_match\": True if accuracy == 100 else False,\n",
    "                               \"pred_pos_x_acc\": pred_pos_x_acc.tolist(), \n",
    "                               \"pred_pos_y_acc\": pred_pos_y_acc.tolist(), \n",
    "                               \"pred_pos_exact_match\": True if pred_pos_x_acc ==100 and pred_pos_y_acc == 100 else False,\n",
    "                               \"pred_pos\": (pred_pos_x.tolist()[0]-5, pred_pos_y.tolist()[0]-5),\n",
    "                               \"true_pos\": (true_pos_x.tolist()[0]-5, true_pos_y.tolist()[0]-5),\n",
    "                              })      \n",
    "            exact_match = (exact_match_count/example_count)*100.0\n",
    "            logger.info(\" Task Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "            if not cfg[\"kwargs\"][\"counterfactual_evaluate\"]:\n",
    "                logger.info(\"Wrote predictions for {} examples.\".format(example_count))\n",
    "                json.dump(output, outfile, indent=4)\n",
    "                return output_file_path\n",
    "\n",
    "            #################\n",
    "            #\n",
    "            # Counterfactual\n",
    "            #\n",
    "            #################\n",
    "            logger.info(\" Starting our counterfactual analysis ... \")\n",
    "            exact_match_count = 0\n",
    "            example_count = 0\n",
    "            for input_sequence, dual_input_sequence, intervene_attribute, intervene_time, \\\n",
    "                output_sequence, target_sequence, original_target_sequence, original_dual_target_str_sequence, \\\n",
    "                aux_acc_target in counterfactual_predict(\n",
    "                    data_iterator=data_iterator, model=model, \n",
    "                    hi_model=hi_model,\n",
    "                    max_decoding_steps=max_decoding_steps, \n",
    "                    eval_max_decoding_steps=eval_max_decoding_steps,\n",
    "                    pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                    sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                    eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                    max_examples_to_evaluate=max_testing_examples, \n",
    "                    device=device,\n",
    "                    intervene_attribute=cfg[\"kwargs\"][\"intervene_attribute\"],\n",
    "                    intervene_time=cfg[\"kwargs\"][\"intervene_time\"],\n",
    "                    intervene_dimension_size=cfg[\"kwargs\"][\"intervene_dimension_size\"],\n",
    "            ):\n",
    "                example_count += 1\n",
    "                accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                dual_input_str_sequence = dataset.array_to_sentence(dual_input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                dual_input_str_sequence = dual_input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                \n",
    "                target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                original_target_str_sequence = dataset.array_to_sentence(original_target_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                original_dual_target_str_sequence = dataset.array_to_sentence(original_dual_target_str_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                if accuracy == 100:\n",
    "                    exact_match_count += 1\n",
    "                output.append({\"input\": input_str_sequence, \n",
    "                               \"dual_input\": dual_input_str_sequence,\n",
    "                               \"intervene_attribute\": intervene_attribute,\n",
    "                               \"intervene_time\": intervene_time,\n",
    "                               \"prediction\": output_str_sequence,\n",
    "                               \"target\": target_str_sequence,\n",
    "                               \"main_target\": original_target_str_sequence,\n",
    "                               \"dual_target\": original_dual_target_str_sequence,\n",
    "                               \"accuracy\": accuracy,\n",
    "                               \"exact_match\": True if accuracy == 100 else False,\n",
    "                               \"position_accuracy\": aux_acc_target})\n",
    "            logger.info(\"Wrote predictions for {} examples including counterfactual ones.\".format(example_count))\n",
    "            json.dump(output, outfile, indent=4)\n",
    "            exact_match = (exact_match_count/example_count)*100.0\n",
    "            logger.info(\" Counterfactual Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "        \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    random.seed(flags[\"seed\"])\n",
    "    torch.manual_seed(flags[\"seed\"])\n",
    "    np.random.seed(flags[\"seed\"])\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        assert False # we don't allow train in the evaluation script.\n",
    "    elif flags[\"mode\"] == \"test\":\n",
    "        logger.info(\"Loading all data into memory for evaluation...\")\n",
    "        logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "        data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "        assert os.path.exists(os.path.join(flags[\"data_directory\"], flags[\"input_vocab_path\"])) and os.path.exists(\n",
    "            os.path.join(flags[\"data_directory\"], flags[\"target_vocab_path\"])), \\\n",
    "            \"No vocabs found at {} and {}\".format(flags[\"input_vocab_path\"], flags[\"target_vocab_path\"])\n",
    "        splits = flags[\"splits\"].split(\",\")\n",
    "        for split in splits:\n",
    "            logger.info(\"Loading {} dataset split...\".format(split))\n",
    "            \n",
    "            test_set = ReaSCANDataset(\n",
    "                data_json, flags[\"data_directory\"], split=split,\n",
    "                input_vocabulary_file=flags[\"input_vocab_path\"],\n",
    "                target_vocabulary_file=flags[\"target_vocab_path\"],\n",
    "                generate_vocabulary=False, k=flags[\"k\"]\n",
    "            )\n",
    "            test_set.read_dataset(\n",
    "                max_examples=flags[\"max_testing_examples\"],\n",
    "                simple_situation_representation=flags[\"simple_situation_representation\"]\n",
    "            )\n",
    "            logger.info(\"Done Loading {} dataset split.\".format(flags[\"split\"]))\n",
    "            logger.info(\"  Loaded {} examples.\".format(test_set.num_examples))\n",
    "            logger.info(\"  Input vocabulary size: {}\".format(test_set.input_vocabulary_size))\n",
    "            logger.info(\"  Most common input words: {}\".format(test_set.input_vocabulary.most_common(5)))\n",
    "            logger.info(\"  Output vocabulary size: {}\".format(test_set.target_vocabulary_size))\n",
    "            logger.info(\"  Most common target words: {}\".format(test_set.target_vocabulary.most_common(5)))\n",
    "            \n",
    "            grid_size = test_set.grid_size\n",
    "            target_position_size = 2*grid_size - 1\n",
    "            \n",
    "            # create modell based on our dataset.\n",
    "            model = Model(input_vocabulary_size=test_set.input_vocabulary_size,\n",
    "                          target_vocabulary_size=test_set.target_vocabulary_size,\n",
    "                          num_cnn_channels=test_set.image_channels,\n",
    "                          input_padding_idx=test_set.input_vocabulary.pad_idx,\n",
    "                          target_pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                          target_eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                          target_position_size=target_position_size,\n",
    "                          **flags)\n",
    "\n",
    "            # gpu setups\n",
    "            use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            # logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            model.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            We have two low model so that our computation is much faster.\n",
    "            \"\"\"\n",
    "            eval_max_decoding_steps = flags[\"max_decoding_steps\"] # we need to use this extended step to measure for eval.\n",
    "\n",
    "            hi_model = HighLevelModel(\n",
    "                # None\n",
    "            )\n",
    "            hi_model.to(device)\n",
    "            # create high level model for counterfactual training.\n",
    "            # hi_model = get_counter_compgraph(\n",
    "            #     eval_max_decoding_steps,\n",
    "            #     cache_results=False\n",
    "            # )\n",
    "            # logger.info(\"Finish loading both low and high models..\")\n",
    "            \n",
    "            # optimizer\n",
    "            # log_parameters(model)\n",
    "            \n",
    "            # Load model and vocabularies if resuming.\n",
    "            evaluate_checkpoint = flags[\"evaluate_checkpoint\"]\n",
    "            if flags[\"evaluate_checkpoint\"] == \"\":\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], \"model_best.pth.tar\")\n",
    "            else:\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], f\"checkpoint-{evaluate_checkpoint}.pth.tar\")\n",
    "            assert os.path.isfile(model_path), \"No checkpoint found at {}\".format(model_path)\n",
    "            logger.info(\"Loading checkpoint from file at '{}'\".format(model_path))\n",
    "            model.load_model(device, model_path, strict=False)\n",
    "            start_iteration = model.trained_iterations\n",
    "            logger.info(\"Loaded checkpoint '{}' (iter {})\".format(model_path, start_iteration))\n",
    "            output_file_name = \"_\".join([split, flags[\"output_file_name\"]])\n",
    "            output_file_path = os.path.join(flags[\"resume_from_file\"], output_file_name)\n",
    "            logger.info(\"All results will be saved to '{}'\".format(output_file_path))\n",
    "            \n",
    "            output_file = predict_and_save(\n",
    "                dataset=test_set, \n",
    "                model=model, hi_model=hi_model,\n",
    "                output_file_path=output_file_path, \n",
    "                device=device,\n",
    "                **flags\n",
    "            )\n",
    "            logger.info(\"Saved predictions to {}\".format(output_file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
