{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script will automatically pick all the saved models\n",
    "in the folder saved using our training script, and analyze\n",
    "them, and output analysis into a file.\n",
    "\n",
    "It will evaluate models based on their task performance as\n",
    "well as counterfactual performance. It will also analyze\n",
    "these metrics using all the checkpoints.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from decode_graphical_models import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(data_iterator):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths, \\\n",
    "            dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "            dual_agent_positions, dual_target_positions, \\\n",
    "            dual_input_lengths, dual_target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        attention_weights_commands = []\n",
    "        attention_weights_situations = []\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "        if model(tag=\"auxiliary_task\"):\n",
    "            pass\n",
    "        else:\n",
    "            auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "        yield (input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counterfactual_predict(\n",
    "    data_iterator, \n",
    "    model, low_model, low_model_cf, hi_model,\n",
    "    max_decoding_steps, \n",
    "    eval_max_decoding_steps,\n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    bad_count = 0\n",
    "    for step, batch in enumerate(data_iterator):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "\n",
    "        # main batch\n",
    "        input_batch, target_batch, situation_batch, \\\n",
    "            agent_positions_batch, target_positions_batch, \\\n",
    "            input_lengths_batch, target_lengths_batch, \\\n",
    "            dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "            dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "            dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        \n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        situation_batch = situation_batch.to(device)\n",
    "        agent_positions_batch = agent_positions_batch.to(device)\n",
    "        target_positions_batch = target_positions_batch.to(device)\n",
    "        input_lengths_batch = input_lengths_batch.to(device)\n",
    "        target_lengths_batch = target_lengths_batch.to(device)\n",
    "\n",
    "        dual_input_max_seq_lens = max(dual_input_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        dual_input_batch = dual_input_batch.to(device)\n",
    "        dual_target_batch = dual_target_batch.to(device)\n",
    "        dual_situation_batch = dual_situation_batch.to(device)\n",
    "        dual_agent_positions_batch = dual_agent_positions_batch.to(device)\n",
    "        dual_target_positions_batch = dual_target_positions_batch.to(device)\n",
    "        dual_input_lengths_batch = dual_input_lengths_batch.to(device)\n",
    "        dual_target_lengths_batch = dual_target_lengths_batch.to(device)\n",
    "\n",
    "        # Step 1: using a for loop to get the hidden states from the dual data\n",
    "        input_batch = input_batch[:,:input_max_seq_lens]\n",
    "        target_batch = target_batch[:,:target_max_seq_lens]\n",
    "        dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "        \n",
    "        # what to intervene\n",
    "        \"\"\"\n",
    "        For the sake of quick training, for a single batch,\n",
    "        we select the same attribute to intervenen on:\n",
    "        0: x\n",
    "        1: y\n",
    "        2: orientation\n",
    "        \"\"\"\n",
    "        input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "        target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "        dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "        intervene_attribute = random.choice([0,1,2])\n",
    "        min_len = min(target_max_seq_lens, dual_target_max_seq_lens)\n",
    "        intervene_time = random.randint(1, min_len-1) # we get rid of SOS and EOS tokens\n",
    "        \n",
    "        #####################\n",
    "        #\n",
    "        # high data start\n",
    "        #\n",
    "        #####################\n",
    "\n",
    "        # to have high quality high data, we need to iterate through each example.\n",
    "        batch_size = agent_positions_batch.size(0)\n",
    "        intervened_target_batch = []\n",
    "        intervened_target_lengths_batch = []\n",
    "        for i in range(batch_size):\n",
    "            ## main\n",
    "            main_high = {\n",
    "                \"agent_positions_batch\": agent_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                \"target_positions_batch\": target_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "            }\n",
    "            g_main_high = GraphInput(main_high, batched=True, batch_dim=0, cache_results=False)\n",
    "            ## dual\n",
    "            dual_high = {\n",
    "                \"agent_positions_batch\": dual_agent_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                \"target_positions_batch\": dual_target_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "            }\n",
    "            g_dual_high = GraphInput(dual_high, batched=True, batch_dim=0, cache_results=False)\n",
    "            dual_target_length = dual_target_lengths_batch[i][0].tolist()\n",
    "\n",
    "            # get the duel high state\n",
    "            main_high_hidden = hi_model.compute_node(f\"s{intervene_time}\", g_main_high)\n",
    "            dual_high_hidden = hi_model.compute_node(f\"s{intervene_time}\", g_dual_high)\n",
    "            # only intervene on an selected attribute.\n",
    "            main_high_hidden[:,intervene_attribute] = dual_high_hidden[:,intervene_attribute]\n",
    "            high_interv = Intervention(\n",
    "                g_main_high, {f\"s{intervene_time}\": main_high_hidden}, \n",
    "                cache_results=False,\n",
    "                cache_base_results=False,\n",
    "                batched=True\n",
    "            )\n",
    "            intervened_outputs = []\n",
    "            for i in range(0, eval_max_decoding_steps-2): # we only have this many steps can intervened.\n",
    "                _, intervened_output = hi_model.intervene_node(f\"s{i+1}\", high_interv)\n",
    "                if intervened_output[0,3] == 0 or i == eval_max_decoding_steps-3:\n",
    "                    # we need to record the length and early stop\n",
    "                    intervened_outputs = [1] + intervened_outputs + [2]\n",
    "                    intervened_outputs = torch.tensor(intervened_outputs).long()\n",
    "                    intervened_length = len(intervened_outputs)\n",
    "                    # we need to confine the longest length!\n",
    "                    if intervened_length > eval_max_decoding_steps:\n",
    "                        intervened_length = eval_max_decoding_steps\n",
    "                        intervened_outputs = intervened_outputs[:eval_max_decoding_steps]\n",
    "                    intervened_target_batch += [intervened_outputs]\n",
    "                    intervened_target_lengths_batch += [intervened_length]\n",
    "                    break\n",
    "                else:\n",
    "                    intervened_outputs.append(intervened_output[0,3].tolist())\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # we need to pad to the longest ones.\n",
    "            intervened_target_batch[i] = torch.cat([\n",
    "                intervened_target_batch[i],\n",
    "                torch.zeros(int(eval_max_decoding_steps-intervened_target_lengths_batch[i]), dtype=torch.long)], dim=0\n",
    "            )\n",
    "        intervened_target_lengths_batch = torch.tensor(intervened_target_lengths_batch).long().unsqueeze(dim=-1)\n",
    "        intervened_target_batch = torch.stack(intervened_target_batch, dim=0)\n",
    "        # intervened data.\n",
    "        intervened_target_batch = intervened_target_batch.to(device)\n",
    "        intervened_target_lengths_batch = intervened_target_lengths_batch.to(device)\n",
    "        \n",
    "        # we need to make sure the intervened action list is NOT\n",
    "        # exactly the same as the original target list!\n",
    "        # otherwise, we are overestimating the performance a lot.\n",
    "        match_target_intervened = intervened_target_batch[:,:intervened_target_lengths_batch[0,0]]\n",
    "        match_target_main = target_batch\n",
    "        is_bad_intervened = torch.equal(match_target_intervened, match_target_main)\n",
    "        if is_bad_intervened:\n",
    "            continue # we need to skip these.\n",
    "        #####################\n",
    "        #\n",
    "        # high data end\n",
    "        #\n",
    "        #####################\n",
    "        \n",
    "        #####################\n",
    "        #\n",
    "        # low data start\n",
    "        #\n",
    "        #####################\n",
    "        ## main\n",
    "        \"\"\"\n",
    "        Low level data requires GPU forwarding.\n",
    "        \"\"\"\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation_batch,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_batch, \n",
    "            commands_lengths=input_lengths_batch,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        main_hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "\n",
    "        dual_input_batch = dual_input_batch[:,:dual_input_max_seq_lens]\n",
    "        dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        dual_encoded_image = model(\n",
    "            situations_input=dual_situation_batch,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        dual_hidden, dual_encoder_outputs = model(\n",
    "            commands_input=dual_input_batch, \n",
    "            commands_lengths=dual_input_lengths_batch,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        dual_hidden = model(\n",
    "            command_hidden=dual_hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        dual_projected_keys_visual = model(\n",
    "            encoded_situations=dual_encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        dual_projected_keys_textual = model(\n",
    "            command_encoder_outputs=dual_encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        for _ in range(intervene_time):\n",
    "            \n",
    "            (output, dual_hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=dual_hidden,\n",
    "                lstm_projected_keys_textual=dual_projected_keys_textual,\n",
    "                lstm_commands_lengths=dual_input_lengths_batch,\n",
    "                lstm_projected_keys_visual=dual_projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "        # Step 2: using a for loop to decode the main data \n",
    "        # as well as inject the hidden states from the dual data\n",
    "\n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        cf_token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        \n",
    "        cf_hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        while cf_token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            if decoding_iteration == intervene_time:\n",
    "                s_idx = intervene_attribute*25\n",
    "                e_idx = intervene_attribute*25+25\n",
    "                cf_hidden[0][:,s_idx:e_idx] = dual_hidden[0][:,s_idx:e_idx] # only swap out this part.\n",
    "                # injection here.\n",
    "                (cf_output, cf_hidden) = model(\n",
    "                    lstm_input_tokens_sorted=cf_token,\n",
    "                    lstm_hidden=cf_hidden,\n",
    "                    lstm_projected_keys_textual=projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "            else:\n",
    "                (cf_output, cf_hidden) = model(\n",
    "                    lstm_input_tokens_sorted=cf_token,\n",
    "                    lstm_hidden=cf_hidden,\n",
    "                    lstm_projected_keys_textual=projected_keys_textual,\n",
    "                    lstm_commands_lengths=input_lengths_batch,\n",
    "                    lstm_projected_keys_visual=projected_keys_visual,\n",
    "                    tag=\"_lstm_step_fxn\"\n",
    "                )\n",
    "            cf_output = F.log_softmax(cf_output, dim=-1)\n",
    "            cf_token = cf_output.max(dim=-1)[1]\n",
    "            output_sequence.append(cf_token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "        \n",
    "        #####################\n",
    "        #\n",
    "        # low data end\n",
    "        #\n",
    "        #####################\n",
    "        \n",
    "        yield input_batch, dual_input_batch, intervene_attribute, intervene_time, \\\n",
    "            output_sequence, intervened_target_batch[:,:intervened_target_lengths_batch[0,0]], target_batch, \\\n",
    "            dual_target_batch, 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_sentence(sentence_array, vocab):\n",
    "    return [vocab.itos[word_idx] for word_idx in sentence_array]\n",
    "\n",
    "def predict_and_save(\n",
    "    dataset: ReaSCANDataset, \n",
    "    model: nn.Module, \n",
    "    low_model, low_model_cf, hi_model,\n",
    "    output_file_path: str, \n",
    "    max_decoding_steps: int,\n",
    "    device,\n",
    "    max_testing_examples=None, \n",
    "    counterfactual_evaluate=False,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict all data in dataset with a model and write the predictions to output_file_path.\n",
    "    :param dataset: a dataset with test examples\n",
    "    :param model: a trained model from model.py\n",
    "    :param output_file_path: a path where a .json file with predictions will be saved.\n",
    "    :param max_decoding_steps: after how many steps to force quit decoding\n",
    "    :param max_testing_examples: after how many examples to stop predicting, if None all examples will be evaluated\n",
    "    \"\"\"\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    \n",
    "    # read-in datasets\n",
    "    test_data, _ = dataset.get_dual_dataset()\n",
    "    data_iterator = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    eval_max_decoding_steps = max_decoding_steps\n",
    "    with open(output_file_path, mode='w') as outfile:\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #################\n",
    "            #\n",
    "            # Task-based\n",
    "            #\n",
    "            #################\n",
    "            exact_match_count = 0\n",
    "            example_count = 0\n",
    "            i = 0\n",
    "            for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "                    data_iterator=data_iterator, model=model, \n",
    "                    max_decoding_steps=max_decoding_steps, \n",
    "                    pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                    sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                    eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                    max_examples_to_evaluate=max_testing_examples, \n",
    "                    device=device\n",
    "            ):\n",
    "                i += 1\n",
    "                example_count += 1\n",
    "                accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                if accuracy == 100:\n",
    "                    exact_match_count += 1\n",
    "                output.append({\"input\": input_str_sequence, \"prediction\": output_str_sequence,\n",
    "                               \"target\": target_str_sequence,\n",
    "                               \"accuracy\": accuracy,\n",
    "                               \"exact_match\": True if accuracy == 100 else False,\n",
    "                               \"position_accuracy\": aux_acc_target})      \n",
    "            exact_match = (exact_match_count/example_count)*100.0\n",
    "            logger.info(\" Task Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "            if not counterfactual_evaluate:\n",
    "                logger.info(\"Wrote predictions for {} examples.\".format(i))\n",
    "                json.dump(output, outfile, indent=4)\n",
    "                return output_file_path\n",
    "\n",
    "            #################\n",
    "            #\n",
    "            # Counterfactual\n",
    "            #\n",
    "            #################\n",
    "            logger.info(\" Starting our counterfactual analysis ... \")\n",
    "            exact_match_count = 0\n",
    "            example_count = 0\n",
    "            for input_sequence, dual_input_sequence, intervene_attribute, intervene_time, \\\n",
    "                output_sequence, target_sequence, original_target_sequence, original_dual_target_str_sequence, \\\n",
    "                aux_acc_target in counterfactual_predict(\n",
    "                    data_iterator=data_iterator, model=model, \n",
    "                    low_model=low_model, low_model_cf=low_model_cf, hi_model=hi_model,\n",
    "                    max_decoding_steps=max_decoding_steps, \n",
    "                    eval_max_decoding_steps=eval_max_decoding_steps,\n",
    "                    pad_idx=dataset.target_vocabulary.pad_idx,\n",
    "                    sos_idx=dataset.target_vocabulary.sos_idx, \n",
    "                    eos_idx=dataset.target_vocabulary.eos_idx, \n",
    "                    max_examples_to_evaluate=max_testing_examples, \n",
    "                    device=device\n",
    "            ):\n",
    "                i += 1\n",
    "                example_count += 1\n",
    "                accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "                input_str_sequence = dataset.array_to_sentence(input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                input_str_sequence = input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                dual_input_str_sequence = dataset.array_to_sentence(dual_input_sequence[0].tolist(), vocabulary=\"input\")\n",
    "                dual_input_str_sequence = dual_input_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                \n",
    "                target_str_sequence = dataset.array_to_sentence(target_sequence[0].tolist(), vocabulary=\"target\")\n",
    "                original_target_str_sequence = dataset.array_to_sentence(original_target_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                original_dual_target_str_sequence = dataset.array_to_sentence(original_dual_target_str_sequence[0].tolist()[1:-1], vocabulary=\"target\")\n",
    "                target_str_sequence = target_str_sequence[1:-1]  # Get rid of <SOS> and <EOS>\n",
    "                output_str_sequence = dataset.array_to_sentence(output_sequence, vocabulary=\"target\")\n",
    "                if accuracy == 100:\n",
    "                    exact_match_count += 1\n",
    "                output.append({\"input\": input_str_sequence, \n",
    "                               \"dual_input\": dual_input_str_sequence,\n",
    "                               \"intervene_attribute\": intervene_attribute,\n",
    "                               \"intervene_time\": intervene_time,\n",
    "                               \"prediction\": output_str_sequence,\n",
    "                               \"target\": target_str_sequence,\n",
    "                               \"main_target\": original_target_str_sequence,\n",
    "                               \"dual_target\": original_dual_target_str_sequence,\n",
    "                               \"accuracy\": accuracy,\n",
    "                               \"exact_match\": True if accuracy == 100 else False,\n",
    "                               \"position_accuracy\": aux_acc_target})\n",
    "\n",
    "            logger.info(\"Wrote predictions for {} examples including counterfactual ones.\".format(i))\n",
    "            json.dump(output, outfile, indent=4)\n",
    "            exact_match = (exact_match_count/example_count)*100.0\n",
    "            logger.info(\" Counterfactual Evaluation Exact Match: %5.2f \" % (exact_match))\n",
    "        \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        assert False # we don't allow train in the evaluation script.\n",
    "    elif flags[\"mode\"] == \"test\":\n",
    "        logger.info(\"Loading all data into memory for evaluation...\")\n",
    "        logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "        data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "        assert os.path.exists(os.path.join(flags[\"data_directory\"], flags[\"input_vocab_path\"])) and os.path.exists(\n",
    "            os.path.join(flags[\"data_directory\"], flags[\"target_vocab_path\"])), \\\n",
    "            \"No vocabs found at {} and {}\".format(flags[\"input_vocab_path\"], flags[\"target_vocab_path\"])\n",
    "        splits = flags[\"splits\"].split(\",\")\n",
    "        for split in splits:\n",
    "            logger.info(\"Loading {} dataset split...\".format(split))\n",
    "            \n",
    "            test_set = ReaSCANDataset(\n",
    "                data_json, flags[\"data_directory\"], split=split,\n",
    "                input_vocabulary_file=flags[\"input_vocab_path\"],\n",
    "                target_vocabulary_file=flags[\"target_vocab_path\"],\n",
    "                generate_vocabulary=False, k=flags[\"k\"]\n",
    "            )\n",
    "            test_set.read_dataset(\n",
    "                max_examples=flags[\"max_testing_examples\"],\n",
    "                simple_situation_representation=flags[\"simple_situation_representation\"]\n",
    "            )\n",
    "            logger.info(\"Done Loading {} dataset split.\".format(flags[\"split\"]))\n",
    "            logger.info(\"  Loaded {} examples.\".format(test_set.num_examples))\n",
    "            logger.info(\"  Input vocabulary size: {}\".format(test_set.input_vocabulary_size))\n",
    "            logger.info(\"  Most common input words: {}\".format(test_set.input_vocabulary.most_common(5)))\n",
    "            logger.info(\"  Output vocabulary size: {}\".format(test_set.target_vocabulary_size))\n",
    "            logger.info(\"  Most common target words: {}\".format(test_set.target_vocabulary.most_common(5)))\n",
    "            \n",
    "            # create modell based on our dataset.\n",
    "            model = Model(input_vocabulary_size=test_set.input_vocabulary_size,\n",
    "                          target_vocabulary_size=test_set.target_vocabulary_size,\n",
    "                          num_cnn_channels=test_set.image_channels,\n",
    "                          input_padding_idx=test_set.input_vocabulary.pad_idx,\n",
    "                          target_pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                          target_eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                          **flags)\n",
    "\n",
    "            # gpu setups\n",
    "            use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            # logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            model.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            We have two low model so that our computation is much faster.\n",
    "            \"\"\"\n",
    "            eval_max_decoding_steps = flags[\"max_decoding_steps\"] # we need to use this extended step to measure for eval.\n",
    "            low_model = ReaSCANMultiModalLSTMCompGraph(\n",
    "                 model,\n",
    "                 eval_max_decoding_steps,\n",
    "                 is_cf=False,\n",
    "                 cache_results=False\n",
    "            )\n",
    "            low_model_cf = ReaSCANMultiModalLSTMCompGraph(\n",
    "                 model,\n",
    "                 eval_max_decoding_steps,\n",
    "                 is_cf=True,\n",
    "                 cache_results=False\n",
    "            )\n",
    "\n",
    "            # create high level model for counterfactual training.\n",
    "            hi_model = get_counter_compgraph(\n",
    "                eval_max_decoding_steps,\n",
    "                cache_results=False\n",
    "            )\n",
    "            # logger.info(\"Finish loading both low and high models..\")\n",
    "            \n",
    "            # optimizer\n",
    "            # log_parameters(model)\n",
    "            \n",
    "            # Load model and vocabularies if resuming.\n",
    "            evaluate_checkpoint = flags[\"evaluate_checkpoint\"]\n",
    "            if flags[\"evaluate_checkpoint\"] == \"\":\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], \"model_best.pth.tar\")\n",
    "            else:\n",
    "                model_path = os.path.join(flags[\"resume_from_file\"], f\"checkpoint-{evaluate_checkpoint}.pth.tar\")\n",
    "            assert os.path.isfile(model_path), \"No checkpoint found at {}\".format(model_path)\n",
    "            logger.info(\"Loading checkpoint from file at '{}'\".format(model_path))\n",
    "            model.load_model(model_path)\n",
    "            start_iteration = model.trained_iterations\n",
    "            logger.info(\"Loaded checkpoint '{}' (iter {})\".format(model_path, start_iteration))\n",
    "            output_file_name = \"_\".join([split, flags[\"output_file_name\"]])\n",
    "            output_file_path = os.path.join(flags[\"resume_from_file\"], output_file_name)\n",
    "            logger.info(\"All results will be saved to '{}'\".format(output_file_path))\n",
    "            output_file = predict_and_save(\n",
    "                dataset=test_set, \n",
    "                model=model, low_model=low_model, low_model_cf=low_model_cf, hi_model=hi_model,\n",
    "                output_file_path=output_file_path, \n",
    "                device=device,\n",
    "                # we don't need counterfactual evaluation on generalization splits for now.\n",
    "                counterfactual_evaluate=True if split == \"dev\" or split == \"test\" else False,\n",
    "                **flags\n",
    "            )\n",
    "            logger.info(\"Saved predictions to {}\".format(output_file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(flags=input_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
