{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from decode_graphical_models import *\n",
    "from decode_abstract_models import *\n",
    "from seq2seq.ReaSCAN_dataset import *\n",
    "from seq2seq.helpers import *\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    data_iterator, \n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx, \n",
    "    sos_idx,\n",
    "    eos_idx, \n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loop over all data in data_iterator and predict until <EOS> token is reached.\n",
    "    :param data_iterator: iterator containing the data to predict\n",
    "    :param model: a trained model from model.py\n",
    "    :param max_decoding_steps: after how many steps to abort decoding\n",
    "    :param pad_idx: the padding idx of the target vocabulary\n",
    "    :param sos_idx: the start-of-sequence idx of the target vocabulary\n",
    "    :param eos_idx: the end-of-sequence idx of the target vocabulary\n",
    "    :param: max_examples_to_evaluate: after how many examples to break prediction, if none all are predicted\n",
    "    \"\"\"\n",
    "    # Disable dropout and other regularization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over the data.\n",
    "    i = 0\n",
    "    for step, batch in enumerate(data_iterator):\n",
    "        \n",
    "        i += 1\n",
    "        if max_examples_to_evaluate:\n",
    "            if i > max_examples_to_evaluate:\n",
    "                break\n",
    "        \n",
    "        # derivation_spec\n",
    "        # situation_spec\n",
    "        input_sequence, target_sequence, situation, \\\n",
    "            agent_positions, target_positions, \\\n",
    "            input_lengths, target_lengths, \\\n",
    "            dual_input_sequence, dual_target_sequence, dual_situation, \\\n",
    "            dual_agent_positions, dual_target_positions, \\\n",
    "            dual_input_lengths, dual_target_lengths = batch\n",
    "        \n",
    "        input_max_seq_lens = max(input_lengths)[0]\n",
    "        target_max_seq_lens = max(target_lengths)[0]\n",
    "        \n",
    "        input_sequence = input_sequence.to(device)\n",
    "        target_sequence = target_sequence.to(device)\n",
    "        situation = situation.to(device)\n",
    "        agent_positions = agent_positions.to(device)\n",
    "        target_positions = target_positions.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "\n",
    "        # We need to chunk\n",
    "        input_sequence = input_sequence[:,:input_max_seq_lens]\n",
    "        target_sequence = target_sequence[:,:target_max_seq_lens]\n",
    "        \n",
    "        # in the evaluation phase, i think we can actually\n",
    "        # use the model itself not the graphical model.\n",
    "        # ENCODE\n",
    "        encoded_image = model(\n",
    "            situations_input=situation,\n",
    "            tag=\"situation_encode\"\n",
    "        )\n",
    "        hidden, encoder_outputs = model(\n",
    "            commands_input=input_sequence, \n",
    "            commands_lengths=input_lengths,\n",
    "            tag=\"command_input_encode_no_dict\"\n",
    "        )\n",
    "\n",
    "        # DECODER INIT\n",
    "        hidden = model(\n",
    "            command_hidden=hidden,\n",
    "            tag=\"initialize_hidden\"\n",
    "        )\n",
    "        projected_keys_visual = model(\n",
    "            encoded_situations=encoded_image,\n",
    "            tag=\"projected_keys_visual\"\n",
    "        )\n",
    "        projected_keys_textual = model(\n",
    "            command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "            tag=\"projected_keys_textual\"\n",
    "        )\n",
    "        \n",
    "        # Iteratively decode the output.\n",
    "        output_sequence = []\n",
    "        contexts_situation = []\n",
    "        token = torch.tensor([sos_idx], dtype=torch.long, device=device)\n",
    "        decoding_iteration = 0\n",
    "        attention_weights_commands = []\n",
    "        attention_weights_situations = []\n",
    "        while token != eos_idx and decoding_iteration <= max_decoding_steps:\n",
    "            \n",
    "            (output, hidden) = model(\n",
    "                lstm_input_tokens_sorted=token,\n",
    "                lstm_hidden=hidden,\n",
    "                lstm_projected_keys_textual=projected_keys_textual,\n",
    "                lstm_commands_lengths=input_lengths,\n",
    "                lstm_projected_keys_visual=projected_keys_visual,\n",
    "                tag=\"_lstm_step_fxn\"\n",
    "            )\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            token = output.max(dim=-1)[1]\n",
    "\n",
    "            output_sequence.append(token.data[0].item())\n",
    "            decoding_iteration += 1\n",
    "\n",
    "        if output_sequence[-1] == eos_idx:\n",
    "            output_sequence.pop()\n",
    "        if model(tag=\"auxiliary_task\"):\n",
    "            pass\n",
    "        else:\n",
    "            auxiliary_accuracy_agent, auxiliary_accuracy_target = 0, 0\n",
    "        yield (input_sequence, output_sequence, target_sequence, auxiliary_accuracy_target)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(\"Predicted for {} examples.\".format(i))\n",
    "    logging.info(\"Done predicting in {} seconds.\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    data_iterator,\n",
    "    model, \n",
    "    max_decoding_steps, \n",
    "    pad_idx,\n",
    "    sos_idx,\n",
    "    eos_idx,\n",
    "    max_examples_to_evaluate,\n",
    "    device\n",
    "):\n",
    "    accuracies = []\n",
    "    target_accuracies = []\n",
    "    exact_match = 0\n",
    "    for input_sequence, output_sequence, target_sequence, aux_acc_target in predict(\n",
    "            data_iterator=data_iterator, model=model, max_decoding_steps=max_decoding_steps, pad_idx=pad_idx,\n",
    "            sos_idx=sos_idx, eos_idx=eos_idx, max_examples_to_evaluate=max_examples_to_evaluate, device=device):\n",
    "        accuracy = sequence_accuracy(output_sequence, target_sequence[0].tolist()[1:-1])\n",
    "        if accuracy == 100:\n",
    "            exact_match += 1\n",
    "        accuracies.append(accuracy)\n",
    "        target_accuracies.append(aux_acc_target)\n",
    "    return (float(np.mean(np.array(accuracies))), (exact_match / len(accuracies)) * 100,\n",
    "            float(np.mean(np.array(target_accuracies))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    data_path: str, \n",
    "    args,\n",
    "    data_directory: str, \n",
    "    generate_vocabularies: bool, \n",
    "    input_vocab_path: str,   \n",
    "    target_vocab_path: str, \n",
    "    embedding_dimension: int, \n",
    "    num_encoder_layers: int, \n",
    "    encoder_dropout_p: float,\n",
    "    encoder_bidirectional: bool, \n",
    "    training_batch_size: int, \n",
    "    test_batch_size: int, \n",
    "    max_decoding_steps: int,\n",
    "    num_decoder_layers: int, \n",
    "    decoder_dropout_p: float, \n",
    "    cnn_kernel_size: int, \n",
    "    cnn_dropout_p: float,\n",
    "    cnn_hidden_num_channels: int, \n",
    "    simple_situation_representation: bool, \n",
    "    decoder_hidden_size: int,\n",
    "    encoder_hidden_size: int, \n",
    "    learning_rate: float, \n",
    "    adam_beta_1: float, \n",
    "    adam_beta_2: float, \n",
    "    lr_decay: float,\n",
    "    lr_decay_steps: int, \n",
    "    resume_from_file: str, \n",
    "    max_training_iterations: int, \n",
    "    output_directory: str,\n",
    "    print_every: int, \n",
    "    evaluate_every: int, \n",
    "    conditional_attention: bool, \n",
    "    auxiliary_task: bool,\n",
    "    weight_target_loss: float, \n",
    "    attention_type: str, \n",
    "    k: int, \n",
    "    # counterfactual training arguments\n",
    "    run_name: str,\n",
    "    cf_mode: str,\n",
    "    cf_sample_p: float,\n",
    "    checkpoint_save_every: int,\n",
    "    include_cf_loss: bool,\n",
    "    include_task_loss: bool,\n",
    "    cf_loss_weight: float,\n",
    "    is_wandb: bool,\n",
    "    max_training_examples=None, \n",
    "    seed=42,\n",
    "    **kwargs\n",
    "):\n",
    "    \n",
    "    # we at least need to have one kind of loss.\n",
    "    logger.info(f\"LOSS CONFIG: include_task_loss={include_task_loss}, \"\n",
    "                f\"include_cf_loss={include_cf_loss} with cf_loss_weight = {cf_loss_weight}...\")\n",
    "\n",
    "    assert include_cf_loss or include_task_loss\n",
    "    cfg = locals().copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    from pathlib import Path\n",
    "    # the output directory name is generated on-the-fly.\n",
    "    run_name = f\"{run_name}_seed_{seed}_cf_loss_{include_cf_loss}_task_loss_{include_task_loss}_cf_weight_{cf_loss_weight}_cf_p_{cf_sample_p}\"\n",
    "    output_directory = os.path.join(output_directory, run_name)\n",
    "    cfg[\"output_directory\"] = output_directory\n",
    "    logger.info(f\"Create the output directory if not exist: {output_directory}\")\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # initialize w&b in the beginning.\n",
    "    if is_wandb:\n",
    "        logger.warning(\"Enabling wandb for tensorboard logging...\")\n",
    "        import wandb\n",
    "        run = wandb.init(\n",
    "            project=\"ReaSCAN-Causal\", \n",
    "            entity=\"wuzhengx\",\n",
    "            name=run_name,\n",
    "        )\n",
    "        wandb.config.update(args)\n",
    "    \n",
    "    logger.info(\"Loading all data into memory...\")\n",
    "    logger.info(f\"Reading dataset from file: {data_path}...\")\n",
    "    data_json = json.load(open(data_path, \"r\"))\n",
    "    \n",
    "    logger.info(\"Loading Training set...\")\n",
    "    training_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"train\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=k\n",
    "    )\n",
    "    training_set.read_dataset(\n",
    "        max_examples=max_training_examples,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "    logger.info(\"Done Loading Training set.\")\n",
    "    logger.info(\"  Loaded {} training examples.\".format(training_set.num_examples))\n",
    "    logger.info(\"  Input vocabulary size training set: {}\".format(training_set.input_vocabulary_size))\n",
    "    logger.info(\"  Most common input words: {}\".format(training_set.input_vocabulary.most_common(5)))\n",
    "    logger.info(\"  Output vocabulary size training set: {}\".format(training_set.target_vocabulary_size))\n",
    "    logger.info(\"  Most common target words: {}\".format(training_set.target_vocabulary.most_common(5)))\n",
    "\n",
    "    if generate_vocabularies:\n",
    "        training_set.save_vocabularies(input_vocab_path, target_vocab_path)\n",
    "        logger.info(\"Saved vocabularies to {} for input and {} for target.\".format(input_vocab_path, target_vocab_path))\n",
    "\n",
    "    logger.info(\"Loading Dev. set...\")\n",
    "    test_set = ReaSCANDataset(\n",
    "        data_json, data_directory, split=\"dev\",\n",
    "        input_vocabulary_file=input_vocab_path,\n",
    "        target_vocabulary_file=target_vocab_path,\n",
    "        generate_vocabulary=generate_vocabularies, k=0\n",
    "    )\n",
    "    test_set.read_dataset(\n",
    "        max_examples=None,\n",
    "        simple_situation_representation=simple_situation_representation\n",
    "    )\n",
    "\n",
    "    # Shuffle the test set to make sure that if we only evaluate max_testing_examples we get a random part of the set.\n",
    "    test_set.shuffle_data()\n",
    "    logger.info(\"Done Loading Dev. set.\")\n",
    "    \n",
    "    # create modell based on our dataset.\n",
    "    model = Model(input_vocabulary_size=training_set.input_vocabulary_size,\n",
    "                  target_vocabulary_size=training_set.target_vocabulary_size,\n",
    "                  num_cnn_channels=training_set.image_channels,\n",
    "                  input_padding_idx=training_set.input_vocabulary.pad_idx,\n",
    "                  target_pad_idx=training_set.target_vocabulary.pad_idx,\n",
    "                  target_eos_idx=training_set.target_vocabulary.eos_idx,\n",
    "                  **cfg)\n",
    "    \n",
    "    # gpu setups\n",
    "    use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(f\"device: {device}, and we recognize {n_gpu} gpu(s) in total.\")\n",
    "\n",
    "    # optimizer\n",
    "    log_parameters(model)\n",
    "    trainable_parameters = [parameter for parameter in model.parameters() if parameter.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_parameters, lr=learning_rate, betas=(adam_beta_1, adam_beta_2))\n",
    "    scheduler = LambdaLR(optimizer,\n",
    "                         lr_lambda=lambda t: lr_decay ** (t / lr_decay_steps))\n",
    "    \n",
    "    \n",
    "    # Load model and vocabularies if resuming.\n",
    "    start_iteration = 1\n",
    "    best_iteration = 1\n",
    "    best_accuracy = 0\n",
    "    best_exact_match = -99\n",
    "    best_loss = float('inf')\n",
    "    if resume_from_file:\n",
    "        assert os.path.isfile(resume_from_file), \"No checkpoint found at {}\".format(resume_from_file)\n",
    "        logger.info(\"Loading checkpoint from file at '{}'\".format(resume_from_file))\n",
    "        optimizer_state_dict = model.load_model(resume_from_file)\n",
    "        optimizer.load_state_dict(optimizer_state_dict)\n",
    "        start_iteration = model.trained_iterations\n",
    "        logger.info(\"Loaded checkpoint '{}' (iter {})\".format(resume_from_file, start_iteration))\n",
    "    \n",
    "    # Loading dataset and preprocessing a bit.\n",
    "    train_data, _ = training_set.get_dual_dataset()\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.training_batch_size)\n",
    "    test_data, _ = test_set.get_dual_dataset()\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args.test_batch_size, shuffle=False)\n",
    "    \n",
    "    if use_cuda and n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # graphical model\n",
    "    train_max_decoding_steps = int(training_set.get_max_seq_length_target())\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    logger.info(f\"MAX_DECODING_STEPS for Training: {train_max_decoding_steps}\")\n",
    "    logger.info(f\"==== WARNING ====\")\n",
    "    \n",
    "    \"\"\"\n",
    "    We have two low model so that our computation is much faster.\n",
    "    \"\"\"\n",
    "    low_model = ReaSCANMultiModalLSTMCompGraph(\n",
    "         model,\n",
    "         train_max_decoding_steps,\n",
    "         is_cf=False,\n",
    "         cache_results=False\n",
    "    )\n",
    "    low_model_cf = ReaSCANMultiModalLSTMCompGraph(\n",
    "         model,\n",
    "         train_max_decoding_steps,\n",
    "         is_cf=True,\n",
    "         cache_results=False\n",
    "    )\n",
    "\n",
    "    # create high level model for counterfactual training.\n",
    "    hi_model = get_counter_compgraph(\n",
    "        train_max_decoding_steps,\n",
    "        cache_results=False\n",
    "    )\n",
    "    logger.info(\"Finish loading both low and high models..\")\n",
    "    \n",
    "    logger.info(\"Training starts..\")\n",
    "    training_iteration = start_iteration\n",
    "    cf_sample_per_batch_in_percentage = cf_sample_p\n",
    "    logger.info(f\"Setting cf_sample_per_batch_in_percentage = {cf_sample_per_batch_in_percentage}\")\n",
    "    while training_iteration < max_training_iterations:\n",
    "\n",
    "        # Shuffle the dataset and loop over it.\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # main batch\n",
    "            input_batch, target_batch, situation_batch, \\\n",
    "                agent_positions_batch, target_positions_batch, \\\n",
    "                input_lengths_batch, target_lengths_batch, \\\n",
    "                dual_input_batch, dual_target_batch, dual_situation_batch, \\\n",
    "                dual_agent_positions_batch, dual_target_positions_batch, \\\n",
    "                dual_input_lengths_batch, dual_target_lengths_batch = batch\n",
    "\n",
    "            is_best = False\n",
    "            model.train()\n",
    "            \n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            situation_batch = situation_batch.to(device)\n",
    "            agent_positions_batch = agent_positions_batch.to(device)\n",
    "            target_positions_batch = target_positions_batch.to(device)\n",
    "            input_lengths_batch = input_lengths_batch.to(device)\n",
    "            target_lengths_batch = target_lengths_batch.to(device)\n",
    "            \n",
    "            dual_input_max_seq_lens = max(dual_input_lengths_batch)[0]\n",
    "            dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "            dual_input_batch = dual_input_batch.to(device)\n",
    "            dual_target_batch = dual_target_batch.to(device)\n",
    "            dual_situation_batch = dual_situation_batch.to(device)\n",
    "            dual_agent_positions_batch = dual_agent_positions_batch.to(device)\n",
    "            dual_target_positions_batch = dual_target_positions_batch.to(device)\n",
    "            dual_input_lengths_batch = dual_input_lengths_batch.to(device)\n",
    "            dual_target_lengths_batch = dual_target_lengths_batch.to(device)\n",
    "            \n",
    "            task_loss = None\n",
    "            cf_loss = None\n",
    "            \n",
    "            # Main task loss first!\n",
    "            '''\n",
    "            We calculate this loss using the pytorch module, \n",
    "            as it is much quicker.\n",
    "            '''\n",
    "            \n",
    "            # we will calculate these anyway\n",
    "            # even if we don't prop loss, we need to evaluate.\n",
    "            forward_main_low = {\n",
    "                \"commands_input\": input_batch, \n",
    "                \"commands_lengths\": input_lengths_batch,\n",
    "                \"situations_input\": situation_batch,\n",
    "                \"target_batch\": target_batch,\n",
    "                \"target_lengths\": target_lengths_batch,\n",
    "            }\n",
    "            g_forward_main_low = GraphInput(forward_main_low, batched=True, batch_dim=0, cache_results=False)\n",
    "            target_scores = low_model.compute(g_forward_main_low)\n",
    "            task_loss = model(\n",
    "                loss_target_scores=target_scores, \n",
    "                loss_target_batch=target_batch,\n",
    "                tag=\"loss\"\n",
    "            )\n",
    "            if use_cuda and n_gpu > 1:\n",
    "                task_loss = task_loss.mean() # mean() to average on multi-gpu.\n",
    "            \n",
    "            # Calculate intervention loss.\n",
    "            \"\"\"\n",
    "            For the sake of quick training, for a single batch,\n",
    "            we select the same attribute to intervenen on:\n",
    "            0: x\n",
    "            1: y\n",
    "            2: orientation\n",
    "            \"\"\"\n",
    "            intervene_attribute = random.choice([0,1,2])\n",
    "            input_max_seq_lens = max(input_lengths_batch)[0]\n",
    "            target_max_seq_lens = max(target_lengths_batch)[0]\n",
    "            dual_target_max_seq_lens = max(dual_target_lengths_batch)[0]\n",
    "            intervene_time = random.randint(1, min(min(input_lengths_batch)[0], min(dual_target_lengths_batch)[0])-2) # we get rid of SOS and EOS tokens\n",
    "\n",
    "            #####################\n",
    "            #\n",
    "            # high data start\n",
    "            #\n",
    "            #####################\n",
    "\n",
    "            # to have high quality high data, we need to iterate through each example.\n",
    "            batch_size = agent_positions_batch.size(0)\n",
    "            intervened_target_batch = []\n",
    "            intervened_target_lengths_batch = []\n",
    "            cf_sample_per_batch = int(batch_size*cf_sample_per_batch_in_percentage)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ## main\n",
    "                main_high = {\n",
    "                    \"agent_positions_batch\": agent_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                    \"target_positions_batch\": target_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                }\n",
    "                g_main_high = GraphInput(main_high, batched=True, batch_dim=0, cache_results=False)\n",
    "                ## dual\n",
    "                dual_high = {\n",
    "                    \"agent_positions_batch\": dual_agent_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                    \"target_positions_batch\": dual_target_positions_batch[i:i+1].unsqueeze(dim=-1),\n",
    "                }\n",
    "                g_dual_high = GraphInput(dual_high, batched=True, batch_dim=0, cache_results=False)\n",
    "                dual_target_length = dual_target_lengths_batch[i][0].tolist()\n",
    "\n",
    "                # get the duel high state\n",
    "                main_high_hidden = hi_model.compute_node(f\"s{intervene_time}\", g_main_high)\n",
    "                dual_high_hidden = hi_model.compute_node(f\"s{intervene_time}\", g_dual_high)\n",
    "                # only intervene on an selected attribute.\n",
    "                # main_high_hidden[:,:] = dual_high_hidden[:,:]\n",
    "                high_interv = Intervention(\n",
    "                    g_main_high, {f\"s{intervene_time}\": main_high_hidden}, \n",
    "                    cache_results=False,\n",
    "                    cache_base_results=False,\n",
    "                    batched=True\n",
    "                )\n",
    "                intervened_outputs = []\n",
    "                for j in range(0, train_max_decoding_steps-2): # we only have this many steps can intervened.\n",
    "                    _, intervened_output = hi_model.intervene_node(f\"s{j+1}\", high_interv)\n",
    "                    if intervened_output[0,3] == 0 or j == train_max_decoding_steps-3:\n",
    "                        # we need to record the length and early stop\n",
    "                        intervened_outputs = [1] + intervened_outputs + [2]\n",
    "                        intervened_outputs = torch.tensor(intervened_outputs).long()\n",
    "                        intervened_length = len(intervened_outputs)\n",
    "                        # we need to confine the longest length!\n",
    "                        if intervened_length > train_max_decoding_steps:\n",
    "                            intervened_length = train_max_decoding_steps\n",
    "                            intervened_outputs = intervened_outputs[:train_max_decoding_steps]\n",
    "                        intervened_target_batch += [intervened_outputs]\n",
    "                        intervened_target_lengths_batch += [intervened_length]\n",
    "                        break\n",
    "                    else:\n",
    "                        intervened_outputs.append(intervened_output[0,3].tolist())\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # we need to pad to the longest ones.\n",
    "                intervened_target_batch[i] = torch.cat([\n",
    "                    intervened_target_batch[i],\n",
    "                    torch.zeros(int(train_max_decoding_steps-intervened_target_lengths_batch[i]), dtype=torch.long)], dim=0\n",
    "                )\n",
    "            intervened_target_lengths_batch = torch.tensor(intervened_target_lengths_batch).long().unsqueeze(dim=-1)\n",
    "            intervened_target_batch = torch.stack(intervened_target_batch, dim=0)\n",
    "            # we need to truncate if this is longer than the maximum target length\n",
    "            # of original target length.\n",
    "\n",
    "            # intervened data.\n",
    "            intervened_target_batch = intervened_target_batch.to(device)\n",
    "            intervened_target_lengths_batch = intervened_target_lengths_batch.to(device)\n",
    "\n",
    "            #####################\n",
    "            #\n",
    "            # high data end\n",
    "            #\n",
    "            #####################\n",
    "\n",
    "\n",
    "            #####################\n",
    "            #\n",
    "            # low data start\n",
    "            #\n",
    "            #####################\n",
    "            ## main\n",
    "            \"\"\"\n",
    "            Low level data requires GPU forwarding.\n",
    "            \"\"\" \n",
    "            # low level intervention.\n",
    "            # Major update:\n",
    "            # We need to batch these operations.\n",
    "            intervened_scores_batch = []\n",
    "            # here, we only care where those intervened target is different\n",
    "            # from the original main target.\n",
    "            idx_generator = []\n",
    "            for i in range(batch_size):\n",
    "                match_target_intervened = intervened_target_batch[i,:intervened_target_lengths_batch[i,0]]\n",
    "                match_target_main = target_batch[i,:target_lengths_batch[i,0]]\n",
    "                is_bad_intervened = torch.equal(match_target_intervened, match_target_main)\n",
    "                if is_bad_intervened:\n",
    "                    continue # we need to skip these.\n",
    "                else:\n",
    "                    idx_generator += [i]\n",
    "\n",
    "            # Let us get rid of antra, using a very simple for loop\n",
    "            # to do this intervention.\n",
    "            idx_selected = []\n",
    "            if len(idx_generator) > 0:\n",
    "                # overwrite a bit.\n",
    "                cf_sample_per_batch = min(cf_sample_per_batch, len(idx_generator))\n",
    "                idx_selected = random.sample(idx_generator, k=cf_sample_per_batch)\n",
    "                intervened_target_batch_selected = []\n",
    "\n",
    "                # filter based on selection all together!\n",
    "                situation_batch = situation_batch[idx_selected]\n",
    "                input_batch = input_batch[idx_selected]\n",
    "                input_lengths_batch = input_lengths_batch[idx_selected]\n",
    "                dual_situation_batch = dual_situation_batch[idx_selected]\n",
    "                dual_input_batch = dual_input_batch[idx_selected]\n",
    "                dual_input_lengths_batch = dual_input_lengths_batch[idx_selected]\n",
    "                dual_target_batch = dual_target_batch[idx_selected]\n",
    "                intervened_target_lengths_batch = intervened_target_lengths_batch[idx_selected]\n",
    "                intervened_target_batch = intervened_target_batch[idx_selected]\n",
    "\n",
    "                # we use the main hidden to track.\n",
    "                encoded_image = model(\n",
    "                    situations_input=situation_batch,\n",
    "                    tag=\"situation_encode\"\n",
    "                )\n",
    "                hidden, encoder_outputs = model(\n",
    "                    commands_input=input_batch, \n",
    "                    commands_lengths=input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict\"\n",
    "                )\n",
    "\n",
    "                main_hidden = model(\n",
    "                    command_hidden=hidden,\n",
    "                    tag=\"initialize_hidden\"\n",
    "                )\n",
    "                projected_keys_visual = model(\n",
    "                    encoded_situations=encoded_image,\n",
    "                    tag=\"projected_keys_visual\"\n",
    "                )\n",
    "                projected_keys_textual = model(\n",
    "                    command_encoder_outputs=encoder_outputs[\"encoder_outputs\"],\n",
    "                    tag=\"projected_keys_textual\"\n",
    "                )\n",
    "\n",
    "                # dual setup.\n",
    "                dual_input_batch = dual_input_batch[:,:dual_input_max_seq_lens]\n",
    "                dual_target_batch = dual_target_batch[:,:dual_target_max_seq_lens]\n",
    "                dual_encoded_image = model(\n",
    "                    situations_input=dual_situation_batch,\n",
    "                    tag=\"situation_encode\"\n",
    "                )\n",
    "                dual_hidden, dual_encoder_outputs = model(\n",
    "                    commands_input=dual_input_batch, \n",
    "                    commands_lengths=dual_input_lengths_batch,\n",
    "                    tag=\"command_input_encode_no_dict\"\n",
    "                )\n",
    "\n",
    "                dual_hidden = model(\n",
    "                    command_hidden=dual_hidden,\n",
    "                    tag=\"initialize_hidden\"\n",
    "                )\n",
    "                dual_projected_keys_visual = model(\n",
    "                    encoded_situations=dual_encoded_image,\n",
    "                    tag=\"projected_keys_visual\"\n",
    "                )\n",
    "                dual_projected_keys_textual = model(\n",
    "                    command_encoder_outputs=dual_encoder_outputs[\"encoder_outputs\"],\n",
    "                    tag=\"projected_keys_textual\"\n",
    "                )\n",
    "\n",
    "                # get the intercepted dual hidden states.\n",
    "                for j in range(intervene_time):\n",
    "                    (_, dual_hidden) = model(\n",
    "                        lstm_input_tokens_sorted=dual_target_batch[:,j],\n",
    "                        lstm_hidden=dual_hidden,\n",
    "                        lstm_projected_keys_textual=dual_projected_keys_textual,\n",
    "                        lstm_commands_lengths=dual_input_lengths_batch,\n",
    "                        lstm_projected_keys_visual=dual_projected_keys_visual,\n",
    "                        tag=\"_lstm_step_fxn\"\n",
    "                    )\n",
    "\n",
    "                # main intervene for loop.\n",
    "                cf_hidden = main_hidden\n",
    "                cf_outputs = []\n",
    "                for j in range(intervened_target_batch.shape[1]):\n",
    "                    if j >= intervene_time:\n",
    "                        # s_idx = intervene_attribute*25\n",
    "                        # e_idx = intervene_attribute*25+25\n",
    "                        # cf_hidden[0][:,s_idx:e_idx] = dual_hidden[0][:,s_idx:e_idx] # only swap out this part.\n",
    "                        # cf_hidden[1][:,s_idx:e_idx] = dual_hidden[1][:,s_idx:e_idx] # only swap out this part.\n",
    "                        (cf_output, dual_hidden) = model(\n",
    "                            lstm_input_tokens_sorted=intervened_target_batch[:,j],\n",
    "                            lstm_hidden=dual_hidden,\n",
    "                            lstm_projected_keys_textual=dual_projected_keys_textual,\n",
    "                            lstm_commands_lengths=dual_input_lengths_batch,\n",
    "                            lstm_projected_keys_visual=dual_projected_keys_visual,\n",
    "                            tag=\"_lstm_step_fxn\"\n",
    "                        )\n",
    "                    else:\n",
    "                        (cf_output, cf_hidden) = model(\n",
    "                            lstm_input_tokens_sorted=intervened_target_batch[:,j],\n",
    "                            lstm_hidden=cf_hidden,\n",
    "                            lstm_projected_keys_textual=projected_keys_textual,\n",
    "                            lstm_commands_lengths=input_lengths_batch,\n",
    "                            lstm_projected_keys_visual=projected_keys_visual,\n",
    "                            tag=\"_lstm_step_fxn\"\n",
    "                        )\n",
    "                    # record the output for loss calculation.\n",
    "                    cf_output = cf_output.unsqueeze(0)\n",
    "                    cf_outputs += [cf_output]\n",
    "                cf_outputs = torch.cat(cf_outputs, dim=0)\n",
    "                intervened_scores_batch = cf_outputs.transpose(0, 1) # [batch_size, max_target_seq_length, target_vocabulary_size]\n",
    "\n",
    "                # Counterfactual loss\n",
    "                intervened_scores_batch = F.log_softmax(intervened_scores_batch, dim=-1)\n",
    "                cf_loss = model(\n",
    "                    loss_target_scores=intervened_scores_batch, \n",
    "                    loss_target_batch=intervened_target_batch,\n",
    "                    tag=\"loss\"\n",
    "                )\n",
    "                if use_cuda and n_gpu > 1:\n",
    "                    cf_loss = cf_loss.mean() # mean() to average on multi-gpu.\n",
    "\n",
    "            #####################\n",
    "            #\n",
    "            # low data end\n",
    "            #\n",
    "            #####################  \n",
    "\n",
    "            # combined two losses.\n",
    "            if include_task_loss:\n",
    "                loss = task_loss\n",
    "                if include_cf_loss:\n",
    "                    if cf_loss:\n",
    "                        loss += cf_loss_weight*cf_loss\n",
    "            else:\n",
    "                assert cf_loss != None\n",
    "                loss = cf_loss\n",
    "\n",
    "            # Backward pass and update model parameters.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model(\n",
    "                is_best=is_best,\n",
    "                tag=\"update_state\"\n",
    "            )\n",
    "            \n",
    "            # Print current metrics.\n",
    "            if training_iteration % print_every == 0:\n",
    "                if auxiliary_task:\n",
    "                    pass\n",
    "                else:\n",
    "                    auxiliary_accuracy_target = 0.\n",
    "                # main task evaluation\n",
    "                accuracy, exact_match = model(\n",
    "                    loss_target_scores=target_scores, \n",
    "                    loss_target_batch=target_batch,\n",
    "                    tag=\"get_metrics\"\n",
    "                )\n",
    "                # cf evaluation\n",
    "                if cf_loss:\n",
    "                    cf_accuracy, cf_exact_match = model(\n",
    "                        loss_target_scores=intervened_scores_batch, \n",
    "                        loss_target_batch=intervened_target_batch,\n",
    "                        tag=\"get_metrics\"\n",
    "                    )\n",
    "                else:\n",
    "                    cf_loss, cf_accuracy, cf_exact_match = 0.0, 0.0, 0.0\n",
    "                learning_rate = scheduler.get_lr()[0]\n",
    "                logger.info(\"Iteration %08d, task loss %8.4f, cf loss %8.4f, accuracy %5.2f, exact match %5.2f, \"\n",
    "                            \"cf count %03d, cf accuracy %5.2f, cf exact match %5.2f,\"\n",
    "                            \" learning_rate %.5f\" % (\n",
    "                                training_iteration, task_loss, cf_loss, accuracy, exact_match,\n",
    "                                len(idx_selected), cf_accuracy, cf_exact_match,\n",
    "                                learning_rate,\n",
    "                            ))\n",
    "                # logging to wandb.\n",
    "                if is_wandb:\n",
    "                    wandb.log({'training_iteration': training_iteration})\n",
    "                    wandb.log({'task_loss': task_loss})\n",
    "                    wandb.log({'task_accuracy': accuracy})\n",
    "                    wandb.log({'task_exact_match': exact_match})\n",
    "                    if cf_loss and len(idx_selected) != 0:\n",
    "                        wandb.log({'counterfactual_loss': cf_loss})\n",
    "                        wandb.log({'counterfactual count': len(idx_selected)})\n",
    "                        wandb.log({'counterfactual_accuracy': cf_accuracy})\n",
    "                        wandb.log({'counterfactual_exact_match': cf_exact_match})\n",
    "                    wandb.log({'learning_rate': learning_rate})\n",
    "            # Evaluate on test set.\n",
    "            \"\"\"\n",
    "            CAVEATS: we only evaluate with the main task loss for now.\n",
    "            It will take too long to evaluate counterfactually, so we\n",
    "            exclude it now from training.\n",
    "            \n",
    "            TODO: add back in the cf evaluation as well if it is efficient!\n",
    "            \"\"\"\n",
    "            if training_iteration % evaluate_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    logger.info(\"Evaluating..\")\n",
    "                    accuracy, exact_match, target_accuracy = evaluate(\n",
    "                        test_dataloader, model=model,\n",
    "                        max_decoding_steps=max_decoding_steps, pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                        sos_idx=test_set.target_vocabulary.sos_idx,\n",
    "                        eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                        max_examples_to_evaluate=kwargs[\"max_testing_examples\"],\n",
    "                        device=device\n",
    "                    )\n",
    "                    logger.info(\"  Evaluation Accuracy: %5.2f Exact Match: %5.2f \"\n",
    "                                \" Target Accuracy: %5.2f\" % (accuracy, exact_match, target_accuracy))\n",
    "                    if exact_match > best_exact_match:\n",
    "                        is_best = True\n",
    "                        best_accuracy = accuracy\n",
    "                        best_exact_match = exact_match\n",
    "                        model(\n",
    "                            accuracy=accuracy, exact_match=exact_match, \n",
    "                            is_best=is_best,\n",
    "                            tag=\"update_state\"\n",
    "                        )\n",
    "                    file_name = f\"checkpoint-{training_iteration}.pth.tar\"\n",
    "                    model.save_checkpoint(file_name=file_name, is_best=is_best,\n",
    "                                          optimizer_state_dict=optimizer.state_dict())\n",
    "            # if training_iteration % checkpoint_save_every == 0:\n",
    "            #     file_name = f\"checkpoint-{training_iteration}.pth.tar\"\n",
    "            #     model.save_checkpoint(file_name=file_name, is_best=False,\n",
    "            #                           optimizer_state_dict=optimizer.state_dict())\n",
    "            training_iteration += 1\n",
    "            if training_iteration > max_training_iterations:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags, args):\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "    \n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "        \n",
    "    # Some checks on the flags\n",
    "    if not flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "        \n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "        \n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"data-compositional-splits.txt\")\n",
    "    # quick check and fail fast!\n",
    "    assert os.path.exists(data_path), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "        data_path)\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        train(data_path=data_path, args=args, **flags)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        is_jupyter = True\n",
    "        args.max_training_examples = 10\n",
    "        args.max_testing_examples = 1\n",
    "        args.max_training_iterations = 5\n",
    "        args.print_every = 1\n",
    "        args.evaluate_every = 1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    input_flags = vars(args)\n",
    "    main(input_flags, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
