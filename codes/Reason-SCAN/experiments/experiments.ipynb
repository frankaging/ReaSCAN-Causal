{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1. Syntactic Dependency Analysis of gSCAN Demo Data\n",
    "Based on current outline of gSCAN, we suspect that gSCAN is not grounded syntactically. This prevents models trained with gSCAN have true reasoning powers and true systematicity. All these codes are run against the original codebase released by authors of gSCAN (i.e., no new codes added in gSCAN), which ensures fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '../multimodal_seq2seq_gSCAN/'))\n",
    "import random\n",
    "import copy \n",
    "\n",
    "from seq2seq.gSCAN_dataset import GroundedScanDataset\n",
    "from seq2seq.model import Model\n",
    "from seq2seq.train import train\n",
    "from seq2seq.predict import predict_and_save\n",
    "from tqdm import tqdm, trange\n",
    "from GroundedScan.dataset import GroundedScan\n",
    "\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from seq2seq.gSCAN_dataset import Vocabulary\n",
    "from seq2seq.helpers import sequence_accuracy\n",
    "from experiments_utils import *\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "if use_cuda:\n",
    "    logger.info(\"Using CUDA.\")\n",
    "    logger.info(\"Cuda version: {}\".format(torch.version.cuda))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on demo data to make sure the pipeline is running correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'train',\n",
       " 'output_directory': './syntactic_dependency-results/',\n",
       " 'resume_from_file': '',\n",
       " 'split': 'test',\n",
       " 'data_directory': '../multimodal_seq2seq_gSCAN/data/demo_dataset/',\n",
       " 'input_vocab_path': 'training_input_vocab.txt',\n",
       " 'target_vocab_path': 'training_target_vocab.txt',\n",
       " 'generate_vocabularies': True,\n",
       " 'training_batch_size': 50,\n",
       " 'k': 0,\n",
       " 'test_batch_size': 1,\n",
       " 'max_training_examples': None,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_decay': 0.9,\n",
       " 'lr_decay_steps': 20000,\n",
       " 'adam_beta_1': 0.9,\n",
       " 'adam_beta_2': 0.999,\n",
       " 'print_every': 100,\n",
       " 'evaluate_every': 1000,\n",
       " 'max_training_iterations': 1000,\n",
       " 'weight_target_loss': 0.3,\n",
       " 'max_testing_examples': None,\n",
       " 'splits': 'test',\n",
       " 'max_decoding_steps': 30,\n",
       " 'output_file_name': 'predict.json',\n",
       " 'simple_situation_representation': True,\n",
       " 'cnn_hidden_num_channels': 50,\n",
       " 'cnn_kernel_size': 7,\n",
       " 'cnn_dropout_p': 0.1,\n",
       " 'auxiliary_task': False,\n",
       " 'embedding_dimension': 25,\n",
       " 'num_encoder_layers': 1,\n",
       " 'encoder_hidden_size': 100,\n",
       " 'encoder_dropout_p': 0.3,\n",
       " 'encoder_bidirectional': True,\n",
       " 'num_decoder_layers': 1,\n",
       " 'attention_type': 'bahdanau',\n",
       " 'decoder_dropout_p': 0.3,\n",
       " 'decoder_hidden_size': 100,\n",
       " 'conditional_attention': True,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_flags = vars(get_gSCAN_parser().parse_args(args=['--mode', 'train', \n",
    "                                           '--data_directory', '../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                                           '--output_directory', './syntactic_dependency-results/', \n",
    "                                           '--attention_type', 'bahdanau', \n",
    "                                           '--max_training_iterations', '1000',\n",
    "                                           '--generate_vocabularies']))\n",
    "# input_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-19 02:45 Initializing dummy gSCAN dataset for adverserial experiments...\n",
      "2021-02-19 02:45 Loading vocabularies...\n",
      "2021-02-19 02:45 Done loading vocabularies.\n"
     ]
    }
   ],
   "source": [
    "demo_data_path = os.path.join('../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                              \"dataset.txt\")\n",
    "preprocessor = DummyGroundedScanDataset(demo_data_path, '../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                                        input_vocabulary_file=\"training_input_vocab.txt\", \n",
    "                                        target_vocabulary_file=\"training_target_vocab.txt\",\n",
    "                                        generate_vocabulary=False,\n",
    "                                        k=0)\n",
    "# get a single example\n",
    "demo_dataset = GroundedScan.load_dataset_from_file(demo_data_path, save_directory='./syntactic_dependency-results/', k=0)\n",
    "raw_example = None\n",
    "for _, example in enumerate(demo_dataset.get_examples_with_image('train', True)):\n",
    "    raw_example = example\n",
    "    break\n",
    "single_example = preprocessor.process(raw_example)\n",
    "# create the model\n",
    "model = Model(input_vocabulary_size=preprocessor.input_vocabulary_size,\n",
    "              target_vocabulary_size=preprocessor.target_vocabulary_size,\n",
    "              num_cnn_channels=preprocessor.image_channels,\n",
    "              input_padding_idx=preprocessor.input_vocabulary.pad_idx,\n",
    "              target_pad_idx=preprocessor.target_vocabulary.pad_idx,\n",
    "              target_eos_idx=preprocessor.target_vocabulary.eos_idx,\n",
    "              **input_flags)\n",
    "model = model.cuda() if use_cuda else model\n",
    "_ = model.load_model(\"./syntactic_dependency-results/model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['walk', 'to', 'a', 'red', 'circle']\n",
      "true: ['turn right', 'walk']\n",
      "pred: ['turn right', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# regular input\n",
    "print(\"in:\", raw_example['input_command'])\n",
    "print(\"true:\", raw_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "single_example = preprocessor.process(raw_example)\n",
    "output = predict_single(single_example, model=model, \n",
    "                        max_decoding_steps=30, \n",
    "                        pad_idx=preprocessor.target_vocabulary.pad_idx, \n",
    "                        sos_idx=preprocessor.target_vocabulary.sos_idx,\n",
    "                        eos_idx=preprocessor.target_vocabulary.eos_idx, \n",
    "                        device=device)\n",
    "pred_command = preprocessor.array_to_sentence(output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", pred_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['to', 'a', 'circle', 'red', 'walk']\n",
      "true: ['turn right', 'walk']\n",
      "pred: ['turn right', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# corrupted input\n",
    "corrupt_example = make_corrupt_example(raw_example)\n",
    "print(\"in:\", corrupt_example['input_command'])\n",
    "print(\"true:\", corrupt_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "corrupt_single_example = preprocessor.process(corrupt_example)\n",
    "corrupt_output = predict_single(corrupt_single_example, model=model, \n",
    "                                max_decoding_steps=30, \n",
    "                                pad_idx=preprocessor.target_vocabulary.pad_idx, \n",
    "                                sos_idx=preprocessor.target_vocabulary.sos_idx,\n",
    "                                eos_idx=preprocessor.target_vocabulary.eos_idx, \n",
    "                                device=device)\n",
    "corrupt_pred_command = preprocessor.array_to_sentence(corrupt_output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", corrupt_pred_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wow!** even with very overfitted example (trained with very small demo), we can see that if we corrupt the input, the output command stays the same! this is a strong evidence that current gSCAN does not push neural models to acquire grounding skills over world knowledge at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2. Syntactic Dependency Analysis of gSCAN Compositional Split Data (Actual Data in the gSCAN Paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'test',\n",
       " 'output_directory': './syntactic_dependency_compositional_splits-results/',\n",
       " 'resume_from_file': '',\n",
       " 'split': 'test',\n",
       " 'data_directory': '../multimodal_seq2seq_gSCAN/data/compositional_splits/',\n",
       " 'input_vocab_path': 'training_input_vocab.txt',\n",
       " 'target_vocab_path': 'training_target_vocab.txt',\n",
       " 'generate_vocabularies': False,\n",
       " 'training_batch_size': 50,\n",
       " 'k': 0,\n",
       " 'test_batch_size': 1,\n",
       " 'max_training_examples': None,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_decay': 0.9,\n",
       " 'lr_decay_steps': 20000,\n",
       " 'adam_beta_1': 0.9,\n",
       " 'adam_beta_2': 0.999,\n",
       " 'print_every': 100,\n",
       " 'evaluate_every': 1000,\n",
       " 'max_training_iterations': 200000,\n",
       " 'weight_target_loss': 0.3,\n",
       " 'max_testing_examples': None,\n",
       " 'splits': 'test',\n",
       " 'max_decoding_steps': 30,\n",
       " 'output_file_name': 'predict.json',\n",
       " 'simple_situation_representation': True,\n",
       " 'cnn_hidden_num_channels': 50,\n",
       " 'cnn_kernel_size': 7,\n",
       " 'cnn_dropout_p': 0.1,\n",
       " 'auxiliary_task': False,\n",
       " 'embedding_dimension': 25,\n",
       " 'num_encoder_layers': 1,\n",
       " 'encoder_hidden_size': 100,\n",
       " 'encoder_dropout_p': 0.3,\n",
       " 'encoder_bidirectional': True,\n",
       " 'num_decoder_layers': 1,\n",
       " 'attention_type': 'bahdanau',\n",
       " 'decoder_dropout_p': 0.3,\n",
       " 'decoder_hidden_size': 100,\n",
       " 'conditional_attention': True,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_flags = vars(parser.parse_args(args=['--mode', 'test', \n",
    "                                           '--data_directory', '../multimodal_seq2seq_gSCAN/data/compositional_splits/', \n",
    "                                           '--output_directory', './syntactic_dependency_compositional_splits-results/', \n",
    "                                           '--attention_type', 'bahdanau', \n",
    "                                           '--max_training_iterations', '200000']))\n",
    "# input_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-20 21:46 Initializing dummy gSCAN dataset for adverserial experiments...\n",
      "2021-02-20 21:47 Loading vocabularies...\n",
      "2021-02-20 21:47 Done loading vocabularies.\n"
     ]
    }
   ],
   "source": [
    "compositional_splits_data_path = os.path.join('../multimodal_seq2seq_gSCAN/data/compositional_splits/', \n",
    "                              \"dataset.txt\")\n",
    "compositional_splits_preprocessor = DummyGroundedScanDataset(compositional_splits_data_path, \n",
    "                                        '../multimodal_seq2seq_gSCAN/data/compositional_splits/', \n",
    "                                        input_vocabulary_file=\"training_input_vocab.txt\", \n",
    "                                        target_vocabulary_file=\"training_target_vocab.txt\",\n",
    "                                        generate_vocabulary=False,\n",
    "                                        k=0)\n",
    "# get a single example\n",
    "compositional_splits_dataset = \\\n",
    "    GroundedScan.load_dataset_from_file(\n",
    "        compositional_splits_data_path, \n",
    "        save_directory='./syntactic_dependency_compositional_splits-results/', \n",
    "        k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "raw_example = None\n",
    "for _, example in enumerate(compositional_splits_dataset.get_examples_with_image('test', True)):\n",
    "    raw_example = example\n",
    "    break\n",
    "single_example = compositional_splits_preprocessor.process(raw_example)\n",
    "# create the model\n",
    "model = Model(input_vocabulary_size=compositional_splits_preprocessor.input_vocabulary_size,\n",
    "              target_vocabulary_size=compositional_splits_preprocessor.target_vocabulary_size,\n",
    "              num_cnn_channels=compositional_splits_preprocessor.image_channels,\n",
    "              input_padding_idx=compositional_splits_preprocessor.input_vocabulary.pad_idx,\n",
    "              target_pad_idx=compositional_splits_preprocessor.target_vocabulary.pad_idx,\n",
    "              target_eos_idx=compositional_splits_preprocessor.target_vocabulary.eos_idx,\n",
    "              **input_flags)\n",
    "model = model.cuda() if use_cuda else model\n",
    "_ = model.load_model(\"./syntactic_dependency_compositional_splits-results/model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['push', 'a', 'green', 'small', 'circle']\n",
      "true: ['turn left', 'turn left', 'walk', 'walk', 'turn right', 'walk', 'walk', 'walk']\n",
      "pred: ['turn left', 'turn left', 'walk', 'walk', 'turn right', 'walk', 'walk', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# regular input\n",
    "print(\"in:\", raw_example['input_command'])\n",
    "print(\"true:\", raw_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "single_example = compositional_splits_preprocessor.process(raw_example)\n",
    "output = predict_single(single_example, model=model, \n",
    "                        max_decoding_steps=30, \n",
    "                        pad_idx=compositional_splits_preprocessor.target_vocabulary.pad_idx, \n",
    "                        sos_idx=compositional_splits_preprocessor.target_vocabulary.sos_idx,\n",
    "                        eos_idx=compositional_splits_preprocessor.target_vocabulary.eos_idx, \n",
    "                        device=device)\n",
    "pred_command = compositional_splits_preprocessor.array_to_sentence(output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", pred_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['push', 'a', 'small', 'green', 'circle']\n",
      "true: ['turn left', 'turn left', 'walk', 'walk', 'turn right', 'walk', 'walk', 'walk']\n",
      "pred: ['turn left', 'turn left', 'walk', 'walk', 'turn right', 'walk', 'walk', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# corrupted input\n",
    "corrupt_example = make_corrupt_example(raw_example)\n",
    "print(\"in:\", corrupt_example['input_command'])\n",
    "print(\"true:\", corrupt_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "corrupt_single_example = compositional_splits_preprocessor.process(corrupt_example)\n",
    "corrupt_output = predict_single(corrupt_single_example, model=model, \n",
    "                                max_decoding_steps=30, \n",
    "                                pad_idx=compositional_splits_preprocessor.target_vocabulary.pad_idx, \n",
    "                                sos_idx=compositional_splits_preprocessor.target_vocabulary.sos_idx,\n",
    "                                eos_idx=compositional_splits_preprocessor.target_vocabulary.eos_idx, \n",
    "                                device=device)\n",
    "corrupt_pred_command = compositional_splits_preprocessor.array_to_sentence(corrupt_output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", corrupt_pred_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3. Systematically look at the effect with full gSCAN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance drops with permuted inputs which kept the same semantics\n",
    "\n",
    "**Hypothesis** If the model do grounded language reasonings syntactically, we should see performance kept in the same level across all test splits. If not, it means model does not reason about the input command, and mostly likely is just doing some pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will the prediction change if we randomly swap words in the inputs?\n",
    "\n",
    "**Hypothesis** If the model output does not change for the majority of cases, it is another strong proof of the model being lacking of systematicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(x=pred_levDs, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Predicted Action Lev. Dist')\n",
    "plt.ylabel('Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
